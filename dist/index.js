/*
 * ATTENTION: The "eval" devtool has been used (maybe by default in mode: "development").
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
/******/ (() => { // webpackBootstrap
/******/ 	var __webpack_modules__ = ({

/***/ "./index.js":
/*!******************!*\
  !*** ./index.js ***!
  \******************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   \"default\": () => (/* binding */ parseWebGLCode)\n/* harmony export */ });\n/* harmony import */ var antlr4__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! antlr4 */ \"./node_modules/antlr4/src/antlr4/index.js\");\n/* harmony import */ var _simplerlangListener__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./simplerlangListener */ \"./simplerlangListener.js\");\n/* harmony import */ var _simplerlangLexer__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ./simplerlangLexer */ \"./simplerlangLexer.js\");\n/* harmony import */ var _simplerlangParser__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ./simplerlangParser */ \"./simplerlangParser.js\");\n\n\n\n\n\nconst fs = __webpack_require__(/*! fs */ \"fs\");\n\nfunction parseWebGLCode() {\n  // const input = `Object obj1 = Create Cube {\n  //     X = 2\n  //     Y = 1\n  //     Z = 0\n  //     Scale = 1\n  // }\n  // Object obj2 = Create Pyramid {\n  //     X = 0\n  //     Y = 0\n  //     Z = 0\n  //     Scale = 2\n  // }\n  // Object obj3 = Create Sphere {\n  //     X = -1\n  //     Y = -2\n  //     Z = 0\n  //     Scale = 1\n  // }`;\n  const input = fs.readFileSync(\"./input.txt\").toString().replaceAll(\"\\t\", \"\").replaceAll(\"\\r\", \"\");\n  const chars = new antlr4__WEBPACK_IMPORTED_MODULE_0__.InputStream(input);\n  const lexer = new _simplerlangLexer__WEBPACK_IMPORTED_MODULE_2__[\"default\"](chars);\n  const tokens = new antlr4__WEBPACK_IMPORTED_MODULE_0__.CommonTokenStream(lexer);\n  const parser = new _simplerlangParser__WEBPACK_IMPORTED_MODULE_3__[\"default\"](tokens);\n  parser.buildParseTrees = true;\n  const tree = parser.prog();\n  const listener = new _simplerlangListener__WEBPACK_IMPORTED_MODULE_1__[\"default\"]();\n  antlr4__WEBPACK_IMPORTED_MODULE_0__.tree.ParseTreeWalker.DEFAULT.walk(listener, tree);\n  return listener.getState();\n}\nfs.writeFileSync(\"./some.json\", JSON.stringify(parseWebGLCode()));\n\n//# sourceURL=webpack://test/./index.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/BufferedTokenStream.js":
/*!***************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/BufferedTokenStream.js ***!
  \***************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Token\n} = __webpack_require__(/*! ./Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\nconst Lexer = __webpack_require__(/*! ./Lexer */ \"./node_modules/antlr4/src/antlr4/Lexer.js\");\n\nconst {\n  Interval\n} = __webpack_require__(/*! ./IntervalSet */ \"./node_modules/antlr4/src/antlr4/IntervalSet.js\"); // this is just to keep meaningful parameter types to Parser\n\n\nclass TokenStream {}\n/**\n * This implementation of {@link TokenStream} loads tokens from a\n * {@link TokenSource} on-demand, and places the tokens in a buffer to provide\n * access to any previous token by index.\n *\n * <p>\n * This token stream ignores the value of {@link Token//getChannel}. If your\n * parser requires the token stream filter tokens to only those on a particular\n * channel, such as {@link Token//DEFAULT_CHANNEL} or\n * {@link Token//HIDDEN_CHANNEL}, use a filtering token stream such a\n * {@link CommonTokenStream}.</p>\n */\n\n\nclass BufferedTokenStream extends TokenStream {\n  constructor(tokenSource) {\n    super(); // The {@link TokenSource} from which tokens for this stream are fetched.\n\n    this.tokenSource = tokenSource;\n    /**\n     * A collection of all tokens fetched from the token source. The list is\n     * considered a complete view of the input once {@link //fetchedEOF} is set\n     * to {@code true}.\n     */\n\n    this.tokens = [];\n    /**\n     * The index into {@link //tokens} of the current token (next token to\n     * {@link //consume}). {@link //tokens}{@code [}{@link //p}{@code ]} should\n     * be\n     * {@link //LT LT(1)}.\n     *\n     * <p>This field is set to -1 when the stream is first constructed or when\n     * {@link //setTokenSource} is called, indicating that the first token has\n     * not yet been fetched from the token source. For additional information,\n     * see the documentation of {@link IntStream} for a description of\n     * Initializing Methods.</p>\n     */\n\n    this.index = -1;\n    /**\n     * Indicates whether the {@link Token//EOF} token has been fetched from\n     * {@link //tokenSource} and added to {@link //tokens}. This field improves\n     * performance for the following cases:\n     *\n     * <ul>\n     * <li>{@link //consume}: The lookahead check in {@link //consume} to\n     * prevent\n     * consuming the EOF symbol is optimized by checking the values of\n     * {@link //fetchedEOF} and {@link //p} instead of calling {@link\n     * //LA}.</li>\n     * <li>{@link //fetch}: The check to prevent adding multiple EOF symbols\n     * into\n     * {@link //tokens} is trivial with this field.</li>\n     * <ul>\n     */\n\n    this.fetchedEOF = false;\n  }\n\n  mark() {\n    return 0;\n  }\n\n  release(marker) {// no resources to release\n  }\n\n  reset() {\n    this.seek(0);\n  }\n\n  seek(index) {\n    this.lazyInit();\n    this.index = this.adjustSeekIndex(index);\n  }\n\n  get(index) {\n    this.lazyInit();\n    return this.tokens[index];\n  }\n\n  consume() {\n    let skipEofCheck = false;\n\n    if (this.index >= 0) {\n      if (this.fetchedEOF) {\n        // the last token in tokens is EOF. skip check if p indexes any\n        // fetched token except the last.\n        skipEofCheck = this.index < this.tokens.length - 1;\n      } else {\n        // no EOF token in tokens. skip check if p indexes a fetched token.\n        skipEofCheck = this.index < this.tokens.length;\n      }\n    } else {\n      // not yet initialized\n      skipEofCheck = false;\n    }\n\n    if (!skipEofCheck && this.LA(1) === Token.EOF) {\n      throw \"cannot consume EOF\";\n    }\n\n    if (this.sync(this.index + 1)) {\n      this.index = this.adjustSeekIndex(this.index + 1);\n    }\n  }\n  /**\n   * Make sure index {@code i} in tokens has a token.\n   *\n   * @return {Boolean} {@code true} if a token is located at index {@code i}, otherwise\n   * {@code false}.\n   * @see //get(int i)\n   */\n\n\n  sync(i) {\n    const n = i - this.tokens.length + 1; // how many more elements we need?\n\n    if (n > 0) {\n      const fetched = this.fetch(n);\n      return fetched >= n;\n    }\n\n    return true;\n  }\n  /**\n   * Add {@code n} elements to buffer.\n   *\n   * @return {Number} The actual number of elements added to the buffer.\n   */\n\n\n  fetch(n) {\n    if (this.fetchedEOF) {\n      return 0;\n    }\n\n    for (let i = 0; i < n; i++) {\n      const t = this.tokenSource.nextToken();\n      t.tokenIndex = this.tokens.length;\n      this.tokens.push(t);\n\n      if (t.type === Token.EOF) {\n        this.fetchedEOF = true;\n        return i + 1;\n      }\n    }\n\n    return n;\n  } // Get all tokens from start..stop inclusively///\n\n\n  getTokens(start, stop, types) {\n    if (types === undefined) {\n      types = null;\n    }\n\n    if (start < 0 || stop < 0) {\n      return null;\n    }\n\n    this.lazyInit();\n    const subset = [];\n\n    if (stop >= this.tokens.length) {\n      stop = this.tokens.length - 1;\n    }\n\n    for (let i = start; i < stop; i++) {\n      const t = this.tokens[i];\n\n      if (t.type === Token.EOF) {\n        break;\n      }\n\n      if (types === null || types.contains(t.type)) {\n        subset.push(t);\n      }\n    }\n\n    return subset;\n  }\n\n  LA(i) {\n    return this.LT(i).type;\n  }\n\n  LB(k) {\n    if (this.index - k < 0) {\n      return null;\n    }\n\n    return this.tokens[this.index - k];\n  }\n\n  LT(k) {\n    this.lazyInit();\n\n    if (k === 0) {\n      return null;\n    }\n\n    if (k < 0) {\n      return this.LB(-k);\n    }\n\n    const i = this.index + k - 1;\n    this.sync(i);\n\n    if (i >= this.tokens.length) {\n      // return EOF token\n      // EOF must be last token\n      return this.tokens[this.tokens.length - 1];\n    }\n\n    return this.tokens[i];\n  }\n  /**\n   * Allowed derived classes to modify the behavior of operations which change\n   * the current stream position by adjusting the target token index of a seek\n   * operation. The default implementation simply returns {@code i}. If an\n   * exception is thrown in this method, the current stream index should not be\n   * changed.\n   *\n   * <p>For example, {@link CommonTokenStream} overrides this method to ensure\n   * that\n   * the seek target is always an on-channel token.</p>\n   *\n   * @param {Number} i The target token index.\n   * @return {Number} The adjusted target token index.\n   */\n\n\n  adjustSeekIndex(i) {\n    return i;\n  }\n\n  lazyInit() {\n    if (this.index === -1) {\n      this.setup();\n    }\n  }\n\n  setup() {\n    this.sync(0);\n    this.index = this.adjustSeekIndex(0);\n  } // Reset this token stream by setting its token source.///\n\n\n  setTokenSource(tokenSource) {\n    this.tokenSource = tokenSource;\n    this.tokens = [];\n    this.index = -1;\n    this.fetchedEOF = false;\n  }\n  /**\n   * Given a starting index, return the index of the next token on channel.\n   * Return i if tokens[i] is on channel. Return -1 if there are no tokens\n   * on channel between i and EOF.\n   */\n\n\n  nextTokenOnChannel(i, channel) {\n    this.sync(i);\n\n    if (i >= this.tokens.length) {\n      return -1;\n    }\n\n    let token = this.tokens[i];\n\n    while (token.channel !== this.channel) {\n      if (token.type === Token.EOF) {\n        return -1;\n      }\n\n      i += 1;\n      this.sync(i);\n      token = this.tokens[i];\n    }\n\n    return i;\n  }\n  /**\n   * Given a starting index, return the index of the previous token on channel.\n   * Return i if tokens[i] is on channel. Return -1 if there are no tokens\n   * on channel between i and 0.\n   */\n\n\n  previousTokenOnChannel(i, channel) {\n    while (i >= 0 && this.tokens[i].channel !== channel) {\n      i -= 1;\n    }\n\n    return i;\n  }\n  /**\n   * Collect all tokens on specified channel to the right of\n   * the current token up until we see a token on DEFAULT_TOKEN_CHANNEL or\n   * EOF. If channel is -1, find any non default channel token.\n   */\n\n\n  getHiddenTokensToRight(tokenIndex, channel) {\n    if (channel === undefined) {\n      channel = -1;\n    }\n\n    this.lazyInit();\n\n    if (tokenIndex < 0 || tokenIndex >= this.tokens.length) {\n      throw \"\" + tokenIndex + \" not in 0..\" + this.tokens.length - 1;\n    }\n\n    const nextOnChannel = this.nextTokenOnChannel(tokenIndex + 1, Lexer.DEFAULT_TOKEN_CHANNEL);\n    const from_ = tokenIndex + 1; // if none onchannel to right, nextOnChannel=-1 so set to = last token\n\n    const to = nextOnChannel === -1 ? this.tokens.length - 1 : nextOnChannel;\n    return this.filterForChannel(from_, to, channel);\n  }\n  /**\n   * Collect all tokens on specified channel to the left of\n   * the current token up until we see a token on DEFAULT_TOKEN_CHANNEL.\n   * If channel is -1, find any non default channel token.\n   */\n\n\n  getHiddenTokensToLeft(tokenIndex, channel) {\n    if (channel === undefined) {\n      channel = -1;\n    }\n\n    this.lazyInit();\n\n    if (tokenIndex < 0 || tokenIndex >= this.tokens.length) {\n      throw \"\" + tokenIndex + \" not in 0..\" + this.tokens.length - 1;\n    }\n\n    const prevOnChannel = this.previousTokenOnChannel(tokenIndex - 1, Lexer.DEFAULT_TOKEN_CHANNEL);\n\n    if (prevOnChannel === tokenIndex - 1) {\n      return null;\n    } // if none on channel to left, prevOnChannel=-1 then from=0\n\n\n    const from_ = prevOnChannel + 1;\n    const to = tokenIndex - 1;\n    return this.filterForChannel(from_, to, channel);\n  }\n\n  filterForChannel(left, right, channel) {\n    const hidden = [];\n\n    for (let i = left; i < right + 1; i++) {\n      const t = this.tokens[i];\n\n      if (channel === -1) {\n        if (t.channel !== Lexer.DEFAULT_TOKEN_CHANNEL) {\n          hidden.push(t);\n        }\n      } else if (t.channel === channel) {\n        hidden.push(t);\n      }\n    }\n\n    if (hidden.length === 0) {\n      return null;\n    }\n\n    return hidden;\n  }\n\n  getSourceName() {\n    return this.tokenSource.getSourceName();\n  } // Get the text of all tokens in this buffer.///\n\n\n  getText(interval) {\n    this.lazyInit();\n    this.fill();\n\n    if (interval === undefined || interval === null) {\n      interval = new Interval(0, this.tokens.length - 1);\n    }\n\n    let start = interval.start;\n\n    if (start instanceof Token) {\n      start = start.tokenIndex;\n    }\n\n    let stop = interval.stop;\n\n    if (stop instanceof Token) {\n      stop = stop.tokenIndex;\n    }\n\n    if (start === null || stop === null || start < 0 || stop < 0) {\n      return \"\";\n    }\n\n    if (stop >= this.tokens.length) {\n      stop = this.tokens.length - 1;\n    }\n\n    let s = \"\";\n\n    for (let i = start; i < stop + 1; i++) {\n      const t = this.tokens[i];\n\n      if (t.type === Token.EOF) {\n        break;\n      }\n\n      s = s + t.text;\n    }\n\n    return s;\n  } // Get all tokens from lexer until EOF///\n\n\n  fill() {\n    this.lazyInit();\n\n    while (this.fetch(1000) === 1000) {\n      continue;\n    }\n  }\n\n}\n\nmodule.exports = BufferedTokenStream;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/BufferedTokenStream.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/CharStreams.js":
/*!*******************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/CharStreams.js ***!
  \*******************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst InputStream = __webpack_require__(/*! ./InputStream */ \"./node_modules/antlr4/src/antlr4/InputStream.js\");\n\nconst fs = __webpack_require__(/*! fs */ \"fs\");\n/**\n * Utility functions to create InputStreams from various sources.\n *\n * All returned InputStreams support the full range of Unicode\n * up to U+10FFFF (the default behavior of InputStream only supports\n * code points up to U+FFFF).\n */\n\n\nconst CharStreams = {\n  // Creates an InputStream from a string.\n  fromString: function (str) {\n    return new InputStream(str, true);\n  },\n\n  /**\n   * Asynchronously creates an InputStream from a blob given the\n   * encoding of the bytes in that blob (defaults to 'utf8' if\n   * encoding is null).\n   *\n   * Invokes onLoad(result) on success, onError(error) on\n   * failure.\n   */\n  fromBlob: function (blob, encoding, onLoad, onError) {\n    const reader = new window.FileReader();\n\n    reader.onload = function (e) {\n      const is = new InputStream(e.target.result, true);\n      onLoad(is);\n    };\n\n    reader.onerror = onError;\n    reader.readAsText(blob, encoding);\n  },\n\n  /**\n   * Creates an InputStream from a Buffer given the\n   * encoding of the bytes in that buffer (defaults to 'utf8' if\n   * encoding is null).\n   */\n  fromBuffer: function (buffer, encoding) {\n    return new InputStream(buffer.toString(encoding), true);\n  },\n\n  /** Asynchronously creates an InputStream from a file on disk given\n   * the encoding of the bytes in that file (defaults to 'utf8' if\n   * encoding is null).\n   *\n   * Invokes callback(error, result) on completion.\n   */\n  fromPath: function (path, encoding, callback) {\n    fs.readFile(path, encoding, function (err, data) {\n      let is = null;\n\n      if (data !== null) {\n        is = new InputStream(data, true);\n      }\n\n      callback(err, is);\n    });\n  },\n\n  /**\n   * Synchronously creates an InputStream given a path to a file\n   * on disk and the encoding of the bytes in that file (defaults to\n   * 'utf8' if encoding is null).\n   */\n  fromPathSync: function (path, encoding) {\n    const data = fs.readFileSync(path, encoding);\n    return new InputStream(data, true);\n  }\n};\nmodule.exports = CharStreams;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/CharStreams.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/CommonTokenFactory.js":
/*!**************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/CommonTokenFactory.js ***!
  \**************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst CommonToken = (__webpack_require__(/*! ./Token */ \"./node_modules/antlr4/src/antlr4/Token.js\").CommonToken);\n\nclass TokenFactory {}\n/**\n * This default implementation of {@link TokenFactory} creates\n * {@link CommonToken} objects.\n */\n\n\nclass CommonTokenFactory extends TokenFactory {\n  constructor(copyText) {\n    super();\n    /**\n     * Indicates whether {@link CommonToken//setText} should be called after\n     * constructing tokens to explicitly set the text. This is useful for cases\n     * where the input stream might not be able to provide arbitrary substrings\n     * of text from the input after the lexer creates a token (e.g. the\n     * implementation of {@link CharStream//getText} in\n     * {@link UnbufferedCharStream} throws an\n     * {@link UnsupportedOperationException}). Explicitly setting the token text\n     * allows {@link Token//getText} to be called at any time regardless of the\n     * input stream implementation.\n     *\n     * <p>\n     * The default value is {@code false} to avoid the performance and memory\n     * overhead of copying text for every token unless explicitly requested.</p>\n     */\n\n    this.copyText = copyText === undefined ? false : copyText;\n  }\n\n  create(source, type, text, channel, start, stop, line, column) {\n    const t = new CommonToken(source, type, channel, start, stop);\n    t.line = line;\n    t.column = column;\n\n    if (text !== null) {\n      t.text = text;\n    } else if (this.copyText && source[1] !== null) {\n      t.text = source[1].getText(start, stop);\n    }\n\n    return t;\n  }\n\n  createThin(type, text) {\n    const t = new CommonToken(null, type);\n    t.text = text;\n    return t;\n  }\n\n}\n/**\n * The default {@link CommonTokenFactory} instance.\n *\n * <p>\n * This token factory does not explicitly copy token text when constructing\n * tokens.</p>\n */\n\n\nCommonTokenFactory.DEFAULT = new CommonTokenFactory();\nmodule.exports = CommonTokenFactory;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/CommonTokenFactory.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/CommonTokenStream.js":
/*!*************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/CommonTokenStream.js ***!
  \*************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst Token = (__webpack_require__(/*! ./Token */ \"./node_modules/antlr4/src/antlr4/Token.js\").Token);\n\nconst BufferedTokenStream = __webpack_require__(/*! ./BufferedTokenStream */ \"./node_modules/antlr4/src/antlr4/BufferedTokenStream.js\");\n/**\n * This class extends {@link BufferedTokenStream} with functionality to filter\n * token streams to tokens on a particular channel (tokens where\n * {@link Token//getChannel} returns a particular value).\n *\n * <p>\n * This token stream provides access to all tokens by index or when calling\n * methods like {@link //getText}. The channel filtering is only used for code\n * accessing tokens via the lookahead methods {@link //LA}, {@link //LT}, and\n * {@link //LB}.</p>\n *\n * <p>\n * By default, tokens are placed on the default channel\n * ({@link Token//DEFAULT_CHANNEL}), but may be reassigned by using the\n * {@code ->channel(HIDDEN)} lexer command, or by using an embedded action to\n * call {@link Lexer//setChannel}.\n * </p>\n *\n * <p>\n * Note: lexer rules which use the {@code ->skip} lexer command or call\n * {@link Lexer//skip} do not produce tokens at all, so input text matched by\n * such a rule will not be available as part of the token stream, regardless of\n * channel.</p>\n */\n\n\nclass CommonTokenStream extends BufferedTokenStream {\n  constructor(lexer, channel) {\n    super(lexer);\n    this.channel = channel === undefined ? Token.DEFAULT_CHANNEL : channel;\n  }\n\n  adjustSeekIndex(i) {\n    return this.nextTokenOnChannel(i, this.channel);\n  }\n\n  LB(k) {\n    if (k === 0 || this.index - k < 0) {\n      return null;\n    }\n\n    let i = this.index;\n    let n = 1; // find k good tokens looking backwards\n\n    while (n <= k) {\n      // skip off-channel tokens\n      i = this.previousTokenOnChannel(i - 1, this.channel);\n      n += 1;\n    }\n\n    if (i < 0) {\n      return null;\n    }\n\n    return this.tokens[i];\n  }\n\n  LT(k) {\n    this.lazyInit();\n\n    if (k === 0) {\n      return null;\n    }\n\n    if (k < 0) {\n      return this.LB(-k);\n    }\n\n    let i = this.index;\n    let n = 1; // we know tokens[pos] is a good one\n    // find k good tokens\n\n    while (n < k) {\n      // skip off-channel tokens, but make sure to not look past EOF\n      if (this.sync(i + 1)) {\n        i = this.nextTokenOnChannel(i + 1, this.channel);\n      }\n\n      n += 1;\n    }\n\n    return this.tokens[i];\n  } // Count EOF just once.\n\n\n  getNumberOfOnChannelTokens() {\n    let n = 0;\n    this.fill();\n\n    for (let i = 0; i < this.tokens.length; i++) {\n      const t = this.tokens[i];\n\n      if (t.channel === this.channel) {\n        n += 1;\n      }\n\n      if (t.type === Token.EOF) {\n        break;\n      }\n    }\n\n    return n;\n  }\n\n}\n\nmodule.exports = CommonTokenStream;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/CommonTokenStream.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/FileStream.js":
/*!******************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/FileStream.js ***!
  \******************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst InputStream = __webpack_require__(/*! ./InputStream */ \"./node_modules/antlr4/src/antlr4/InputStream.js\");\n\nconst fs = __webpack_require__(/*! fs */ \"fs\");\n/**\n * This is an InputStream that is loaded from a file all at once\n * when you construct the object.\n */\n\n\nclass FileStream extends InputStream {\n  constructor(fileName, decodeToUnicodeCodePoints) {\n    const data = fs.readFileSync(fileName, \"utf8\");\n    super(data, decodeToUnicodeCodePoints);\n    this.fileName = fileName;\n  }\n\n}\n\nmodule.exports = FileStream;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/FileStream.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/InputStream.js":
/*!*******************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/InputStream.js ***!
  \*******************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Token\n} = __webpack_require__(/*! ./Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\n__webpack_require__(/*! ./polyfills/codepointat */ \"./node_modules/antlr4/src/antlr4/polyfills/codepointat.js\");\n\n__webpack_require__(/*! ./polyfills/fromcodepoint */ \"./node_modules/antlr4/src/antlr4/polyfills/fromcodepoint.js\");\n/**\n * If decodeToUnicodeCodePoints is true, the input is treated\n * as a series of Unicode code points.\n *\n * Otherwise, the input is treated as a series of 16-bit UTF-16 code\n * units.\n */\n\n\nclass InputStream {\n  constructor(data, decodeToUnicodeCodePoints) {\n    this.name = \"<empty>\";\n    this.strdata = data;\n    this.decodeToUnicodeCodePoints = decodeToUnicodeCodePoints || false; // _loadString - Vacuum all input from a string and then treat it like a buffer.\n\n    this._index = 0;\n    this.data = [];\n\n    if (this.decodeToUnicodeCodePoints) {\n      for (let i = 0; i < this.strdata.length;) {\n        const codePoint = this.strdata.codePointAt(i);\n        this.data.push(codePoint);\n        i += codePoint <= 0xFFFF ? 1 : 2;\n      }\n    } else {\n      this.data = new Array(this.strdata.length);\n\n      for (let i = 0; i < this.strdata.length; i++) {\n        const codeUnit = this.strdata.charCodeAt(i);\n        this.data[i] = codeUnit;\n      }\n    }\n\n    this._size = this.data.length;\n  }\n  /**\n   * Reset the stream so that it's in the same state it was\n   * when the object was created *except* the data array is not\n   * touched.\n   */\n\n\n  reset() {\n    this._index = 0;\n  }\n\n  consume() {\n    if (this._index >= this._size) {\n      // assert this.LA(1) == Token.EOF\n      throw \"cannot consume EOF\";\n    }\n\n    this._index += 1;\n  }\n\n  LA(offset) {\n    if (offset === 0) {\n      return 0; // undefined\n    }\n\n    if (offset < 0) {\n      offset += 1; // e.g., translate LA(-1) to use offset=0\n    }\n\n    const pos = this._index + offset - 1;\n\n    if (pos < 0 || pos >= this._size) {\n      // invalid\n      return Token.EOF;\n    }\n\n    return this.data[pos];\n  }\n\n  LT(offset) {\n    return this.LA(offset);\n  } // mark/release do nothing; we have entire buffer\n\n\n  mark() {\n    return -1;\n  }\n\n  release(marker) {}\n  /**\n   * consume() ahead until p==_index; can't just set p=_index as we must\n   * update line and column. If we seek backwards, just set p\n   */\n\n\n  seek(_index) {\n    if (_index <= this._index) {\n      this._index = _index; // just jump; don't update stream state (line,\n      // ...)\n\n      return;\n    } // seek forward\n\n\n    this._index = Math.min(_index, this._size);\n  }\n\n  getText(start, stop) {\n    if (stop >= this._size) {\n      stop = this._size - 1;\n    }\n\n    if (start >= this._size) {\n      return \"\";\n    } else {\n      if (this.decodeToUnicodeCodePoints) {\n        let result = \"\";\n\n        for (let i = start; i <= stop; i++) {\n          result += String.fromCodePoint(this.data[i]);\n        }\n\n        return result;\n      } else {\n        return this.strdata.slice(start, stop + 1);\n      }\n    }\n  }\n\n  toString() {\n    return this.strdata;\n  }\n\n  get index() {\n    return this._index;\n  }\n\n  get size() {\n    return this._size;\n  }\n\n}\n\nmodule.exports = InputStream;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/InputStream.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/IntervalSet.js":
/*!*******************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/IntervalSet.js ***!
  \*******************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Token\n} = __webpack_require__(/*! ./Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n/* stop is not included! */\n\n\nclass Interval {\n  constructor(start, stop) {\n    this.start = start;\n    this.stop = stop;\n  }\n\n  clone() {\n    return new Interval(this.start, this.stop);\n  }\n\n  contains(item) {\n    return item >= this.start && item < this.stop;\n  }\n\n  toString() {\n    if (this.start === this.stop - 1) {\n      return this.start.toString();\n    } else {\n      return this.start.toString() + \"..\" + (this.stop - 1).toString();\n    }\n  }\n\n  get length() {\n    return this.stop - this.start;\n  }\n\n}\n\nclass IntervalSet {\n  constructor() {\n    this.intervals = null;\n    this.readOnly = false;\n  }\n\n  first(v) {\n    if (this.intervals === null || this.intervals.length === 0) {\n      return Token.INVALID_TYPE;\n    } else {\n      return this.intervals[0].start;\n    }\n  }\n\n  addOne(v) {\n    this.addInterval(new Interval(v, v + 1));\n  }\n\n  addRange(l, h) {\n    this.addInterval(new Interval(l, h + 1));\n  }\n\n  addInterval(toAdd) {\n    if (this.intervals === null) {\n      this.intervals = [];\n      this.intervals.push(toAdd.clone());\n    } else {\n      // find insert pos\n      for (let pos = 0; pos < this.intervals.length; pos++) {\n        const existing = this.intervals[pos]; // distinct range -> insert\n\n        if (toAdd.stop < existing.start) {\n          this.intervals.splice(pos, 0, toAdd);\n          return;\n        } // contiguous range -> adjust\n        else if (toAdd.stop === existing.start) {\n          this.intervals[pos] = new Interval(toAdd.start, existing.stop);\n          return;\n        } // overlapping range -> adjust and reduce\n        else if (toAdd.start <= existing.stop) {\n          this.intervals[pos] = new Interval(Math.min(existing.start, toAdd.start), Math.max(existing.stop, toAdd.stop));\n          this.reduce(pos);\n          return;\n        }\n      } // greater than any existing\n\n\n      this.intervals.push(toAdd.clone());\n    }\n  }\n\n  addSet(other) {\n    if (other.intervals !== null) {\n      other.intervals.forEach(toAdd => this.addInterval(toAdd), this);\n    }\n\n    return this;\n  }\n\n  reduce(pos) {\n    // only need to reduce if pos is not the last\n    if (pos < this.intervals.length - 1) {\n      const current = this.intervals[pos];\n      const next = this.intervals[pos + 1]; // if next contained in current\n\n      if (current.stop >= next.stop) {\n        this.intervals.splice(pos + 1, 1);\n        this.reduce(pos);\n      } else if (current.stop >= next.start) {\n        this.intervals[pos] = new Interval(current.start, next.stop);\n        this.intervals.splice(pos + 1, 1);\n      }\n    }\n  }\n\n  complement(start, stop) {\n    const result = new IntervalSet();\n    result.addInterval(new Interval(start, stop + 1));\n    if (this.intervals !== null) this.intervals.forEach(toRemove => result.removeRange(toRemove));\n    return result;\n  }\n\n  contains(item) {\n    if (this.intervals === null) {\n      return false;\n    } else {\n      for (let k = 0; k < this.intervals.length; k++) {\n        if (this.intervals[k].contains(item)) {\n          return true;\n        }\n      }\n\n      return false;\n    }\n  }\n\n  removeRange(toRemove) {\n    if (toRemove.start === toRemove.stop - 1) {\n      this.removeOne(toRemove.start);\n    } else if (this.intervals !== null) {\n      let pos = 0;\n\n      for (let n = 0; n < this.intervals.length; n++) {\n        const existing = this.intervals[pos]; // intervals are ordered\n\n        if (toRemove.stop <= existing.start) {\n          return;\n        } // check for including range, split it\n        else if (toRemove.start > existing.start && toRemove.stop < existing.stop) {\n          this.intervals[pos] = new Interval(existing.start, toRemove.start);\n          const x = new Interval(toRemove.stop, existing.stop);\n          this.intervals.splice(pos, 0, x);\n          return;\n        } // check for included range, remove it\n        else if (toRemove.start <= existing.start && toRemove.stop >= existing.stop) {\n          this.intervals.splice(pos, 1);\n          pos = pos - 1; // need another pass\n        } // check for lower boundary\n        else if (toRemove.start < existing.stop) {\n          this.intervals[pos] = new Interval(existing.start, toRemove.start);\n        } // check for upper boundary\n        else if (toRemove.stop < existing.stop) {\n          this.intervals[pos] = new Interval(toRemove.stop, existing.stop);\n        }\n\n        pos += 1;\n      }\n    }\n  }\n\n  removeOne(value) {\n    if (this.intervals !== null) {\n      for (let i = 0; i < this.intervals.length; i++) {\n        const existing = this.intervals[i]; // intervals are ordered\n\n        if (value < existing.start) {\n          return;\n        } // check for single value range\n        else if (value === existing.start && value === existing.stop - 1) {\n          this.intervals.splice(i, 1);\n          return;\n        } // check for lower boundary\n        else if (value === existing.start) {\n          this.intervals[i] = new Interval(existing.start + 1, existing.stop);\n          return;\n        } // check for upper boundary\n        else if (value === existing.stop - 1) {\n          this.intervals[i] = new Interval(existing.start, existing.stop - 1);\n          return;\n        } // split existing range\n        else if (value < existing.stop - 1) {\n          const replace = new Interval(existing.start, value);\n          existing.start = value + 1;\n          this.intervals.splice(i, 0, replace);\n          return;\n        }\n      }\n    }\n  }\n\n  toString(literalNames, symbolicNames, elemsAreChar) {\n    literalNames = literalNames || null;\n    symbolicNames = symbolicNames || null;\n    elemsAreChar = elemsAreChar || false;\n\n    if (this.intervals === null) {\n      return \"{}\";\n    } else if (literalNames !== null || symbolicNames !== null) {\n      return this.toTokenString(literalNames, symbolicNames);\n    } else if (elemsAreChar) {\n      return this.toCharString();\n    } else {\n      return this.toIndexString();\n    }\n  }\n\n  toCharString() {\n    const names = [];\n\n    for (let i = 0; i < this.intervals.length; i++) {\n      const existing = this.intervals[i];\n\n      if (existing.stop === existing.start + 1) {\n        if (existing.start === Token.EOF) {\n          names.push(\"<EOF>\");\n        } else {\n          names.push(\"'\" + String.fromCharCode(existing.start) + \"'\");\n        }\n      } else {\n        names.push(\"'\" + String.fromCharCode(existing.start) + \"'..'\" + String.fromCharCode(existing.stop - 1) + \"'\");\n      }\n    }\n\n    if (names.length > 1) {\n      return \"{\" + names.join(\", \") + \"}\";\n    } else {\n      return names[0];\n    }\n  }\n\n  toIndexString() {\n    const names = [];\n\n    for (let i = 0; i < this.intervals.length; i++) {\n      const existing = this.intervals[i];\n\n      if (existing.stop === existing.start + 1) {\n        if (existing.start === Token.EOF) {\n          names.push(\"<EOF>\");\n        } else {\n          names.push(existing.start.toString());\n        }\n      } else {\n        names.push(existing.start.toString() + \"..\" + (existing.stop - 1).toString());\n      }\n    }\n\n    if (names.length > 1) {\n      return \"{\" + names.join(\", \") + \"}\";\n    } else {\n      return names[0];\n    }\n  }\n\n  toTokenString(literalNames, symbolicNames) {\n    const names = [];\n\n    for (let i = 0; i < this.intervals.length; i++) {\n      const existing = this.intervals[i];\n\n      for (let j = existing.start; j < existing.stop; j++) {\n        names.push(this.elementName(literalNames, symbolicNames, j));\n      }\n    }\n\n    if (names.length > 1) {\n      return \"{\" + names.join(\", \") + \"}\";\n    } else {\n      return names[0];\n    }\n  }\n\n  elementName(literalNames, symbolicNames, token) {\n    if (token === Token.EOF) {\n      return \"<EOF>\";\n    } else if (token === Token.EPSILON) {\n      return \"<EPSILON>\";\n    } else {\n      return literalNames[token] || symbolicNames[token];\n    }\n  }\n\n  get length() {\n    return this.intervals.map(interval => interval.length).reduce((acc, val) => acc + val);\n  }\n\n}\n\nmodule.exports = {\n  Interval,\n  IntervalSet\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/IntervalSet.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/LL1Analyzer.js":
/*!*******************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/LL1Analyzer.js ***!
  \*******************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Set,\n  BitSet\n} = __webpack_require__(/*! ./Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n\nconst {\n  Token\n} = __webpack_require__(/*! ./Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\nconst {\n  ATNConfig\n} = __webpack_require__(/*! ./atn/ATNConfig */ \"./node_modules/antlr4/src/antlr4/atn/ATNConfig.js\");\n\nconst {\n  IntervalSet\n} = __webpack_require__(/*! ./IntervalSet */ \"./node_modules/antlr4/src/antlr4/IntervalSet.js\");\n\nconst {\n  RuleStopState\n} = __webpack_require__(/*! ./atn/ATNState */ \"./node_modules/antlr4/src/antlr4/atn/ATNState.js\");\n\nconst {\n  RuleTransition,\n  NotSetTransition,\n  WildcardTransition,\n  AbstractPredicateTransition\n} = __webpack_require__(/*! ./atn/Transition */ \"./node_modules/antlr4/src/antlr4/atn/Transition.js\");\n\nconst {\n  predictionContextFromRuleContext,\n  PredictionContext,\n  SingletonPredictionContext\n} = __webpack_require__(/*! ./PredictionContext */ \"./node_modules/antlr4/src/antlr4/PredictionContext.js\");\n\nclass LL1Analyzer {\n  constructor(atn) {\n    this.atn = atn;\n  }\n  /**\n   * Calculates the SLL(1) expected lookahead set for each outgoing transition\n   * of an {@link ATNState}. The returned array has one element for each\n   * outgoing transition in {@code s}. If the closure from transition\n   * <em>i</em> leads to a semantic predicate before matching a symbol, the\n   * element at index <em>i</em> of the result will be {@code null}.\n   *\n   * @param s the ATN state\n   * @return the expected symbols for each outgoing transition of {@code s}.\n   */\n\n\n  getDecisionLookahead(s) {\n    if (s === null) {\n      return null;\n    }\n\n    const count = s.transitions.length;\n    const look = [];\n\n    for (let alt = 0; alt < count; alt++) {\n      look[alt] = new IntervalSet();\n      const lookBusy = new Set();\n      const seeThruPreds = false; // fail to get lookahead upon pred\n\n      this._LOOK(s.transition(alt).target, null, PredictionContext.EMPTY, look[alt], lookBusy, new BitSet(), seeThruPreds, false); // Wipe out lookahead for this alternative if we found nothing\n      // or we had a predicate when we !seeThruPreds\n\n\n      if (look[alt].length === 0 || look[alt].contains(LL1Analyzer.HIT_PRED)) {\n        look[alt] = null;\n      }\n    }\n\n    return look;\n  }\n  /**\n   * Compute set of tokens that can follow {@code s} in the ATN in the\n   * specified {@code ctx}.\n   *\n   * <p>If {@code ctx} is {@code null} and the end of the rule containing\n   * {@code s} is reached, {@link Token//EPSILON} is added to the result set.\n   * If {@code ctx} is not {@code null} and the end of the outermost rule is\n   * reached, {@link Token//EOF} is added to the result set.</p>\n   *\n   * @param s the ATN state\n   * @param stopState the ATN state to stop at. This can be a\n   * {@link BlockEndState} to detect epsilon paths through a closure.\n   * @param ctx the complete parser context, or {@code null} if the context\n   * should be ignored\n   *\n   * @return The set of tokens that can follow {@code s} in the ATN in the\n   * specified {@code ctx}.\n   */\n\n\n  LOOK(s, stopState, ctx) {\n    const r = new IntervalSet();\n    const seeThruPreds = true; // ignore preds; get all lookahead\n\n    ctx = ctx || null;\n    const lookContext = ctx !== null ? predictionContextFromRuleContext(s.atn, ctx) : null;\n\n    this._LOOK(s, stopState, lookContext, r, new Set(), new BitSet(), seeThruPreds, true);\n\n    return r;\n  }\n  /**\n   * Compute set of tokens that can follow {@code s} in the ATN in the\n   * specified {@code ctx}.\n   *\n   * <p>If {@code ctx} is {@code null} and {@code stopState} or the end of the\n   * rule containing {@code s} is reached, {@link Token//EPSILON} is added to\n   * the result set. If {@code ctx} is not {@code null} and {@code addEOF} is\n   * {@code true} and {@code stopState} or the end of the outermost rule is\n   * reached, {@link Token//EOF} is added to the result set.</p>\n   *\n   * @param s the ATN state.\n   * @param stopState the ATN state to stop at. This can be a\n   * {@link BlockEndState} to detect epsilon paths through a closure.\n   * @param ctx The outer context, or {@code null} if the outer context should\n   * not be used.\n   * @param look The result lookahead set.\n   * @param lookBusy A set used for preventing epsilon closures in the ATN\n   * from causing a stack overflow. Outside code should pass\n   * {@code new Set<ATNConfig>} for this argument.\n   * @param calledRuleStack A set used for preventing left recursion in the\n   * ATN from causing a stack overflow. Outside code should pass\n   * {@code new BitSet()} for this argument.\n   * @param seeThruPreds {@code true} to true semantic predicates as\n   * implicitly {@code true} and \"see through them\", otherwise {@code false}\n   * to treat semantic predicates as opaque and add {@link //HIT_PRED} to the\n   * result if one is encountered.\n   * @param addEOF Add {@link Token//EOF} to the result if the end of the\n   * outermost context is reached. This parameter has no effect if {@code ctx}\n   * is {@code null}.\n   */\n\n\n  _LOOK(s, stopState, ctx, look, lookBusy, calledRuleStack, seeThruPreds, addEOF) {\n    const c = new ATNConfig({\n      state: s,\n      alt: 0,\n      context: ctx\n    }, null);\n\n    if (lookBusy.contains(c)) {\n      return;\n    }\n\n    lookBusy.add(c);\n\n    if (s === stopState) {\n      if (ctx === null) {\n        look.addOne(Token.EPSILON);\n        return;\n      } else if (ctx.isEmpty() && addEOF) {\n        look.addOne(Token.EOF);\n        return;\n      }\n    }\n\n    if (s instanceof RuleStopState) {\n      if (ctx === null) {\n        look.addOne(Token.EPSILON);\n        return;\n      } else if (ctx.isEmpty() && addEOF) {\n        look.addOne(Token.EOF);\n        return;\n      }\n\n      if (ctx !== PredictionContext.EMPTY) {\n        const removed = calledRuleStack.contains(s.ruleIndex);\n\n        try {\n          calledRuleStack.remove(s.ruleIndex); // run thru all possible stack tops in ctx\n\n          for (let i = 0; i < ctx.length; i++) {\n            const returnState = this.atn.states[ctx.getReturnState(i)];\n\n            this._LOOK(returnState, stopState, ctx.getParent(i), look, lookBusy, calledRuleStack, seeThruPreds, addEOF);\n          }\n        } finally {\n          if (removed) {\n            calledRuleStack.add(s.ruleIndex);\n          }\n        }\n\n        return;\n      }\n    }\n\n    for (let j = 0; j < s.transitions.length; j++) {\n      const t = s.transitions[j];\n\n      if (t.constructor === RuleTransition) {\n        if (calledRuleStack.contains(t.target.ruleIndex)) {\n          continue;\n        }\n\n        const newContext = SingletonPredictionContext.create(ctx, t.followState.stateNumber);\n\n        try {\n          calledRuleStack.add(t.target.ruleIndex);\n\n          this._LOOK(t.target, stopState, newContext, look, lookBusy, calledRuleStack, seeThruPreds, addEOF);\n        } finally {\n          calledRuleStack.remove(t.target.ruleIndex);\n        }\n      } else if (t instanceof AbstractPredicateTransition) {\n        if (seeThruPreds) {\n          this._LOOK(t.target, stopState, ctx, look, lookBusy, calledRuleStack, seeThruPreds, addEOF);\n        } else {\n          look.addOne(LL1Analyzer.HIT_PRED);\n        }\n      } else if (t.isEpsilon) {\n        this._LOOK(t.target, stopState, ctx, look, lookBusy, calledRuleStack, seeThruPreds, addEOF);\n      } else if (t.constructor === WildcardTransition) {\n        look.addRange(Token.MIN_USER_TOKEN_TYPE, this.atn.maxTokenType);\n      } else {\n        let set = t.label;\n\n        if (set !== null) {\n          if (t instanceof NotSetTransition) {\n            set = set.complement(Token.MIN_USER_TOKEN_TYPE, this.atn.maxTokenType);\n          }\n\n          look.addSet(set);\n        }\n      }\n    }\n  }\n\n}\n/**\n * Special value added to the lookahead sets to indicate that we hit\n * a predicate during analysis if {@code seeThruPreds==false}.\n */\n\n\nLL1Analyzer.HIT_PRED = Token.INVALID_TYPE;\nmodule.exports = LL1Analyzer;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/LL1Analyzer.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/Lexer.js":
/*!*************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/Lexer.js ***!
  \*************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Token\n} = __webpack_require__(/*! ./Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\nconst Recognizer = __webpack_require__(/*! ./Recognizer */ \"./node_modules/antlr4/src/antlr4/Recognizer.js\");\n\nconst CommonTokenFactory = __webpack_require__(/*! ./CommonTokenFactory */ \"./node_modules/antlr4/src/antlr4/CommonTokenFactory.js\");\n\nconst {\n  RecognitionException\n} = __webpack_require__(/*! ./error/Errors */ \"./node_modules/antlr4/src/antlr4/error/Errors.js\");\n\nconst {\n  LexerNoViableAltException\n} = __webpack_require__(/*! ./error/Errors */ \"./node_modules/antlr4/src/antlr4/error/Errors.js\");\n\nclass TokenSource {}\n/**\n * A lexer is recognizer that draws input symbols from a character stream.\n * lexer grammars result in a subclass of this object. A Lexer object\n * uses simplified match() and error recovery mechanisms in the interest of speed.\n */\n\n\nclass Lexer extends Recognizer {\n  constructor(input) {\n    super();\n    this._input = input;\n    this._factory = CommonTokenFactory.DEFAULT;\n    this._tokenFactorySourcePair = [this, input];\n    this._interp = null; // child classes must populate this\n\n    /**\n     * The goal of all lexer rules/methods is to create a token object.\n     * this is an instance variable as multiple rules may collaborate to\n     * create a single token. nextToken will return this object after\n     * matching lexer rule(s). If you subclass to allow multiple token\n     * emissions, then set this to the last token to be matched or\n     * something nonnull so that the auto token emit mechanism will not\n     * emit another token.\n     */\n\n    this._token = null;\n    /**\n     * What character index in the stream did the current token start at?\n     * Needed, for example, to get the text for current token. Set at\n     * the start of nextToken.\n     */\n\n    this._tokenStartCharIndex = -1; // The line on which the first character of the token resides///\n\n    this._tokenStartLine = -1; // The character position of first character within the line///\n\n    this._tokenStartColumn = -1; // Once we see EOF on char stream, next token will be EOF.\n    // If you have DONE : EOF ; then you see DONE EOF.\n\n    this._hitEOF = false; // The channel number for the current token///\n\n    this._channel = Token.DEFAULT_CHANNEL; // The token type for the current token///\n\n    this._type = Token.INVALID_TYPE;\n    this._modeStack = [];\n    this._mode = Lexer.DEFAULT_MODE;\n    /**\n     * You can set the text for the current token to override what is in\n     * the input char buffer. Use setText() or can set this instance var.\n     */\n\n    this._text = null;\n  }\n\n  reset() {\n    // wack Lexer state variables\n    if (this._input !== null) {\n      this._input.seek(0); // rewind the input\n\n    }\n\n    this._token = null;\n    this._type = Token.INVALID_TYPE;\n    this._channel = Token.DEFAULT_CHANNEL;\n    this._tokenStartCharIndex = -1;\n    this._tokenStartColumn = -1;\n    this._tokenStartLine = -1;\n    this._text = null;\n    this._hitEOF = false;\n    this._mode = Lexer.DEFAULT_MODE;\n    this._modeStack = [];\n\n    this._interp.reset();\n  } // Return a token from this source; i.e., match a token on the char stream.\n\n\n  nextToken() {\n    if (this._input === null) {\n      throw \"nextToken requires a non-null input stream.\";\n    }\n    /**\n     * Mark start location in char stream so unbuffered streams are\n     * guaranteed at least have text of current token\n     */\n\n\n    const tokenStartMarker = this._input.mark();\n\n    try {\n      while (true) {\n        if (this._hitEOF) {\n          this.emitEOF();\n          return this._token;\n        }\n\n        this._token = null;\n        this._channel = Token.DEFAULT_CHANNEL;\n        this._tokenStartCharIndex = this._input.index;\n        this._tokenStartColumn = this._interp.column;\n        this._tokenStartLine = this._interp.line;\n        this._text = null;\n        let continueOuter = false;\n\n        while (true) {\n          this._type = Token.INVALID_TYPE;\n          let ttype = Lexer.SKIP;\n\n          try {\n            ttype = this._interp.match(this._input, this._mode);\n          } catch (e) {\n            if (e instanceof RecognitionException) {\n              this.notifyListeners(e); // report error\n\n              this.recover(e);\n            } else {\n              console.log(e.stack);\n              throw e;\n            }\n          }\n\n          if (this._input.LA(1) === Token.EOF) {\n            this._hitEOF = true;\n          }\n\n          if (this._type === Token.INVALID_TYPE) {\n            this._type = ttype;\n          }\n\n          if (this._type === Lexer.SKIP) {\n            continueOuter = true;\n            break;\n          }\n\n          if (this._type !== Lexer.MORE) {\n            break;\n          }\n        }\n\n        if (continueOuter) {\n          continue;\n        }\n\n        if (this._token === null) {\n          this.emit();\n        }\n\n        return this._token;\n      }\n    } finally {\n      // make sure we release marker after match or\n      // unbuffered char stream will keep buffering\n      this._input.release(tokenStartMarker);\n    }\n  }\n  /**\n   * Instruct the lexer to skip creating a token for current lexer rule\n   * and look for another token. nextToken() knows to keep looking when\n   * a lexer rule finishes with token set to SKIP_TOKEN. Recall that\n   * if token==null at end of any token rule, it creates one for you\n   * and emits it.\n   */\n\n\n  skip() {\n    this._type = Lexer.SKIP;\n  }\n\n  more() {\n    this._type = Lexer.MORE;\n  }\n\n  mode(m) {\n    this._mode = m;\n  }\n\n  pushMode(m) {\n    if (this._interp.debug) {\n      console.log(\"pushMode \" + m);\n    }\n\n    this._modeStack.push(this._mode);\n\n    this.mode(m);\n  }\n\n  popMode() {\n    if (this._modeStack.length === 0) {\n      throw \"Empty Stack\";\n    }\n\n    if (this._interp.debug) {\n      console.log(\"popMode back to \" + this._modeStack.slice(0, -1));\n    }\n\n    this.mode(this._modeStack.pop());\n    return this._mode;\n  }\n  /**\n   * By default does not support multiple emits per nextToken invocation\n   * for efficiency reasons. Subclass and override this method, nextToken,\n   * and getToken (to push tokens into a list and pull from that list\n   * rather than a single variable as this implementation does).\n   */\n\n\n  emitToken(token) {\n    this._token = token;\n  }\n  /**\n   * The standard method called to automatically emit a token at the\n   * outermost lexical rule. The token object should point into the\n   * char buffer start..stop. If there is a text override in 'text',\n   * use that to set the token's text. Override this method to emit\n   * custom Token objects or provide a new factory.\n   */\n\n\n  emit() {\n    const t = this._factory.create(this._tokenFactorySourcePair, this._type, this._text, this._channel, this._tokenStartCharIndex, this.getCharIndex() - 1, this._tokenStartLine, this._tokenStartColumn);\n\n    this.emitToken(t);\n    return t;\n  }\n\n  emitEOF() {\n    const cpos = this.column;\n    const lpos = this.line;\n\n    const eof = this._factory.create(this._tokenFactorySourcePair, Token.EOF, null, Token.DEFAULT_CHANNEL, this._input.index, this._input.index - 1, lpos, cpos);\n\n    this.emitToken(eof);\n    return eof;\n  } // What is the index of the current character of lookahead?///\n\n\n  getCharIndex() {\n    return this._input.index;\n  }\n  /**\n   * Return a list of all Token objects in input char stream.\n   * Forces load of all tokens. Does not include EOF token.\n   */\n\n\n  getAllTokens() {\n    const tokens = [];\n    let t = this.nextToken();\n\n    while (t.type !== Token.EOF) {\n      tokens.push(t);\n      t = this.nextToken();\n    }\n\n    return tokens;\n  }\n\n  notifyListeners(e) {\n    const start = this._tokenStartCharIndex;\n    const stop = this._input.index;\n\n    const text = this._input.getText(start, stop);\n\n    const msg = \"token recognition error at: '\" + this.getErrorDisplay(text) + \"'\";\n    const listener = this.getErrorListenerDispatch();\n    listener.syntaxError(this, null, this._tokenStartLine, this._tokenStartColumn, msg, e);\n  }\n\n  getErrorDisplay(s) {\n    const d = [];\n\n    for (let i = 0; i < s.length; i++) {\n      d.push(s[i]);\n    }\n\n    return d.join('');\n  }\n\n  getErrorDisplayForChar(c) {\n    if (c.charCodeAt(0) === Token.EOF) {\n      return \"<EOF>\";\n    } else if (c === '\\n') {\n      return \"\\\\n\";\n    } else if (c === '\\t') {\n      return \"\\\\t\";\n    } else if (c === '\\r') {\n      return \"\\\\r\";\n    } else {\n      return c;\n    }\n  }\n\n  getCharErrorDisplay(c) {\n    return \"'\" + this.getErrorDisplayForChar(c) + \"'\";\n  }\n  /**\n   * Lexers can normally match any char in it's vocabulary after matching\n   * a token, so do the easy thing and just kill a character and hope\n   * it all works out. You can instead use the rule invocation stack\n   * to do sophisticated error recovery if you are in a fragment rule.\n   */\n\n\n  recover(re) {\n    if (this._input.LA(1) !== Token.EOF) {\n      if (re instanceof LexerNoViableAltException) {\n        // skip a char and try again\n        this._interp.consume(this._input);\n      } else {\n        // TODO: Do we lose character or line position information?\n        this._input.consume();\n      }\n    }\n  }\n\n  get inputStream() {\n    return this._input;\n  }\n\n  set inputStream(input) {\n    this._input = null;\n    this._tokenFactorySourcePair = [this, this._input];\n    this.reset();\n    this._input = input;\n    this._tokenFactorySourcePair = [this, this._input];\n  }\n\n  get sourceName() {\n    return this._input.sourceName;\n  }\n\n  get type() {\n    return this._type;\n  }\n\n  set type(type) {\n    this._type = type;\n  }\n\n  get line() {\n    return this._interp.line;\n  }\n\n  set line(line) {\n    this._interp.line = line;\n  }\n\n  get column() {\n    return this._interp.column;\n  }\n\n  set column(column) {\n    this._interp.column = column;\n  }\n\n  get text() {\n    if (this._text !== null) {\n      return this._text;\n    } else {\n      return this._interp.getText(this._input);\n    }\n  }\n\n  set text(text) {\n    this._text = text;\n  }\n\n}\n\nLexer.DEFAULT_MODE = 0;\nLexer.MORE = -2;\nLexer.SKIP = -3;\nLexer.DEFAULT_TOKEN_CHANNEL = Token.DEFAULT_CHANNEL;\nLexer.HIDDEN = Token.HIDDEN_CHANNEL;\nLexer.MIN_CHAR_VALUE = 0x0000;\nLexer.MAX_CHAR_VALUE = 0x10FFFF; // Set the char stream and reset the lexer\n\nmodule.exports = Lexer;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/Lexer.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/Parser.js":
/*!**************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/Parser.js ***!
  \**************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Token\n} = __webpack_require__(/*! ./Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\nconst {\n  ParseTreeListener,\n  TerminalNode,\n  ErrorNode\n} = __webpack_require__(/*! ./tree/Tree */ \"./node_modules/antlr4/src/antlr4/tree/Tree.js\");\n\nconst Recognizer = __webpack_require__(/*! ./Recognizer */ \"./node_modules/antlr4/src/antlr4/Recognizer.js\");\n\nconst {\n  DefaultErrorStrategy\n} = __webpack_require__(/*! ./error/ErrorStrategy */ \"./node_modules/antlr4/src/antlr4/error/ErrorStrategy.js\");\n\nconst ATNDeserializer = __webpack_require__(/*! ./atn/ATNDeserializer */ \"./node_modules/antlr4/src/antlr4/atn/ATNDeserializer.js\");\n\nconst ATNDeserializationOptions = __webpack_require__(/*! ./atn/ATNDeserializationOptions */ \"./node_modules/antlr4/src/antlr4/atn/ATNDeserializationOptions.js\");\n\nconst Lexer = __webpack_require__(/*! ./Lexer */ \"./node_modules/antlr4/src/antlr4/Lexer.js\");\n\nclass TraceListener extends ParseTreeListener {\n  constructor(parser) {\n    super();\n    this.parser = parser;\n  }\n\n  enterEveryRule(ctx) {\n    console.log(\"enter   \" + this.parser.ruleNames[ctx.ruleIndex] + \", LT(1)=\" + this.parser._input.LT(1).text);\n  }\n\n  visitTerminal(node) {\n    console.log(\"consume \" + node.symbol + \" rule \" + this.parser.ruleNames[this.parser._ctx.ruleIndex]);\n  }\n\n  exitEveryRule(ctx) {\n    console.log(\"exit    \" + this.parser.ruleNames[ctx.ruleIndex] + \", LT(1)=\" + this.parser._input.LT(1).text);\n  }\n\n}\n\nclass Parser extends Recognizer {\n  /**\n   * this is all the parsing support code essentially; most of it is error\n   * recovery stuff.\n   */\n  constructor(input) {\n    super(); // The input stream.\n\n    this._input = null;\n    /**\n     * The error handling strategy for the parser. The default value is a new\n     * instance of {@link DefaultErrorStrategy}.\n     */\n\n    this._errHandler = new DefaultErrorStrategy();\n    this._precedenceStack = [];\n\n    this._precedenceStack.push(0);\n    /**\n     * The {@link ParserRuleContext} object for the currently executing rule.\n     * this is always non-null during the parsing process.\n     */\n\n\n    this._ctx = null;\n    /**\n     * Specifies whether or not the parser should construct a parse tree during\n     * the parsing process. The default value is {@code true}.\n     */\n\n    this.buildParseTrees = true;\n    /**\n     * When {@link //setTrace}{@code (true)} is called, a reference to the\n     * {@link TraceListener} is stored here so it can be easily removed in a\n     * later call to {@link //setTrace}{@code (false)}. The listener itself is\n     * implemented as a parser listener so this field is not directly used by\n     * other parser methods.\n     */\n\n    this._tracer = null;\n    /**\n     * The list of {@link ParseTreeListener} listeners registered to receive\n     * events during the parse.\n     */\n\n    this._parseListeners = null;\n    /**\n     * The number of syntax errors reported during parsing. this value is\n     * incremented each time {@link //notifyErrorListeners} is called.\n     */\n\n    this._syntaxErrors = 0;\n    this.setInputStream(input);\n  } // reset the parser's state\n\n\n  reset() {\n    if (this._input !== null) {\n      this._input.seek(0);\n    }\n\n    this._errHandler.reset(this);\n\n    this._ctx = null;\n    this._syntaxErrors = 0;\n    this.setTrace(false);\n    this._precedenceStack = [];\n\n    this._precedenceStack.push(0);\n\n    if (this._interp !== null) {\n      this._interp.reset();\n    }\n  }\n  /**\n   * Match current input symbol against {@code ttype}. If the symbol type\n   * matches, {@link ANTLRErrorStrategy//reportMatch} and {@link //consume} are\n   * called to complete the match process.\n   *\n   * <p>If the symbol type does not match,\n   * {@link ANTLRErrorStrategy//recoverInline} is called on the current error\n   * strategy to attempt recovery. If {@link //getBuildParseTree} is\n   * {@code true} and the token index of the symbol returned by\n   * {@link ANTLRErrorStrategy//recoverInline} is -1, the symbol is added to\n   * the parse tree by calling {@link ParserRuleContext//addErrorNode}.</p>\n   *\n   * @param ttype the token type to match\n   * @return the matched symbol\n   * @throws RecognitionException if the current input symbol did not match\n   * {@code ttype} and the error strategy could not recover from the\n   * mismatched symbol\n   */\n\n\n  match(ttype) {\n    let t = this.getCurrentToken();\n\n    if (t.type === ttype) {\n      this._errHandler.reportMatch(this);\n\n      this.consume();\n    } else {\n      t = this._errHandler.recoverInline(this);\n\n      if (this.buildParseTrees && t.tokenIndex === -1) {\n        // we must have conjured up a new token during single token\n        // insertion\n        // if it's not the current symbol\n        this._ctx.addErrorNode(t);\n      }\n    }\n\n    return t;\n  }\n  /**\n   * Match current input symbol as a wildcard. If the symbol type matches\n   * (i.e. has a value greater than 0), {@link ANTLRErrorStrategy//reportMatch}\n   * and {@link //consume} are called to complete the match process.\n   *\n   * <p>If the symbol type does not match,\n   * {@link ANTLRErrorStrategy//recoverInline} is called on the current error\n   * strategy to attempt recovery. If {@link //getBuildParseTree} is\n   * {@code true} and the token index of the symbol returned by\n   * {@link ANTLRErrorStrategy//recoverInline} is -1, the symbol is added to\n   * the parse tree by calling {@link ParserRuleContext//addErrorNode}.</p>\n   *\n   * @return the matched symbol\n   * @throws RecognitionException if the current input symbol did not match\n   * a wildcard and the error strategy could not recover from the mismatched\n   * symbol\n   */\n\n\n  matchWildcard() {\n    let t = this.getCurrentToken();\n\n    if (t.type > 0) {\n      this._errHandler.reportMatch(this);\n\n      this.consume();\n    } else {\n      t = this._errHandler.recoverInline(this);\n\n      if (this._buildParseTrees && t.tokenIndex === -1) {\n        // we must have conjured up a new token during single token\n        // insertion\n        // if it's not the current symbol\n        this._ctx.addErrorNode(t);\n      }\n    }\n\n    return t;\n  }\n\n  getParseListeners() {\n    return this._parseListeners || [];\n  }\n  /**\n   * Registers {@code listener} to receive events during the parsing process.\n   *\n   * <p>To support output-preserving grammar transformations (including but not\n   * limited to left-recursion removal, automated left-factoring, and\n   * optimized code generation), calls to listener methods during the parse\n   * may differ substantially from calls made by\n   * {@link ParseTreeWalker//DEFAULT} used after the parse is complete. In\n   * particular, rule entry and exit events may occur in a different order\n   * during the parse than after the parser. In addition, calls to certain\n   * rule entry methods may be omitted.</p>\n   *\n   * <p>With the following specific exceptions, calls to listener events are\n   * <em>deterministic</em>, i.e. for identical input the calls to listener\n   * methods will be the same.</p>\n   *\n   * <ul>\n   * <li>Alterations to the grammar used to generate code may change the\n   * behavior of the listener calls.</li>\n   * <li>Alterations to the command line options passed to ANTLR 4 when\n   * generating the parser may change the behavior of the listener calls.</li>\n   * <li>Changing the version of the ANTLR Tool used to generate the parser\n   * may change the behavior of the listener calls.</li>\n   * </ul>\n   *\n   * @param listener the listener to add\n   *\n   * @throws NullPointerException if {@code} listener is {@code null}\n   */\n\n\n  addParseListener(listener) {\n    if (listener === null) {\n      throw \"listener\";\n    }\n\n    if (this._parseListeners === null) {\n      this._parseListeners = [];\n    }\n\n    this._parseListeners.push(listener);\n  }\n  /**\n   * Remove {@code listener} from the list of parse listeners.\n   *\n   * <p>If {@code listener} is {@code null} or has not been added as a parse\n   * listener, this method does nothing.</p>\n   * @param listener the listener to remove\n   */\n\n\n  removeParseListener(listener) {\n    if (this._parseListeners !== null) {\n      const idx = this._parseListeners.indexOf(listener);\n\n      if (idx >= 0) {\n        this._parseListeners.splice(idx, 1);\n      }\n\n      if (this._parseListeners.length === 0) {\n        this._parseListeners = null;\n      }\n    }\n  } // Remove all parse listeners.\n\n\n  removeParseListeners() {\n    this._parseListeners = null;\n  } // Notify any parse listeners of an enter rule event.\n\n\n  triggerEnterRuleEvent() {\n    if (this._parseListeners !== null) {\n      const ctx = this._ctx;\n\n      this._parseListeners.forEach(function (listener) {\n        listener.enterEveryRule(ctx);\n        ctx.enterRule(listener);\n      });\n    }\n  }\n  /**\n   * Notify any parse listeners of an exit rule event.\n   * @see //addParseListener\n   */\n\n\n  triggerExitRuleEvent() {\n    if (this._parseListeners !== null) {\n      // reverse order walk of listeners\n      const ctx = this._ctx;\n\n      this._parseListeners.slice(0).reverse().forEach(function (listener) {\n        ctx.exitRule(listener);\n        listener.exitEveryRule(ctx);\n      });\n    }\n  }\n\n  getTokenFactory() {\n    return this._input.tokenSource._factory;\n  } // Tell our token source and error strategy about a new way to create tokens.\n\n\n  setTokenFactory(factory) {\n    this._input.tokenSource._factory = factory;\n  }\n  /**\n   * The ATN with bypass alternatives is expensive to create so we create it\n   * lazily.\n   *\n   * @throws UnsupportedOperationException if the current parser does not\n   * implement the {@link //getSerializedATN()} method.\n   */\n\n\n  getATNWithBypassAlts() {\n    const serializedAtn = this.getSerializedATN();\n\n    if (serializedAtn === null) {\n      throw \"The current parser does not support an ATN with bypass alternatives.\";\n    }\n\n    let result = this.bypassAltsAtnCache[serializedAtn];\n\n    if (result === null) {\n      const deserializationOptions = new ATNDeserializationOptions();\n      deserializationOptions.generateRuleBypassTransitions = true;\n      result = new ATNDeserializer(deserializationOptions).deserialize(serializedAtn);\n      this.bypassAltsAtnCache[serializedAtn] = result;\n    }\n\n    return result;\n  }\n  /**\n   * The preferred method of getting a tree pattern. For example, here's a\n   * sample use:\n   *\n   * <pre>\n   * ParseTree t = parser.expr();\n   * ParseTreePattern p = parser.compileParseTreePattern(\"&lt;ID&gt;+0\",\n   * MyParser.RULE_expr);\n   * ParseTreeMatch m = p.match(t);\n   * String id = m.get(\"ID\");\n   * </pre>\n   */\n\n\n  compileParseTreePattern(pattern, patternRuleIndex, lexer) {\n    lexer = lexer || null;\n\n    if (lexer === null) {\n      if (this.getTokenStream() !== null) {\n        const tokenSource = this.getTokenStream().tokenSource;\n\n        if (tokenSource instanceof Lexer) {\n          lexer = tokenSource;\n        }\n      }\n    }\n\n    if (lexer === null) {\n      throw \"Parser can't discover a lexer to use\";\n    }\n\n    const m = new ParseTreePatternMatcher(lexer, this);\n    return m.compile(pattern, patternRuleIndex);\n  }\n\n  getInputStream() {\n    return this.getTokenStream();\n  }\n\n  setInputStream(input) {\n    this.setTokenStream(input);\n  }\n\n  getTokenStream() {\n    return this._input;\n  } // Set the token stream and reset the parser.\n\n\n  setTokenStream(input) {\n    this._input = null;\n    this.reset();\n    this._input = input;\n  }\n  /**\n   * Match needs to return the current input symbol, which gets put\n   * into the label for the associated token ref; e.g., x=ID.\n   */\n\n\n  getCurrentToken() {\n    return this._input.LT(1);\n  }\n\n  notifyErrorListeners(msg, offendingToken, err) {\n    offendingToken = offendingToken || null;\n    err = err || null;\n\n    if (offendingToken === null) {\n      offendingToken = this.getCurrentToken();\n    }\n\n    this._syntaxErrors += 1;\n    const line = offendingToken.line;\n    const column = offendingToken.column;\n    const listener = this.getErrorListenerDispatch();\n    listener.syntaxError(this, offendingToken, line, column, msg, err);\n  }\n  /**\n   * Consume and return the {@linkplain //getCurrentToken current symbol}.\n   *\n   * <p>E.g., given the following input with {@code A} being the current\n   * lookahead symbol, this function moves the cursor to {@code B} and returns\n   * {@code A}.</p>\n   *\n   * <pre>\n   * A B\n   * ^\n   * </pre>\n   *\n   * If the parser is not in error recovery mode, the consumed symbol is added\n   * to the parse tree using {@link ParserRuleContext//addChild(Token)}, and\n   * {@link ParseTreeListener//visitTerminal} is called on any parse listeners.\n   * If the parser <em>is</em> in error recovery mode, the consumed symbol is\n   * added to the parse tree using\n   * {@link ParserRuleContext//addErrorNode(Token)}, and\n   * {@link ParseTreeListener//visitErrorNode} is called on any parse\n   * listeners.\n   */\n\n\n  consume() {\n    const o = this.getCurrentToken();\n\n    if (o.type !== Token.EOF) {\n      this.getInputStream().consume();\n    }\n\n    const hasListener = this._parseListeners !== null && this._parseListeners.length > 0;\n\n    if (this.buildParseTrees || hasListener) {\n      let node;\n\n      if (this._errHandler.inErrorRecoveryMode(this)) {\n        node = this._ctx.addErrorNode(o);\n      } else {\n        node = this._ctx.addTokenNode(o);\n      }\n\n      node.invokingState = this.state;\n\n      if (hasListener) {\n        this._parseListeners.forEach(function (listener) {\n          if (node instanceof ErrorNode || node.isErrorNode !== undefined && node.isErrorNode()) {\n            listener.visitErrorNode(node);\n          } else if (node instanceof TerminalNode) {\n            listener.visitTerminal(node);\n          }\n        });\n      }\n    }\n\n    return o;\n  }\n\n  addContextToParseTree() {\n    // add current context to parent if we have a parent\n    if (this._ctx.parentCtx !== null) {\n      this._ctx.parentCtx.addChild(this._ctx);\n    }\n  }\n  /**\n   * Always called by generated parsers upon entry to a rule. Access field\n   * {@link //_ctx} get the current context.\n   */\n\n\n  enterRule(localctx, state, ruleIndex) {\n    this.state = state;\n    this._ctx = localctx;\n    this._ctx.start = this._input.LT(1);\n\n    if (this.buildParseTrees) {\n      this.addContextToParseTree();\n    }\n\n    this.triggerEnterRuleEvent();\n  }\n\n  exitRule() {\n    this._ctx.stop = this._input.LT(-1); // trigger event on _ctx, before it reverts to parent\n\n    this.triggerExitRuleEvent();\n    this.state = this._ctx.invokingState;\n    this._ctx = this._ctx.parentCtx;\n  }\n\n  enterOuterAlt(localctx, altNum) {\n    localctx.setAltNumber(altNum); // if we have new localctx, make sure we replace existing ctx\n    // that is previous child of parse tree\n\n    if (this.buildParseTrees && this._ctx !== localctx) {\n      if (this._ctx.parentCtx !== null) {\n        this._ctx.parentCtx.removeLastChild();\n\n        this._ctx.parentCtx.addChild(localctx);\n      }\n    }\n\n    this._ctx = localctx;\n  }\n  /**\n   * Get the precedence level for the top-most precedence rule.\n   *\n   * @return The precedence level for the top-most precedence rule, or -1 if\n   * the parser context is not nested within a precedence rule.\n   */\n\n\n  getPrecedence() {\n    if (this._precedenceStack.length === 0) {\n      return -1;\n    } else {\n      return this._precedenceStack[this._precedenceStack.length - 1];\n    }\n  }\n\n  enterRecursionRule(localctx, state, ruleIndex, precedence) {\n    this.state = state;\n\n    this._precedenceStack.push(precedence);\n\n    this._ctx = localctx;\n    this._ctx.start = this._input.LT(1);\n    this.triggerEnterRuleEvent(); // simulates rule entry for left-recursive rules\n  } // Like {@link //enterRule} but for recursive rules.\n\n\n  pushNewRecursionContext(localctx, state, ruleIndex) {\n    const previous = this._ctx;\n    previous.parentCtx = localctx;\n    previous.invokingState = state;\n    previous.stop = this._input.LT(-1);\n    this._ctx = localctx;\n    this._ctx.start = previous.start;\n\n    if (this.buildParseTrees) {\n      this._ctx.addChild(previous);\n    }\n\n    this.triggerEnterRuleEvent(); // simulates rule entry for left-recursive rules\n  }\n\n  unrollRecursionContexts(parentCtx) {\n    this._precedenceStack.pop();\n\n    this._ctx.stop = this._input.LT(-1);\n    const retCtx = this._ctx; // save current ctx (return value)\n    // unroll so _ctx is as it was before call to recursive method\n\n    const parseListeners = this.getParseListeners();\n\n    if (parseListeners !== null && parseListeners.length > 0) {\n      while (this._ctx !== parentCtx) {\n        this.triggerExitRuleEvent();\n        this._ctx = this._ctx.parentCtx;\n      }\n    } else {\n      this._ctx = parentCtx;\n    } // hook into tree\n\n\n    retCtx.parentCtx = parentCtx;\n\n    if (this.buildParseTrees && parentCtx !== null) {\n      // add return ctx into invoking rule's tree\n      parentCtx.addChild(retCtx);\n    }\n  }\n\n  getInvokingContext(ruleIndex) {\n    let ctx = this._ctx;\n\n    while (ctx !== null) {\n      if (ctx.ruleIndex === ruleIndex) {\n        return ctx;\n      }\n\n      ctx = ctx.parentCtx;\n    }\n\n    return null;\n  }\n\n  precpred(localctx, precedence) {\n    return precedence >= this._precedenceStack[this._precedenceStack.length - 1];\n  }\n\n  inContext(context) {\n    // TODO: useful in parser?\n    return false;\n  }\n  /**\n   * Checks whether or not {@code symbol} can follow the current state in the\n   * ATN. The behavior of this method is equivalent to the following, but is\n   * implemented such that the complete context-sensitive follow set does not\n   * need to be explicitly constructed.\n   *\n   * <pre>\n   * return getExpectedTokens().contains(symbol);\n   * </pre>\n   *\n   * @param symbol the symbol type to check\n   * @return {@code true} if {@code symbol} can follow the current state in\n   * the ATN, otherwise {@code false}.\n   */\n\n\n  isExpectedToken(symbol) {\n    const atn = this._interp.atn;\n    let ctx = this._ctx;\n    const s = atn.states[this.state];\n    let following = atn.nextTokens(s);\n\n    if (following.contains(symbol)) {\n      return true;\n    }\n\n    if (!following.contains(Token.EPSILON)) {\n      return false;\n    }\n\n    while (ctx !== null && ctx.invokingState >= 0 && following.contains(Token.EPSILON)) {\n      const invokingState = atn.states[ctx.invokingState];\n      const rt = invokingState.transitions[0];\n      following = atn.nextTokens(rt.followState);\n\n      if (following.contains(symbol)) {\n        return true;\n      }\n\n      ctx = ctx.parentCtx;\n    }\n\n    if (following.contains(Token.EPSILON) && symbol === Token.EOF) {\n      return true;\n    } else {\n      return false;\n    }\n  }\n  /**\n   * Computes the set of input symbols which could follow the current parser\n   * state and context, as given by {@link //getState} and {@link //getContext},\n   * respectively.\n   *\n   * @see ATN//getExpectedTokens(int, RuleContext)\n   */\n\n\n  getExpectedTokens() {\n    return this._interp.atn.getExpectedTokens(this.state, this._ctx);\n  }\n\n  getExpectedTokensWithinCurrentRule() {\n    const atn = this._interp.atn;\n    const s = atn.states[this.state];\n    return atn.nextTokens(s);\n  } // Get a rule's index (i.e., {@code RULE_ruleName} field) or -1 if not found.\n\n\n  getRuleIndex(ruleName) {\n    const ruleIndex = this.getRuleIndexMap()[ruleName];\n\n    if (ruleIndex !== null) {\n      return ruleIndex;\n    } else {\n      return -1;\n    }\n  }\n  /**\n   * Return List&lt;String&gt; of the rule names in your parser instance\n   * leading up to a call to the current rule. You could override if\n   * you want more details such as the file/line info of where\n   * in the ATN a rule is invoked.\n   *\n   * this is very useful for error messages.\n   */\n\n\n  getRuleInvocationStack(p) {\n    p = p || null;\n\n    if (p === null) {\n      p = this._ctx;\n    }\n\n    const stack = [];\n\n    while (p !== null) {\n      // compute what follows who invoked us\n      const ruleIndex = p.ruleIndex;\n\n      if (ruleIndex < 0) {\n        stack.push(\"n/a\");\n      } else {\n        stack.push(this.ruleNames[ruleIndex]);\n      }\n\n      p = p.parentCtx;\n    }\n\n    return stack;\n  } // For debugging and other purposes.\n\n\n  getDFAStrings() {\n    return this._interp.decisionToDFA.toString();\n  } // For debugging and other purposes.\n\n\n  dumpDFA() {\n    let seenOne = false;\n\n    for (let i = 0; i < this._interp.decisionToDFA.length; i++) {\n      const dfa = this._interp.decisionToDFA[i];\n\n      if (dfa.states.length > 0) {\n        if (seenOne) {\n          console.log();\n        }\n\n        this.printer.println(\"Decision \" + dfa.decision + \":\");\n        this.printer.print(dfa.toString(this.literalNames, this.symbolicNames));\n        seenOne = true;\n      }\n    }\n  }\n  /*\n  \t\"\t\t\tprinter = function() {\\r\\n\" +\n  \t\"\t\t\t\tthis.println = function(s) { document.getElementById('output') += s + '\\\\n'; }\\r\\n\" +\n  \t\"\t\t\t\tthis.print = function(s) { document.getElementById('output') += s; }\\r\\n\" +\n  \t\"\t\t\t};\\r\\n\" +\n  \t*/\n\n\n  getSourceName() {\n    return this._input.sourceName;\n  }\n  /**\n   * During a parse is sometimes useful to listen in on the rule entry and exit\n   * events as well as token matches. this is for quick and dirty debugging.\n   */\n\n\n  setTrace(trace) {\n    if (!trace) {\n      this.removeParseListener(this._tracer);\n      this._tracer = null;\n    } else {\n      if (this._tracer !== null) {\n        this.removeParseListener(this._tracer);\n      }\n\n      this._tracer = new TraceListener(this);\n      this.addParseListener(this._tracer);\n    }\n  }\n\n}\n/**\n * this field maps from the serialized ATN string to the deserialized {@link\n * ATN} with\n * bypass alternatives.\n *\n * @see ATNDeserializationOptions//isGenerateRuleBypassTransitions()\n */\n\n\nParser.bypassAltsAtnCache = {};\nmodule.exports = Parser;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/Parser.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/ParserRuleContext.js":
/*!*************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/ParserRuleContext.js ***!
  \*************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst RuleContext = __webpack_require__(/*! ./RuleContext */ \"./node_modules/antlr4/src/antlr4/RuleContext.js\");\n\nconst Tree = __webpack_require__(/*! ./tree/Tree */ \"./node_modules/antlr4/src/antlr4/tree/Tree.js\");\n\nconst INVALID_INTERVAL = Tree.INVALID_INTERVAL;\nconst TerminalNode = Tree.TerminalNode;\nconst TerminalNodeImpl = Tree.TerminalNodeImpl;\nconst ErrorNodeImpl = Tree.ErrorNodeImpl;\n\nconst Interval = (__webpack_require__(/*! ./IntervalSet */ \"./node_modules/antlr4/src/antlr4/IntervalSet.js\").Interval);\n/**\n * A rule invocation record for parsing.\n *\n *  Contains all of the information about the current rule not stored in the\n *  RuleContext. It handles parse tree children list, Any ATN state\n *  tracing, and the default values available for rule indications:\n *  start, stop, rule index, current alt number, current\n *  ATN state.\n *\n *  Subclasses made for each rule and grammar track the parameters,\n *  return values, locals, and labels specific to that rule. These\n *  are the objects that are returned from rules.\n *\n *  Note text is not an actual field of a rule return value; it is computed\n *  from start and stop using the input stream's toString() method.  I\n *  could add a ctor to this so that we can pass in and store the input\n *  stream, but I'm not sure we want to do that.  It would seem to be undefined\n *  to get the .text property anyway if the rule matches tokens from multiple\n *  input streams.\n *\n *  I do not use getters for fields of objects that are used simply to\n *  group values such as this aggregate.  The getters/setters are there to\n *  satisfy the superclass interface.\n */\n\n\nclass ParserRuleContext extends RuleContext {\n  constructor(parent, invokingStateNumber) {\n    parent = parent || null;\n    invokingStateNumber = invokingStateNumber || null;\n    super(parent, invokingStateNumber);\n    this.ruleIndex = -1;\n    /**\n     * If we are debugging or building a parse tree for a visitor,\n     * we need to track all of the tokens and rule invocations associated\n     * with this rule's context. This is empty for parsing w/o tree constr.\n     * operation because we don't the need to track the details about\n     * how we parse this rule.\n     */\n\n    this.children = null;\n    this.start = null;\n    this.stop = null;\n    /**\n     * The exception that forced this rule to return. If the rule successfully\n     * completed, this is {@code null}.\n     */\n\n    this.exception = null;\n  } // COPY a ctx (I'm deliberately not using copy constructor)\n\n\n  copyFrom(ctx) {\n    // from RuleContext\n    this.parentCtx = ctx.parentCtx;\n    this.invokingState = ctx.invokingState;\n    this.children = null;\n    this.start = ctx.start;\n    this.stop = ctx.stop; // copy any error nodes to alt label node\n\n    if (ctx.children) {\n      this.children = []; // reset parent pointer for any error nodes\n\n      ctx.children.map(function (child) {\n        if (child instanceof ErrorNodeImpl) {\n          this.children.push(child);\n          child.parentCtx = this;\n        }\n      }, this);\n    }\n  } // Double dispatch methods for listeners\n\n\n  enterRule(listener) {}\n\n  exitRule(listener) {} // Does not set parent link; other add methods do that\n\n\n  addChild(child) {\n    if (this.children === null) {\n      this.children = [];\n    }\n\n    this.children.push(child);\n    return child;\n  }\n  /** Used by enterOuterAlt to toss out a RuleContext previously added as\n   * we entered a rule. If we have // label, we will need to remove\n   * generic ruleContext object.\n   */\n\n\n  removeLastChild() {\n    if (this.children !== null) {\n      this.children.pop();\n    }\n  }\n\n  addTokenNode(token) {\n    const node = new TerminalNodeImpl(token);\n    this.addChild(node);\n    node.parentCtx = this;\n    return node;\n  }\n\n  addErrorNode(badToken) {\n    const node = new ErrorNodeImpl(badToken);\n    this.addChild(node);\n    node.parentCtx = this;\n    return node;\n  }\n\n  getChild(i, type) {\n    type = type || null;\n\n    if (this.children === null || i < 0 || i >= this.children.length) {\n      return null;\n    }\n\n    if (type === null) {\n      return this.children[i];\n    } else {\n      for (let j = 0; j < this.children.length; j++) {\n        const child = this.children[j];\n\n        if (child instanceof type) {\n          if (i === 0) {\n            return child;\n          } else {\n            i -= 1;\n          }\n        }\n      }\n\n      return null;\n    }\n  }\n\n  getToken(ttype, i) {\n    if (this.children === null || i < 0 || i >= this.children.length) {\n      return null;\n    }\n\n    for (let j = 0; j < this.children.length; j++) {\n      const child = this.children[j];\n\n      if (child instanceof TerminalNode) {\n        if (child.symbol.type === ttype) {\n          if (i === 0) {\n            return child;\n          } else {\n            i -= 1;\n          }\n        }\n      }\n    }\n\n    return null;\n  }\n\n  getTokens(ttype) {\n    if (this.children === null) {\n      return [];\n    } else {\n      const tokens = [];\n\n      for (let j = 0; j < this.children.length; j++) {\n        const child = this.children[j];\n\n        if (child instanceof TerminalNode) {\n          if (child.symbol.type === ttype) {\n            tokens.push(child);\n          }\n        }\n      }\n\n      return tokens;\n    }\n  }\n\n  getTypedRuleContext(ctxType, i) {\n    return this.getChild(i, ctxType);\n  }\n\n  getTypedRuleContexts(ctxType) {\n    if (this.children === null) {\n      return [];\n    } else {\n      const contexts = [];\n\n      for (let j = 0; j < this.children.length; j++) {\n        const child = this.children[j];\n\n        if (child instanceof ctxType) {\n          contexts.push(child);\n        }\n      }\n\n      return contexts;\n    }\n  }\n\n  getChildCount() {\n    if (this.children === null) {\n      return 0;\n    } else {\n      return this.children.length;\n    }\n  }\n\n  getSourceInterval() {\n    if (this.start === null || this.stop === null) {\n      return INVALID_INTERVAL;\n    } else {\n      return new Interval(this.start.tokenIndex, this.stop.tokenIndex);\n    }\n  }\n\n}\n\nRuleContext.EMPTY = new ParserRuleContext();\n\nclass InterpreterRuleContext extends ParserRuleContext {\n  constructor(parent, invokingStateNumber, ruleIndex) {\n    super(parent, invokingStateNumber);\n    this.ruleIndex = ruleIndex;\n  }\n\n}\n\nmodule.exports = ParserRuleContext;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/ParserRuleContext.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/PredictionContext.js":
/*!*************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/PredictionContext.js ***!
  \*************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst RuleContext = __webpack_require__(/*! ./RuleContext */ \"./node_modules/antlr4/src/antlr4/RuleContext.js\");\n\nconst {\n  Hash,\n  Map,\n  equalArrays\n} = __webpack_require__(/*! ./Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n\nclass PredictionContext {\n  constructor(cachedHashCode) {\n    this.cachedHashCode = cachedHashCode;\n  }\n  /**\n   * Stores the computed hash code of this {@link PredictionContext}. The hash\n   * code is computed in parts to match the following reference algorithm.\n   *\n   * <pre>\n   * private int referenceHashCode() {\n   * int hash = {@link MurmurHash//initialize MurmurHash.initialize}({@link\n   * //INITIAL_HASH});\n   *\n   * for (int i = 0; i &lt; {@link //size()}; i++) {\n   * hash = {@link MurmurHash//update MurmurHash.update}(hash, {@link //getParent\n   * getParent}(i));\n   * }\n   *\n   * for (int i = 0; i &lt; {@link //size()}; i++) {\n   * hash = {@link MurmurHash//update MurmurHash.update}(hash, {@link\n   * //getReturnState getReturnState}(i));\n   * }\n   *\n   * hash = {@link MurmurHash//finish MurmurHash.finish}(hash, 2// {@link\n   * //size()});\n   * return hash;\n   * }\n   * </pre>\n   * This means only the {@link //EMPTY} context is in set.\n   */\n\n\n  isEmpty() {\n    return this === PredictionContext.EMPTY;\n  }\n\n  hasEmptyPath() {\n    return this.getReturnState(this.length - 1) === PredictionContext.EMPTY_RETURN_STATE;\n  }\n\n  hashCode() {\n    return this.cachedHashCode;\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.cachedHashCode);\n  }\n\n}\n/**\n * Represents {@code $} in local context prediction, which means wildcard.\n * {@code//+x =//}.\n */\n\n\nPredictionContext.EMPTY = null;\n/**\n * Represents {@code $} in an array in full context mode, when {@code $}\n * doesn't mean wildcard: {@code $ + x = [$,x]}. Here,\n * {@code $} = {@link //EMPTY_RETURN_STATE}.\n */\n\nPredictionContext.EMPTY_RETURN_STATE = 0x7FFFFFFF;\nPredictionContext.globalNodeCount = 1;\nPredictionContext.id = PredictionContext.globalNodeCount;\n/*\nfunction calculateHashString(parent, returnState) {\n\treturn \"\" + parent + returnState;\n}\n*/\n\n/**\n * Used to cache {@link PredictionContext} objects. Its used for the shared\n * context cash associated with contexts in DFA states. This cache\n * can be used for both lexers and parsers.\n */\n\nclass PredictionContextCache {\n  constructor() {\n    this.cache = new Map();\n  }\n  /**\n   * Add a context to the cache and return it. If the context already exists,\n   * return that one instead and do not add a new context to the cache.\n   * Protect shared cache from unsafe thread access.\n   */\n\n\n  add(ctx) {\n    if (ctx === PredictionContext.EMPTY) {\n      return PredictionContext.EMPTY;\n    }\n\n    const existing = this.cache.get(ctx) || null;\n\n    if (existing !== null) {\n      return existing;\n    }\n\n    this.cache.put(ctx, ctx);\n    return ctx;\n  }\n\n  get(ctx) {\n    return this.cache.get(ctx) || null;\n  }\n\n  get length() {\n    return this.cache.length;\n  }\n\n}\n\nclass SingletonPredictionContext extends PredictionContext {\n  constructor(parent, returnState) {\n    let hashCode = 0;\n    const hash = new Hash();\n\n    if (parent !== null) {\n      hash.update(parent, returnState);\n    } else {\n      hash.update(1);\n    }\n\n    hashCode = hash.finish();\n    super(hashCode);\n    this.parentCtx = parent;\n    this.returnState = returnState;\n  }\n\n  getParent(index) {\n    return this.parentCtx;\n  }\n\n  getReturnState(index) {\n    return this.returnState;\n  }\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof SingletonPredictionContext)) {\n      return false;\n    } else if (this.hashCode() !== other.hashCode()) {\n      return false; // can't be same if hash is different\n    } else {\n      if (this.returnState !== other.returnState) return false;else if (this.parentCtx == null) return other.parentCtx == null;else return this.parentCtx.equals(other.parentCtx);\n    }\n  }\n\n  toString() {\n    const up = this.parentCtx === null ? \"\" : this.parentCtx.toString();\n\n    if (up.length === 0) {\n      if (this.returnState === PredictionContext.EMPTY_RETURN_STATE) {\n        return \"$\";\n      } else {\n        return \"\" + this.returnState;\n      }\n    } else {\n      return \"\" + this.returnState + \" \" + up;\n    }\n  }\n\n  get length() {\n    return 1;\n  }\n\n  static create(parent, returnState) {\n    if (returnState === PredictionContext.EMPTY_RETURN_STATE && parent === null) {\n      // someone can pass in the bits of an array ctx that mean $\n      return PredictionContext.EMPTY;\n    } else {\n      return new SingletonPredictionContext(parent, returnState);\n    }\n  }\n\n}\n\nclass EmptyPredictionContext extends SingletonPredictionContext {\n  constructor() {\n    super(null, PredictionContext.EMPTY_RETURN_STATE);\n  }\n\n  isEmpty() {\n    return true;\n  }\n\n  getParent(index) {\n    return null;\n  }\n\n  getReturnState(index) {\n    return this.returnState;\n  }\n\n  equals(other) {\n    return this === other;\n  }\n\n  toString() {\n    return \"$\";\n  }\n\n}\n\nPredictionContext.EMPTY = new EmptyPredictionContext();\n\nclass ArrayPredictionContext extends PredictionContext {\n  constructor(parents, returnStates) {\n    /**\n     * Parent can be null only if full ctx mode and we make an array\n     * from {@link //EMPTY} and non-empty. We merge {@link //EMPTY} by using\n     * null parent and\n     * returnState == {@link //EMPTY_RETURN_STATE}.\n     */\n    const h = new Hash();\n    h.update(parents, returnStates);\n    const hashCode = h.finish();\n    super(hashCode);\n    this.parents = parents;\n    this.returnStates = returnStates;\n    return this;\n  }\n\n  isEmpty() {\n    // since EMPTY_RETURN_STATE can only appear in the last position, we\n    // don't need to verify that size==1\n    return this.returnStates[0] === PredictionContext.EMPTY_RETURN_STATE;\n  }\n\n  getParent(index) {\n    return this.parents[index];\n  }\n\n  getReturnState(index) {\n    return this.returnStates[index];\n  }\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof ArrayPredictionContext)) {\n      return false;\n    } else if (this.hashCode() !== other.hashCode()) {\n      return false; // can't be same if hash is different\n    } else {\n      return equalArrays(this.returnStates, other.returnStates) && equalArrays(this.parents, other.parents);\n    }\n  }\n\n  toString() {\n    if (this.isEmpty()) {\n      return \"[]\";\n    } else {\n      let s = \"[\";\n\n      for (let i = 0; i < this.returnStates.length; i++) {\n        if (i > 0) {\n          s = s + \", \";\n        }\n\n        if (this.returnStates[i] === PredictionContext.EMPTY_RETURN_STATE) {\n          s = s + \"$\";\n          continue;\n        }\n\n        s = s + this.returnStates[i];\n\n        if (this.parents[i] !== null) {\n          s = s + \" \" + this.parents[i];\n        } else {\n          s = s + \"null\";\n        }\n      }\n\n      return s + \"]\";\n    }\n  }\n\n  get length() {\n    return this.returnStates.length;\n  }\n\n}\n/**\n * Convert a {@link RuleContext} tree to a {@link PredictionContext} graph.\n * Return {@link //EMPTY} if {@code outerContext} is empty or null.\n */\n\n\nfunction predictionContextFromRuleContext(atn, outerContext) {\n  if (outerContext === undefined || outerContext === null) {\n    outerContext = RuleContext.EMPTY;\n  } // if we are in RuleContext of start rule, s, then PredictionContext\n  // is EMPTY. Nobody called us. (if we are empty, return empty)\n\n\n  if (outerContext.parentCtx === null || outerContext === RuleContext.EMPTY) {\n    return PredictionContext.EMPTY;\n  } // If we have a parent, convert it to a PredictionContext graph\n\n\n  const parent = predictionContextFromRuleContext(atn, outerContext.parentCtx);\n  const state = atn.states[outerContext.invokingState];\n  const transition = state.transitions[0];\n  return SingletonPredictionContext.create(parent, transition.followState.stateNumber);\n}\n/*\nfunction calculateListsHashString(parents, returnStates) {\n\tconst s = \"\";\n\tparents.map(function(p) {\n\t\ts = s + p;\n\t});\n\treturnStates.map(function(r) {\n\t\ts = s + r;\n\t});\n\treturn s;\n}\n*/\n\n\nfunction merge(a, b, rootIsWildcard, mergeCache) {\n  // share same graph if both same\n  if (a === b) {\n    return a;\n  }\n\n  if (a instanceof SingletonPredictionContext && b instanceof SingletonPredictionContext) {\n    return mergeSingletons(a, b, rootIsWildcard, mergeCache);\n  } // At least one of a or b is array\n  // If one is $ and rootIsWildcard, return $ as// wildcard\n\n\n  if (rootIsWildcard) {\n    if (a instanceof EmptyPredictionContext) {\n      return a;\n    }\n\n    if (b instanceof EmptyPredictionContext) {\n      return b;\n    }\n  } // convert singleton so both are arrays to normalize\n\n\n  if (a instanceof SingletonPredictionContext) {\n    a = new ArrayPredictionContext([a.getParent()], [a.returnState]);\n  }\n\n  if (b instanceof SingletonPredictionContext) {\n    b = new ArrayPredictionContext([b.getParent()], [b.returnState]);\n  }\n\n  return mergeArrays(a, b, rootIsWildcard, mergeCache);\n}\n/**\n * Merge two {@link SingletonPredictionContext} instances.\n *\n * <p>Stack tops equal, parents merge is same; return left graph.<br>\n * <embed src=\"images/SingletonMerge_SameRootSamePar.svg\"\n * type=\"image/svg+xml\"/></p>\n *\n * <p>Same stack top, parents differ; merge parents giving array node, then\n * remainders of those graphs. A new root node is created to point to the\n * merged parents.<br>\n * <embed src=\"images/SingletonMerge_SameRootDiffPar.svg\"\n * type=\"image/svg+xml\"/></p>\n *\n * <p>Different stack tops pointing to same parent. Make array node for the\n * root where both element in the root point to the same (original)\n * parent.<br>\n * <embed src=\"images/SingletonMerge_DiffRootSamePar.svg\"\n * type=\"image/svg+xml\"/></p>\n *\n * <p>Different stack tops pointing to different parents. Make array node for\n * the root where each element points to the corresponding original\n * parent.<br>\n * <embed src=\"images/SingletonMerge_DiffRootDiffPar.svg\"\n * type=\"image/svg+xml\"/></p>\n *\n * @param a the first {@link SingletonPredictionContext}\n * @param b the second {@link SingletonPredictionContext}\n * @param rootIsWildcard {@code true} if this is a local-context merge,\n * otherwise false to indicate a full-context merge\n * @param mergeCache\n */\n\n\nfunction mergeSingletons(a, b, rootIsWildcard, mergeCache) {\n  if (mergeCache !== null) {\n    let previous = mergeCache.get(a, b);\n\n    if (previous !== null) {\n      return previous;\n    }\n\n    previous = mergeCache.get(b, a);\n\n    if (previous !== null) {\n      return previous;\n    }\n  }\n\n  const rootMerge = mergeRoot(a, b, rootIsWildcard);\n\n  if (rootMerge !== null) {\n    if (mergeCache !== null) {\n      mergeCache.set(a, b, rootMerge);\n    }\n\n    return rootMerge;\n  }\n\n  if (a.returnState === b.returnState) {\n    const parent = merge(a.parentCtx, b.parentCtx, rootIsWildcard, mergeCache); // if parent is same as existing a or b parent or reduced to a parent,\n    // return it\n\n    if (parent === a.parentCtx) {\n      return a; // ax + bx = ax, if a=b\n    }\n\n    if (parent === b.parentCtx) {\n      return b; // ax + bx = bx, if a=b\n    } // else: ax + ay = a'[x,y]\n    // merge parents x and y, giving array node with x,y then remainders\n    // of those graphs. dup a, a' points at merged array\n    // new joined parent so create new singleton pointing to it, a'\n\n\n    const spc = SingletonPredictionContext.create(parent, a.returnState);\n\n    if (mergeCache !== null) {\n      mergeCache.set(a, b, spc);\n    }\n\n    return spc;\n  } else {\n    // a != b payloads differ\n    // see if we can collapse parents due to $+x parents if local ctx\n    let singleParent = null;\n\n    if (a === b || a.parentCtx !== null && a.parentCtx === b.parentCtx) {\n      // ax +\n      // bx =\n      // [a,b]x\n      singleParent = a.parentCtx;\n    }\n\n    if (singleParent !== null) {\n      // parents are same\n      // sort payloads and use same parent\n      const payloads = [a.returnState, b.returnState];\n\n      if (a.returnState > b.returnState) {\n        payloads[0] = b.returnState;\n        payloads[1] = a.returnState;\n      }\n\n      const parents = [singleParent, singleParent];\n      const apc = new ArrayPredictionContext(parents, payloads);\n\n      if (mergeCache !== null) {\n        mergeCache.set(a, b, apc);\n      }\n\n      return apc;\n    } // parents differ and can't merge them. Just pack together\n    // into array; can't merge.\n    // ax + by = [ax,by]\n\n\n    const payloads = [a.returnState, b.returnState];\n    let parents = [a.parentCtx, b.parentCtx];\n\n    if (a.returnState > b.returnState) {\n      // sort by payload\n      payloads[0] = b.returnState;\n      payloads[1] = a.returnState;\n      parents = [b.parentCtx, a.parentCtx];\n    }\n\n    const a_ = new ArrayPredictionContext(parents, payloads);\n\n    if (mergeCache !== null) {\n      mergeCache.set(a, b, a_);\n    }\n\n    return a_;\n  }\n}\n/**\n * Handle case where at least one of {@code a} or {@code b} is\n * {@link //EMPTY}. In the following diagrams, the symbol {@code $} is used\n * to represent {@link //EMPTY}.\n *\n * <h2>Local-Context Merges</h2>\n *\n * <p>These local-context merge operations are used when {@code rootIsWildcard}\n * is true.</p>\n *\n * <p>{@link //EMPTY} is superset of any graph; return {@link //EMPTY}.<br>\n * <embed src=\"images/LocalMerge_EmptyRoot.svg\" type=\"image/svg+xml\"/></p>\n *\n * <p>{@link //EMPTY} and anything is {@code //EMPTY}, so merged parent is\n * {@code //EMPTY}; return left graph.<br>\n * <embed src=\"images/LocalMerge_EmptyParent.svg\" type=\"image/svg+xml\"/></p>\n *\n * <p>Special case of last merge if local context.<br>\n * <embed src=\"images/LocalMerge_DiffRoots.svg\" type=\"image/svg+xml\"/></p>\n *\n * <h2>Full-Context Merges</h2>\n *\n * <p>These full-context merge operations are used when {@code rootIsWildcard}\n * is false.</p>\n *\n * <p><embed src=\"images/FullMerge_EmptyRoots.svg\" type=\"image/svg+xml\"/></p>\n *\n * <p>Must keep all contexts; {@link //EMPTY} in array is a special value (and\n * null parent).<br>\n * <embed src=\"images/FullMerge_EmptyRoot.svg\" type=\"image/svg+xml\"/></p>\n *\n * <p><embed src=\"images/FullMerge_SameRoot.svg\" type=\"image/svg+xml\"/></p>\n *\n * @param a the first {@link SingletonPredictionContext}\n * @param b the second {@link SingletonPredictionContext}\n * @param rootIsWildcard {@code true} if this is a local-context merge,\n * otherwise false to indicate a full-context merge\n */\n\n\nfunction mergeRoot(a, b, rootIsWildcard) {\n  if (rootIsWildcard) {\n    if (a === PredictionContext.EMPTY) {\n      return PredictionContext.EMPTY; // // + b =//\n    }\n\n    if (b === PredictionContext.EMPTY) {\n      return PredictionContext.EMPTY; // a +// =//\n    }\n  } else {\n    if (a === PredictionContext.EMPTY && b === PredictionContext.EMPTY) {\n      return PredictionContext.EMPTY; // $ + $ = $\n    } else if (a === PredictionContext.EMPTY) {\n      // $ + x = [$,x]\n      const payloads = [b.returnState, PredictionContext.EMPTY_RETURN_STATE];\n      const parents = [b.parentCtx, null];\n      return new ArrayPredictionContext(parents, payloads);\n    } else if (b === PredictionContext.EMPTY) {\n      // x + $ = [$,x] ($ is always first if present)\n      const payloads = [a.returnState, PredictionContext.EMPTY_RETURN_STATE];\n      const parents = [a.parentCtx, null];\n      return new ArrayPredictionContext(parents, payloads);\n    }\n  }\n\n  return null;\n}\n/**\n * Merge two {@link ArrayPredictionContext} instances.\n *\n * <p>Different tops, different parents.<br>\n * <embed src=\"images/ArrayMerge_DiffTopDiffPar.svg\" type=\"image/svg+xml\"/></p>\n *\n * <p>Shared top, same parents.<br>\n * <embed src=\"images/ArrayMerge_ShareTopSamePar.svg\" type=\"image/svg+xml\"/></p>\n *\n * <p>Shared top, different parents.<br>\n * <embed src=\"images/ArrayMerge_ShareTopDiffPar.svg\" type=\"image/svg+xml\"/></p>\n *\n * <p>Shared top, all shared parents.<br>\n * <embed src=\"images/ArrayMerge_ShareTopSharePar.svg\"\n * type=\"image/svg+xml\"/></p>\n *\n * <p>Equal tops, merge parents and reduce top to\n * {@link SingletonPredictionContext}.<br>\n * <embed src=\"images/ArrayMerge_EqualTop.svg\" type=\"image/svg+xml\"/></p>\n */\n\n\nfunction mergeArrays(a, b, rootIsWildcard, mergeCache) {\n  if (mergeCache !== null) {\n    let previous = mergeCache.get(a, b);\n\n    if (previous !== null) {\n      return previous;\n    }\n\n    previous = mergeCache.get(b, a);\n\n    if (previous !== null) {\n      return previous;\n    }\n  } // merge sorted payloads a + b => M\n\n\n  let i = 0; // walks a\n\n  let j = 0; // walks b\n\n  let k = 0; // walks target M array\n\n  let mergedReturnStates = [];\n  let mergedParents = []; // walk and merge to yield mergedParents, mergedReturnStates\n\n  while (i < a.returnStates.length && j < b.returnStates.length) {\n    const a_parent = a.parents[i];\n    const b_parent = b.parents[j];\n\n    if (a.returnStates[i] === b.returnStates[j]) {\n      // same payload (stack tops are equal), must yield merged singleton\n      const payload = a.returnStates[i]; // $+$ = $\n\n      const bothDollars = payload === PredictionContext.EMPTY_RETURN_STATE && a_parent === null && b_parent === null;\n      const ax_ax = a_parent !== null && b_parent !== null && a_parent === b_parent; // ax+ax\n      // ->\n      // ax\n\n      if (bothDollars || ax_ax) {\n        mergedParents[k] = a_parent; // choose left\n\n        mergedReturnStates[k] = payload;\n      } else {\n        // ax+ay -> a'[x,y]\n        mergedParents[k] = merge(a_parent, b_parent, rootIsWildcard, mergeCache);\n        mergedReturnStates[k] = payload;\n      }\n\n      i += 1; // hop over left one as usual\n\n      j += 1; // but also skip one in right side since we merge\n    } else if (a.returnStates[i] < b.returnStates[j]) {\n      // copy a[i] to M\n      mergedParents[k] = a_parent;\n      mergedReturnStates[k] = a.returnStates[i];\n      i += 1;\n    } else {\n      // b > a, copy b[j] to M\n      mergedParents[k] = b_parent;\n      mergedReturnStates[k] = b.returnStates[j];\n      j += 1;\n    }\n\n    k += 1;\n  } // copy over any payloads remaining in either array\n\n\n  if (i < a.returnStates.length) {\n    for (let p = i; p < a.returnStates.length; p++) {\n      mergedParents[k] = a.parents[p];\n      mergedReturnStates[k] = a.returnStates[p];\n      k += 1;\n    }\n  } else {\n    for (let p = j; p < b.returnStates.length; p++) {\n      mergedParents[k] = b.parents[p];\n      mergedReturnStates[k] = b.returnStates[p];\n      k += 1;\n    }\n  } // trim merged if we combined a few that had same stack tops\n\n\n  if (k < mergedParents.length) {\n    // write index < last position; trim\n    if (k === 1) {\n      // for just one merged element, return singleton top\n      const a_ = SingletonPredictionContext.create(mergedParents[0], mergedReturnStates[0]);\n\n      if (mergeCache !== null) {\n        mergeCache.set(a, b, a_);\n      }\n\n      return a_;\n    }\n\n    mergedParents = mergedParents.slice(0, k);\n    mergedReturnStates = mergedReturnStates.slice(0, k);\n  }\n\n  const M = new ArrayPredictionContext(mergedParents, mergedReturnStates); // if we created same array as a or b, return that instead\n  // TODO: track whether this is possible above during merge sort for speed\n\n  if (M === a) {\n    if (mergeCache !== null) {\n      mergeCache.set(a, b, a);\n    }\n\n    return a;\n  }\n\n  if (M === b) {\n    if (mergeCache !== null) {\n      mergeCache.set(a, b, b);\n    }\n\n    return b;\n  }\n\n  combineCommonParents(mergedParents);\n\n  if (mergeCache !== null) {\n    mergeCache.set(a, b, M);\n  }\n\n  return M;\n}\n/**\n * Make pass over all <em>M</em> {@code parents}; merge any {@code equals()}\n * ones.\n */\n\n\nfunction combineCommonParents(parents) {\n  const uniqueParents = new Map();\n\n  for (let p = 0; p < parents.length; p++) {\n    const parent = parents[p];\n\n    if (!uniqueParents.containsKey(parent)) {\n      uniqueParents.put(parent, parent);\n    }\n  }\n\n  for (let q = 0; q < parents.length; q++) {\n    parents[q] = uniqueParents.get(parents[q]);\n  }\n}\n\nfunction getCachedPredictionContext(context, contextCache, visited) {\n  if (context.isEmpty()) {\n    return context;\n  }\n\n  let existing = visited.get(context) || null;\n\n  if (existing !== null) {\n    return existing;\n  }\n\n  existing = contextCache.get(context);\n\n  if (existing !== null) {\n    visited.put(context, existing);\n    return existing;\n  }\n\n  let changed = false;\n  let parents = [];\n\n  for (let i = 0; i < parents.length; i++) {\n    const parent = getCachedPredictionContext(context.getParent(i), contextCache, visited);\n\n    if (changed || parent !== context.getParent(i)) {\n      if (!changed) {\n        parents = [];\n\n        for (let j = 0; j < context.length; j++) {\n          parents[j] = context.getParent(j);\n        }\n\n        changed = true;\n      }\n\n      parents[i] = parent;\n    }\n  }\n\n  if (!changed) {\n    contextCache.add(context);\n    visited.put(context, context);\n    return context;\n  }\n\n  let updated = null;\n\n  if (parents.length === 0) {\n    updated = PredictionContext.EMPTY;\n  } else if (parents.length === 1) {\n    updated = SingletonPredictionContext.create(parents[0], context.getReturnState(0));\n  } else {\n    updated = new ArrayPredictionContext(parents, context.returnStates);\n  }\n\n  contextCache.add(updated);\n  visited.put(updated, updated);\n  visited.put(context, updated);\n  return updated;\n} // ter's recursive version of Sam's getAllNodes()\n\n\nfunction getAllContextNodes(context, nodes, visited) {\n  if (nodes === null) {\n    nodes = [];\n    return getAllContextNodes(context, nodes, visited);\n  } else if (visited === null) {\n    visited = new Map();\n    return getAllContextNodes(context, nodes, visited);\n  } else {\n    if (context === null || visited.containsKey(context)) {\n      return nodes;\n    }\n\n    visited.put(context, context);\n    nodes.push(context);\n\n    for (let i = 0; i < context.length; i++) {\n      getAllContextNodes(context.getParent(i), nodes, visited);\n    }\n\n    return nodes;\n  }\n}\n\nmodule.exports = {\n  merge,\n  PredictionContext,\n  PredictionContextCache,\n  SingletonPredictionContext,\n  predictionContextFromRuleContext,\n  getCachedPredictionContext\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/PredictionContext.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/Recognizer.js":
/*!******************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/Recognizer.js ***!
  \******************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Token\n} = __webpack_require__(/*! ./Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\nconst {\n  ConsoleErrorListener\n} = __webpack_require__(/*! ./error/ErrorListener */ \"./node_modules/antlr4/src/antlr4/error/ErrorListener.js\");\n\nconst {\n  ProxyErrorListener\n} = __webpack_require__(/*! ./error/ErrorListener */ \"./node_modules/antlr4/src/antlr4/error/ErrorListener.js\");\n\nclass Recognizer {\n  constructor() {\n    this._listeners = [ConsoleErrorListener.INSTANCE];\n    this._interp = null;\n    this._stateNumber = -1;\n  }\n\n  checkVersion(toolVersion) {\n    const runtimeVersion = \"4.9.3\";\n\n    if (runtimeVersion !== toolVersion) {\n      console.log(\"ANTLR runtime and generated code versions disagree: \" + runtimeVersion + \"!=\" + toolVersion);\n    }\n  }\n\n  addErrorListener(listener) {\n    this._listeners.push(listener);\n  }\n\n  removeErrorListeners() {\n    this._listeners = [];\n  }\n\n  getLiteralNames() {\n    return Object.getPrototypeOf(this).constructor.literalNames || [];\n  }\n\n  getSymbolicNames() {\n    return Object.getPrototypeOf(this).constructor.symbolicNames || [];\n  }\n\n  getTokenNames() {\n    if (!this.tokenNames) {\n      const literalNames = this.getLiteralNames();\n      const symbolicNames = this.getSymbolicNames();\n      const length = literalNames.length > symbolicNames.length ? literalNames.length : symbolicNames.length;\n      this.tokenNames = [];\n\n      for (let i = 0; i < length; i++) {\n        this.tokenNames[i] = literalNames[i] || symbolicNames[i] || \"<INVALID\";\n      }\n    }\n\n    return this.tokenNames;\n  }\n\n  getTokenTypeMap() {\n    const tokenNames = this.getTokenNames();\n\n    if (tokenNames === null) {\n      throw \"The current recognizer does not provide a list of token names.\";\n    }\n\n    let result = this.tokenTypeMapCache[tokenNames];\n\n    if (result === undefined) {\n      result = tokenNames.reduce(function (o, k, i) {\n        o[k] = i;\n      });\n      result.EOF = Token.EOF;\n      this.tokenTypeMapCache[tokenNames] = result;\n    }\n\n    return result;\n  }\n  /**\n   * Get a map from rule names to rule indexes.\n   * <p>Used for XPath and tree pattern compilation.</p>\n   */\n\n\n  getRuleIndexMap() {\n    const ruleNames = this.ruleNames;\n\n    if (ruleNames === null) {\n      throw \"The current recognizer does not provide a list of rule names.\";\n    }\n\n    let result = this.ruleIndexMapCache[ruleNames]; // todo: should it be Recognizer.ruleIndexMapCache ?\n\n    if (result === undefined) {\n      result = ruleNames.reduce(function (o, k, i) {\n        o[k] = i;\n      });\n      this.ruleIndexMapCache[ruleNames] = result;\n    }\n\n    return result;\n  }\n\n  getTokenType(tokenName) {\n    const ttype = this.getTokenTypeMap()[tokenName];\n\n    if (ttype !== undefined) {\n      return ttype;\n    } else {\n      return Token.INVALID_TYPE;\n    }\n  } // What is the error header, normally line/character position information?\n\n\n  getErrorHeader(e) {\n    const line = e.getOffendingToken().line;\n    const column = e.getOffendingToken().column;\n    return \"line \" + line + \":\" + column;\n  }\n  /**\n   * How should a token be displayed in an error message? The default\n   * is to display just the text, but during development you might\n   * want to have a lot of information spit out.  Override in that case\n   * to use t.toString() (which, for CommonToken, dumps everything about\n   * the token). This is better than forcing you to override a method in\n   * your token objects because you don't have to go modify your lexer\n   * so that it creates a new Java type.\n   *\n   * @deprecated This method is not called by the ANTLR 4 Runtime. Specific\n   * implementations of {@link ANTLRErrorStrategy} may provide a similar\n   * feature when necessary. For example, see\n   * {@link DefaultErrorStrategy//getTokenErrorDisplay}.*/\n\n\n  getTokenErrorDisplay(t) {\n    if (t === null) {\n      return \"<no token>\";\n    }\n\n    let s = t.text;\n\n    if (s === null) {\n      if (t.type === Token.EOF) {\n        s = \"<EOF>\";\n      } else {\n        s = \"<\" + t.type + \">\";\n      }\n    }\n\n    s = s.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\").replace(\"\\t\", \"\\\\t\");\n    return \"'\" + s + \"'\";\n  }\n\n  getErrorListenerDispatch() {\n    return new ProxyErrorListener(this._listeners);\n  }\n  /**\n   * subclass needs to override these if there are sempreds or actions\n   * that the ATN interp needs to execute\n   */\n\n\n  sempred(localctx, ruleIndex, actionIndex) {\n    return true;\n  }\n\n  precpred(localctx, precedence) {\n    return true;\n  }\n\n  get state() {\n    return this._stateNumber;\n  }\n\n  set state(state) {\n    this._stateNumber = state;\n  }\n\n}\n\nRecognizer.tokenTypeMapCache = {};\nRecognizer.ruleIndexMapCache = {};\nmodule.exports = Recognizer;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/Recognizer.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/RuleContext.js":
/*!*******************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/RuleContext.js ***!
  \*******************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  RuleNode\n} = __webpack_require__(/*! ./tree/Tree */ \"./node_modules/antlr4/src/antlr4/tree/Tree.js\");\n\nconst {\n  INVALID_INTERVAL\n} = __webpack_require__(/*! ./tree/Tree */ \"./node_modules/antlr4/src/antlr4/tree/Tree.js\");\n\nconst Trees = __webpack_require__(/*! ./tree/Trees */ \"./node_modules/antlr4/src/antlr4/tree/Trees.js\");\n\nclass RuleContext extends RuleNode {\n  /** A rule context is a record of a single rule invocation. It knows\n   * which context invoked it, if any. If there is no parent context, then\n   * naturally the invoking state is not valid.  The parent link\n   * provides a chain upwards from the current rule invocation to the root\n   * of the invocation tree, forming a stack. We actually carry no\n   * information about the rule associated with this context (except\n   * when parsing). We keep only the state number of the invoking state from\n   * the ATN submachine that invoked this. Contrast this with the s\n   * pointer inside ParserRuleContext that tracks the current state\n   * being \"executed\" for the current rule.\n   *\n   * The parent contexts are useful for computing lookahead sets and\n   * getting error information.\n   *\n   * These objects are used during parsing and prediction.\n   * For the special case of parsers, we use the subclass\n   * ParserRuleContext.\n   *\n   * @see ParserRuleContext\n   */\n  constructor(parent, invokingState) {\n    // What context invoked this rule?\n    super();\n    this.parentCtx = parent || null;\n    /**\n     * What state invoked the rule associated with this context?\n     * The \"return address\" is the followState of invokingState\n     * If parent is null, this should be -1.\n     */\n\n    this.invokingState = invokingState || -1;\n  }\n\n  depth() {\n    let n = 0;\n    let p = this;\n\n    while (p !== null) {\n      p = p.parentCtx;\n      n += 1;\n    }\n\n    return n;\n  }\n  /**\n   * A context is empty if there is no invoking state; meaning nobody call\n   * current context.\n   */\n\n\n  isEmpty() {\n    return this.invokingState === -1;\n  } // satisfy the ParseTree / SyntaxTree interface\n\n\n  getSourceInterval() {\n    return INVALID_INTERVAL;\n  }\n\n  getRuleContext() {\n    return this;\n  }\n\n  getPayload() {\n    return this;\n  }\n  /**\n   * Return the combined text of all child nodes. This method only considers\n   * tokens which have been added to the parse tree.\n   * <p>\n   * Since tokens on hidden channels (e.g. whitespace or comments) are not\n   * added to the parse trees, they will not appear in the output of this\n   * method.\n   */\n\n\n  getText() {\n    if (this.getChildCount() === 0) {\n      return \"\";\n    } else {\n      return this.children.map(function (child) {\n        return child.getText();\n      }).join(\"\");\n    }\n  }\n  /**\n   * For rule associated with this parse tree internal node, return\n   * the outer alternative number used to match the input. Default\n   * implementation does not compute nor store this alt num. Create\n   * a subclass of ParserRuleContext with backing field and set\n   * option contextSuperClass.\n   * to set it.\n   */\n\n\n  getAltNumber() {\n    // use constant value of ATN.INVALID_ALT_NUMBER to avoid circular dependency\n    return 0;\n  }\n  /**\n   * Set the outer alternative number for this context node. Default\n   * implementation does nothing to avoid backing field overhead for\n   * trees that don't need it.  Create\n   * a subclass of ParserRuleContext with backing field and set\n   * option contextSuperClass.\n   */\n\n\n  setAltNumber(altNumber) {}\n\n  getChild(i) {\n    return null;\n  }\n\n  getChildCount() {\n    return 0;\n  }\n\n  accept(visitor) {\n    return visitor.visitChildren(this);\n  }\n  /**\n   * Print out a whole tree, not just a node, in LISP format\n   * (root child1 .. childN). Print just a node if this is a leaf.\n   */\n\n\n  toStringTree(ruleNames, recog) {\n    return Trees.toStringTree(this, ruleNames, recog);\n  }\n\n  toString(ruleNames, stop) {\n    ruleNames = ruleNames || null;\n    stop = stop || null;\n    let p = this;\n    let s = \"[\";\n\n    while (p !== null && p !== stop) {\n      if (ruleNames === null) {\n        if (!p.isEmpty()) {\n          s += p.invokingState;\n        }\n      } else {\n        const ri = p.ruleIndex;\n        const ruleName = ri >= 0 && ri < ruleNames.length ? ruleNames[ri] : \"\" + ri;\n        s += ruleName;\n      }\n\n      if (p.parentCtx !== null && (ruleNames !== null || !p.parentCtx.isEmpty())) {\n        s += \" \";\n      }\n\n      p = p.parentCtx;\n    }\n\n    s += \"]\";\n    return s;\n  }\n\n}\n\nmodule.exports = RuleContext;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/RuleContext.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/Token.js":
/*!*************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/Token.js ***!
  \*************************************************/
/***/ ((module) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\n\n/**\n * A token has properties: text, type, line, character position in the line\n * (so we can ignore tabs), token channel, index, and source from which\n * we obtained this token.\n */\nclass Token {\n  constructor() {\n    this.source = null;\n    this.type = null; // token type of the token\n\n    this.channel = null; // The parser ignores everything not on DEFAULT_CHANNEL\n\n    this.start = null; // optional; return -1 if not implemented.\n\n    this.stop = null; // optional; return -1 if not implemented.\n\n    this.tokenIndex = null; // from 0..n-1 of the token object in the input stream\n\n    this.line = null; // line=1..n of the 1st character\n\n    this.column = null; // beginning of the line at which it occurs, 0..n-1\n\n    this._text = null; // text of the token.\n  }\n\n  getTokenSource() {\n    return this.source[0];\n  }\n\n  getInputStream() {\n    return this.source[1];\n  }\n\n  get text() {\n    return this._text;\n  }\n\n  set text(text) {\n    this._text = text;\n  }\n\n}\n\nToken.INVALID_TYPE = 0;\n/**\n * During lookahead operations, this \"token\" signifies we hit rule end ATN state\n * and did not follow it despite needing to.\n */\n\nToken.EPSILON = -2;\nToken.MIN_USER_TOKEN_TYPE = 1;\nToken.EOF = -1;\n/**\n * All tokens go to the parser (unless skip() is called in that rule)\n * on a particular \"channel\". The parser tunes to a particular channel\n * so that whitespace etc... can go to the parser on a \"hidden\" channel.\n */\n\nToken.DEFAULT_CHANNEL = 0;\n/**\n * Anything on different channel than DEFAULT_CHANNEL is not parsed\n * by parser.\n */\n\nToken.HIDDEN_CHANNEL = 1;\n\nclass CommonToken extends Token {\n  constructor(source, type, channel, start, stop) {\n    super();\n    this.source = source !== undefined ? source : CommonToken.EMPTY_SOURCE;\n    this.type = type !== undefined ? type : null;\n    this.channel = channel !== undefined ? channel : Token.DEFAULT_CHANNEL;\n    this.start = start !== undefined ? start : -1;\n    this.stop = stop !== undefined ? stop : -1;\n    this.tokenIndex = -1;\n\n    if (this.source[0] !== null) {\n      this.line = source[0].line;\n      this.column = source[0].column;\n    } else {\n      this.column = -1;\n    }\n  }\n  /**\n   * Constructs a new {@link CommonToken} as a copy of another {@link Token}.\n   *\n   * <p>\n   * If {@code oldToken} is also a {@link CommonToken} instance, the newly\n   * constructed token will share a reference to the {@link //text} field and\n   * the {@link Pair} stored in {@link //source}. Otherwise, {@link //text} will\n   * be assigned the result of calling {@link //getText}, and {@link //source}\n   * will be constructed from the result of {@link Token//getTokenSource} and\n   * {@link Token//getInputStream}.</p>\n   *\n   * @param oldToken The token to copy.\n   */\n\n\n  clone() {\n    const t = new CommonToken(this.source, this.type, this.channel, this.start, this.stop);\n    t.tokenIndex = this.tokenIndex;\n    t.line = this.line;\n    t.column = this.column;\n    t.text = this.text;\n    return t;\n  }\n\n  toString() {\n    let txt = this.text;\n\n    if (txt !== null) {\n      txt = txt.replace(/\\n/g, \"\\\\n\").replace(/\\r/g, \"\\\\r\").replace(/\\t/g, \"\\\\t\");\n    } else {\n      txt = \"<no text>\";\n    }\n\n    return \"[@\" + this.tokenIndex + \",\" + this.start + \":\" + this.stop + \"='\" + txt + \"',<\" + this.type + \">\" + (this.channel > 0 ? \",channel=\" + this.channel : \"\") + \",\" + this.line + \":\" + this.column + \"]\";\n  }\n\n  get text() {\n    if (this._text !== null) {\n      return this._text;\n    }\n\n    const input = this.getInputStream();\n\n    if (input === null) {\n      return null;\n    }\n\n    const n = input.size;\n\n    if (this.start < n && this.stop < n) {\n      return input.getText(this.start, this.stop);\n    } else {\n      return \"<EOF>\";\n    }\n  }\n\n  set text(text) {\n    this._text = text;\n  }\n\n}\n/**\n * An empty {@link Pair} which is used as the default value of\n * {@link //source} for tokens that do not have a source.\n */\n\n\nCommonToken.EMPTY_SOURCE = [null, null];\nmodule.exports = {\n  Token,\n  CommonToken\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/Token.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/Utils.js":
/*!*************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/Utils.js ***!
  \*************************************************/
/***/ ((module) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nfunction valueToString(v) {\n  return v === null ? \"null\" : v;\n}\n\nfunction arrayToString(a) {\n  return Array.isArray(a) ? \"[\" + a.map(valueToString).join(\", \") + \"]\" : \"null\";\n}\n\nString.prototype.seed = String.prototype.seed || Math.round(Math.random() * Math.pow(2, 32));\n\nString.prototype.hashCode = function () {\n  const key = this.toString();\n  let h1b, k1;\n  const remainder = key.length & 3; // key.length % 4\n\n  const bytes = key.length - remainder;\n  let h1 = String.prototype.seed;\n  const c1 = 0xcc9e2d51;\n  const c2 = 0x1b873593;\n  let i = 0;\n\n  while (i < bytes) {\n    k1 = key.charCodeAt(i) & 0xff | (key.charCodeAt(++i) & 0xff) << 8 | (key.charCodeAt(++i) & 0xff) << 16 | (key.charCodeAt(++i) & 0xff) << 24;\n    ++i;\n    k1 = (k1 & 0xffff) * c1 + (((k1 >>> 16) * c1 & 0xffff) << 16) & 0xffffffff;\n    k1 = k1 << 15 | k1 >>> 17;\n    k1 = (k1 & 0xffff) * c2 + (((k1 >>> 16) * c2 & 0xffff) << 16) & 0xffffffff;\n    h1 ^= k1;\n    h1 = h1 << 13 | h1 >>> 19;\n    h1b = (h1 & 0xffff) * 5 + (((h1 >>> 16) * 5 & 0xffff) << 16) & 0xffffffff;\n    h1 = (h1b & 0xffff) + 0x6b64 + (((h1b >>> 16) + 0xe654 & 0xffff) << 16);\n  }\n\n  k1 = 0;\n\n  switch (remainder) {\n    case 3:\n      k1 ^= (key.charCodeAt(i + 2) & 0xff) << 16;\n\n    case 2:\n      k1 ^= (key.charCodeAt(i + 1) & 0xff) << 8;\n\n    case 1:\n      k1 ^= key.charCodeAt(i) & 0xff;\n      k1 = (k1 & 0xffff) * c1 + (((k1 >>> 16) * c1 & 0xffff) << 16) & 0xffffffff;\n      k1 = k1 << 15 | k1 >>> 17;\n      k1 = (k1 & 0xffff) * c2 + (((k1 >>> 16) * c2 & 0xffff) << 16) & 0xffffffff;\n      h1 ^= k1;\n  }\n\n  h1 ^= key.length;\n  h1 ^= h1 >>> 16;\n  h1 = (h1 & 0xffff) * 0x85ebca6b + (((h1 >>> 16) * 0x85ebca6b & 0xffff) << 16) & 0xffffffff;\n  h1 ^= h1 >>> 13;\n  h1 = (h1 & 0xffff) * 0xc2b2ae35 + (((h1 >>> 16) * 0xc2b2ae35 & 0xffff) << 16) & 0xffffffff;\n  h1 ^= h1 >>> 16;\n  return h1 >>> 0;\n};\n\nfunction standardEqualsFunction(a, b) {\n  return a ? a.equals(b) : a == b;\n}\n\nfunction standardHashCodeFunction(a) {\n  return a ? a.hashCode() : -1;\n}\n\nclass Set {\n  constructor(hashFunction, equalsFunction) {\n    this.data = {};\n    this.hashFunction = hashFunction || standardHashCodeFunction;\n    this.equalsFunction = equalsFunction || standardEqualsFunction;\n  }\n\n  add(value) {\n    const hash = this.hashFunction(value);\n    const key = \"hash_\" + hash;\n\n    if (key in this.data) {\n      const values = this.data[key];\n\n      for (let i = 0; i < values.length; i++) {\n        if (this.equalsFunction(value, values[i])) {\n          return values[i];\n        }\n      }\n\n      values.push(value);\n      return value;\n    } else {\n      this.data[key] = [value];\n      return value;\n    }\n  }\n\n  contains(value) {\n    return this.get(value) != null;\n  }\n\n  get(value) {\n    const hash = this.hashFunction(value);\n    const key = \"hash_\" + hash;\n\n    if (key in this.data) {\n      const values = this.data[key];\n\n      for (let i = 0; i < values.length; i++) {\n        if (this.equalsFunction(value, values[i])) {\n          return values[i];\n        }\n      }\n    }\n\n    return null;\n  }\n\n  values() {\n    let l = [];\n\n    for (const key in this.data) {\n      if (key.indexOf(\"hash_\") === 0) {\n        l = l.concat(this.data[key]);\n      }\n    }\n\n    return l;\n  }\n\n  toString() {\n    return arrayToString(this.values());\n  }\n\n  get length() {\n    let l = 0;\n\n    for (const key in this.data) {\n      if (key.indexOf(\"hash_\") === 0) {\n        l = l + this.data[key].length;\n      }\n    }\n\n    return l;\n  }\n\n}\n\nclass BitSet {\n  constructor() {\n    this.data = [];\n  }\n\n  add(value) {\n    this.data[value] = true;\n  }\n\n  or(set) {\n    const bits = this;\n    Object.keys(set.data).map(function (alt) {\n      bits.add(alt);\n    });\n  }\n\n  remove(value) {\n    delete this.data[value];\n  }\n\n  contains(value) {\n    return this.data[value] === true;\n  }\n\n  values() {\n    return Object.keys(this.data);\n  }\n\n  minValue() {\n    return Math.min.apply(null, this.values());\n  }\n\n  hashCode() {\n    const hash = new Hash();\n    hash.update(this.values());\n    return hash.finish();\n  }\n\n  equals(other) {\n    if (!(other instanceof BitSet)) {\n      return false;\n    }\n\n    return this.hashCode() === other.hashCode();\n  }\n\n  toString() {\n    return \"{\" + this.values().join(\", \") + \"}\";\n  }\n\n  get length() {\n    return this.values().length;\n  }\n\n}\n\nclass Map {\n  constructor(hashFunction, equalsFunction) {\n    this.data = {};\n    this.hashFunction = hashFunction || standardHashCodeFunction;\n    this.equalsFunction = equalsFunction || standardEqualsFunction;\n  }\n\n  put(key, value) {\n    const hashKey = \"hash_\" + this.hashFunction(key);\n\n    if (hashKey in this.data) {\n      const entries = this.data[hashKey];\n\n      for (let i = 0; i < entries.length; i++) {\n        const entry = entries[i];\n\n        if (this.equalsFunction(key, entry.key)) {\n          const oldValue = entry.value;\n          entry.value = value;\n          return oldValue;\n        }\n      }\n\n      entries.push({\n        key: key,\n        value: value\n      });\n      return value;\n    } else {\n      this.data[hashKey] = [{\n        key: key,\n        value: value\n      }];\n      return value;\n    }\n  }\n\n  containsKey(key) {\n    const hashKey = \"hash_\" + this.hashFunction(key);\n\n    if (hashKey in this.data) {\n      const entries = this.data[hashKey];\n\n      for (let i = 0; i < entries.length; i++) {\n        const entry = entries[i];\n        if (this.equalsFunction(key, entry.key)) return true;\n      }\n    }\n\n    return false;\n  }\n\n  get(key) {\n    const hashKey = \"hash_\" + this.hashFunction(key);\n\n    if (hashKey in this.data) {\n      const entries = this.data[hashKey];\n\n      for (let i = 0; i < entries.length; i++) {\n        const entry = entries[i];\n        if (this.equalsFunction(key, entry.key)) return entry.value;\n      }\n    }\n\n    return null;\n  }\n\n  entries() {\n    let l = [];\n\n    for (const key in this.data) {\n      if (key.indexOf(\"hash_\") === 0) {\n        l = l.concat(this.data[key]);\n      }\n    }\n\n    return l;\n  }\n\n  getKeys() {\n    return this.entries().map(function (e) {\n      return e.key;\n    });\n  }\n\n  getValues() {\n    return this.entries().map(function (e) {\n      return e.value;\n    });\n  }\n\n  toString() {\n    const ss = this.entries().map(function (entry) {\n      return '{' + entry.key + ':' + entry.value + '}';\n    });\n    return '[' + ss.join(\", \") + ']';\n  }\n\n  get length() {\n    let l = 0;\n\n    for (const hashKey in this.data) {\n      if (hashKey.indexOf(\"hash_\") === 0) {\n        l = l + this.data[hashKey].length;\n      }\n    }\n\n    return l;\n  }\n\n}\n\nclass AltDict {\n  constructor() {\n    this.data = {};\n  }\n\n  get(key) {\n    key = \"k-\" + key;\n\n    if (key in this.data) {\n      return this.data[key];\n    } else {\n      return null;\n    }\n  }\n\n  put(key, value) {\n    key = \"k-\" + key;\n    this.data[key] = value;\n  }\n\n  values() {\n    const data = this.data;\n    const keys = Object.keys(this.data);\n    return keys.map(function (key) {\n      return data[key];\n    });\n  }\n\n}\n\nclass DoubleDict {\n  constructor(defaultMapCtor) {\n    this.defaultMapCtor = defaultMapCtor || Map;\n    this.cacheMap = new this.defaultMapCtor();\n  }\n\n  get(a, b) {\n    const d = this.cacheMap.get(a) || null;\n    return d === null ? null : d.get(b) || null;\n  }\n\n  set(a, b, o) {\n    let d = this.cacheMap.get(a) || null;\n\n    if (d === null) {\n      d = new this.defaultMapCtor();\n      this.cacheMap.put(a, d);\n    }\n\n    d.put(b, o);\n  }\n\n}\n\nclass Hash {\n  constructor() {\n    this.count = 0;\n    this.hash = 0;\n  }\n\n  update() {\n    for (let i = 0; i < arguments.length; i++) {\n      const value = arguments[i];\n      if (value == null) continue;\n      if (Array.isArray(value)) this.update.apply(this, value);else {\n        let k = 0;\n\n        switch (typeof value) {\n          case 'undefined':\n          case 'function':\n            continue;\n\n          case 'number':\n          case 'boolean':\n            k = value;\n            break;\n\n          case 'string':\n            k = value.hashCode();\n            break;\n\n          default:\n            if (value.updateHashCode) value.updateHashCode(this);else console.log(\"No updateHashCode for \" + value.toString());\n            continue;\n        }\n\n        k = k * 0xCC9E2D51;\n        k = k << 15 | k >>> 32 - 15;\n        k = k * 0x1B873593;\n        this.count = this.count + 1;\n        let hash = this.hash ^ k;\n        hash = hash << 13 | hash >>> 32 - 13;\n        hash = hash * 5 + 0xE6546B64;\n        this.hash = hash;\n      }\n    }\n  }\n\n  finish() {\n    let hash = this.hash ^ this.count * 4;\n    hash = hash ^ hash >>> 16;\n    hash = hash * 0x85EBCA6B;\n    hash = hash ^ hash >>> 13;\n    hash = hash * 0xC2B2AE35;\n    hash = hash ^ hash >>> 16;\n    return hash;\n  }\n\n}\n\nfunction hashStuff() {\n  const hash = new Hash();\n  hash.update.apply(hash, arguments);\n  return hash.finish();\n}\n\nfunction escapeWhitespace(s, escapeSpaces) {\n  s = s.replace(/\\t/g, \"\\\\t\").replace(/\\n/g, \"\\\\n\").replace(/\\r/g, \"\\\\r\");\n\n  if (escapeSpaces) {\n    s = s.replace(/ /g, \"\\u00B7\");\n  }\n\n  return s;\n}\n\nfunction titleCase(str) {\n  return str.replace(/\\w\\S*/g, function (txt) {\n    return txt.charAt(0).toUpperCase() + txt.substr(1);\n  });\n}\n\nfunction equalArrays(a, b) {\n  if (!Array.isArray(a) || !Array.isArray(b)) return false;\n  if (a === b) return true;\n  if (a.length !== b.length) return false;\n\n  for (let i = 0; i < a.length; i++) {\n    if (a[i] === b[i]) continue;\n    if (!a[i].equals || !a[i].equals(b[i])) return false;\n  }\n\n  return true;\n}\n\nmodule.exports = {\n  Hash,\n  Set,\n  Map,\n  BitSet,\n  AltDict,\n  DoubleDict,\n  hashStuff,\n  escapeWhitespace,\n  arrayToString,\n  titleCase,\n  equalArrays\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/Utils.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/ATN.js":
/*!***************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/ATN.js ***!
  \***************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst LL1Analyzer = __webpack_require__(/*! ./../LL1Analyzer */ \"./node_modules/antlr4/src/antlr4/LL1Analyzer.js\");\n\nconst {\n  IntervalSet\n} = __webpack_require__(/*! ./../IntervalSet */ \"./node_modules/antlr4/src/antlr4/IntervalSet.js\");\n\nconst {\n  Token\n} = __webpack_require__(/*! ./../Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\nclass ATN {\n  constructor(grammarType, maxTokenType) {\n    /**\n     * Used for runtime deserialization of ATNs from strings\n     * The type of the ATN.\n    */\n    this.grammarType = grammarType; // The maximum value for any symbol recognized by a transition in the ATN.\n\n    this.maxTokenType = maxTokenType;\n    this.states = [];\n    /**\n     * Each subrule/rule is a decision point and we must track them so we\n     * can go back later and build DFA predictors for them.  This includes\n     * all the rules, subrules, optional blocks, ()+, ()* etc...\n     */\n\n    this.decisionToState = []; // Maps from rule index to starting state number.\n\n    this.ruleToStartState = []; // Maps from rule index to stop state number.\n\n    this.ruleToStopState = null;\n    this.modeNameToStartState = {};\n    /**\n     * For lexer ATNs, this maps the rule index to the resulting token type.\n     * For parser ATNs, this maps the rule index to the generated bypass token\n     * type if the {@link ATNDeserializationOptions//isGenerateRuleBypassTransitions}\n     * deserialization option was specified; otherwise, this is {@code null}\n     */\n\n    this.ruleToTokenType = null;\n    /**\n     * For lexer ATNs, this is an array of {@link LexerAction} objects which may\n     * be referenced by action transitions in the ATN\n     */\n\n    this.lexerActions = null;\n    this.modeToStartState = [];\n  }\n  /**\n   * Compute the set of valid tokens that can occur starting in state {@code s}.\n   * If {@code ctx} is null, the set of tokens will not include what can follow\n   * the rule surrounding {@code s}. In other words, the set will be\n   * restricted to tokens reachable staying within {@code s}'s rule\n   */\n\n\n  nextTokensInContext(s, ctx) {\n    const anal = new LL1Analyzer(this);\n    return anal.LOOK(s, null, ctx);\n  }\n  /**\n   * Compute the set of valid tokens that can occur starting in {@code s} and\n   * staying in same rule. {@link Token//EPSILON} is in set if we reach end of\n   * rule\n   */\n\n\n  nextTokensNoContext(s) {\n    if (s.nextTokenWithinRule !== null) {\n      return s.nextTokenWithinRule;\n    }\n\n    s.nextTokenWithinRule = this.nextTokensInContext(s, null);\n    s.nextTokenWithinRule.readOnly = true;\n    return s.nextTokenWithinRule;\n  }\n\n  nextTokens(s, ctx) {\n    if (ctx === undefined) {\n      return this.nextTokensNoContext(s);\n    } else {\n      return this.nextTokensInContext(s, ctx);\n    }\n  }\n\n  addState(state) {\n    if (state !== null) {\n      state.atn = this;\n      state.stateNumber = this.states.length;\n    }\n\n    this.states.push(state);\n  }\n\n  removeState(state) {\n    this.states[state.stateNumber] = null; // just free mem, don't shift states in list\n  }\n\n  defineDecisionState(s) {\n    this.decisionToState.push(s);\n    s.decision = this.decisionToState.length - 1;\n    return s.decision;\n  }\n\n  getDecisionState(decision) {\n    if (this.decisionToState.length === 0) {\n      return null;\n    } else {\n      return this.decisionToState[decision];\n    }\n  }\n  /**\n   * Computes the set of input symbols which could follow ATN state number\n   * {@code stateNumber} in the specified full {@code context}. This method\n   * considers the complete parser context, but does not evaluate semantic\n   * predicates (i.e. all predicates encountered during the calculation are\n   * assumed true). If a path in the ATN exists from the starting state to the\n   * {@link RuleStopState} of the outermost context without matching any\n   * symbols, {@link Token//EOF} is added to the returned set.\n   *\n   * <p>If {@code context} is {@code null}, it is treated as\n   * {@link ParserRuleContext//EMPTY}.</p>\n   *\n   * @param stateNumber the ATN state number\n   * @param ctx the full parse context\n   *\n   * @return {IntervalSet} The set of potentially valid input symbols which could follow the\n   * specified state in the specified context.\n   *\n   * @throws IllegalArgumentException if the ATN does not contain a state with\n   * number {@code stateNumber}\n   */\n\n\n  getExpectedTokens(stateNumber, ctx) {\n    if (stateNumber < 0 || stateNumber >= this.states.length) {\n      throw \"Invalid state number.\";\n    }\n\n    const s = this.states[stateNumber];\n    let following = this.nextTokens(s);\n\n    if (!following.contains(Token.EPSILON)) {\n      return following;\n    }\n\n    const expected = new IntervalSet();\n    expected.addSet(following);\n    expected.removeOne(Token.EPSILON);\n\n    while (ctx !== null && ctx.invokingState >= 0 && following.contains(Token.EPSILON)) {\n      const invokingState = this.states[ctx.invokingState];\n      const rt = invokingState.transitions[0];\n      following = this.nextTokens(rt.followState);\n      expected.addSet(following);\n      expected.removeOne(Token.EPSILON);\n      ctx = ctx.parentCtx;\n    }\n\n    if (following.contains(Token.EPSILON)) {\n      expected.addOne(Token.EOF);\n    }\n\n    return expected;\n  }\n\n}\n\nATN.INVALID_ALT_NUMBER = 0;\nmodule.exports = ATN;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/ATN.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/ATNConfig.js":
/*!*********************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/ATNConfig.js ***!
  \*********************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  DecisionState\n} = __webpack_require__(/*! ./ATNState */ \"./node_modules/antlr4/src/antlr4/atn/ATNState.js\");\n\nconst {\n  SemanticContext\n} = __webpack_require__(/*! ./SemanticContext */ \"./node_modules/antlr4/src/antlr4/atn/SemanticContext.js\");\n\nconst {\n  Hash\n} = __webpack_require__(/*! ../Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n\nfunction checkParams(params, isCfg) {\n  if (params === null) {\n    const result = {\n      state: null,\n      alt: null,\n      context: null,\n      semanticContext: null\n    };\n\n    if (isCfg) {\n      result.reachesIntoOuterContext = 0;\n    }\n\n    return result;\n  } else {\n    const props = {};\n    props.state = params.state || null;\n    props.alt = params.alt === undefined ? null : params.alt;\n    props.context = params.context || null;\n    props.semanticContext = params.semanticContext || null;\n\n    if (isCfg) {\n      props.reachesIntoOuterContext = params.reachesIntoOuterContext || 0;\n      props.precedenceFilterSuppressed = params.precedenceFilterSuppressed || false;\n    }\n\n    return props;\n  }\n}\n\nclass ATNConfig {\n  /**\n   * @param {Object} params A tuple: (ATN state, predicted alt, syntactic, semantic context).\n   * The syntactic context is a graph-structured stack node whose\n   * path(s) to the root is the rule invocation(s)\n   * chain used to arrive at the state.  The semantic context is\n   * the tree of semantic predicates encountered before reaching\n   * an ATN state\n   */\n  constructor(params, config) {\n    this.checkContext(params, config);\n    params = checkParams(params);\n    config = checkParams(config, true); // The ATN state associated with this configuration///\n\n    this.state = params.state !== null ? params.state : config.state; // What alt (or lexer rule) is predicted by this configuration///\n\n    this.alt = params.alt !== null ? params.alt : config.alt;\n    /**\n     * The stack of invoking states leading to the rule/states associated\n     * with this config.  We track only those contexts pushed during\n     * execution of the ATN simulator\n     */\n\n    this.context = params.context !== null ? params.context : config.context;\n    this.semanticContext = params.semanticContext !== null ? params.semanticContext : config.semanticContext !== null ? config.semanticContext : SemanticContext.NONE; // TODO: make it a boolean then\n\n    /**\n     * We cannot execute predicates dependent upon local context unless\n     * we know for sure we are in the correct context. Because there is\n     * no way to do this efficiently, we simply cannot evaluate\n     * dependent predicates unless we are in the rule that initially\n     * invokes the ATN simulator.\n     * closure() tracks the depth of how far we dip into the\n     * outer context: depth &gt; 0.  Note that it may not be totally\n     * accurate depth since I don't ever decrement\n     */\n\n    this.reachesIntoOuterContext = config.reachesIntoOuterContext;\n    this.precedenceFilterSuppressed = config.precedenceFilterSuppressed;\n  }\n\n  checkContext(params, config) {\n    if ((params.context === null || params.context === undefined) && (config === null || config.context === null || config.context === undefined)) {\n      this.context = null;\n    }\n  }\n\n  hashCode() {\n    const hash = new Hash();\n    this.updateHashCode(hash);\n    return hash.finish();\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.state.stateNumber, this.alt, this.context, this.semanticContext);\n  }\n  /**\n   * An ATN configuration is equal to another if both have\n   * the same state, they predict the same alternative, and\n   * syntactic/semantic contexts are the same\n   */\n\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof ATNConfig)) {\n      return false;\n    } else {\n      return this.state.stateNumber === other.state.stateNumber && this.alt === other.alt && (this.context === null ? other.context === null : this.context.equals(other.context)) && this.semanticContext.equals(other.semanticContext) && this.precedenceFilterSuppressed === other.precedenceFilterSuppressed;\n    }\n  }\n\n  hashCodeForConfigSet() {\n    const hash = new Hash();\n    hash.update(this.state.stateNumber, this.alt, this.semanticContext);\n    return hash.finish();\n  }\n\n  equalsForConfigSet(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof ATNConfig)) {\n      return false;\n    } else {\n      return this.state.stateNumber === other.state.stateNumber && this.alt === other.alt && this.semanticContext.equals(other.semanticContext);\n    }\n  }\n\n  toString() {\n    return \"(\" + this.state + \",\" + this.alt + (this.context !== null ? \",[\" + this.context.toString() + \"]\" : \"\") + (this.semanticContext !== SemanticContext.NONE ? \",\" + this.semanticContext.toString() : \"\") + (this.reachesIntoOuterContext > 0 ? \",up=\" + this.reachesIntoOuterContext : \"\") + \")\";\n  }\n\n}\n\nclass LexerATNConfig extends ATNConfig {\n  constructor(params, config) {\n    super(params, config); // This is the backing field for {@link //getLexerActionExecutor}.\n\n    const lexerActionExecutor = params.lexerActionExecutor || null;\n    this.lexerActionExecutor = lexerActionExecutor || (config !== null ? config.lexerActionExecutor : null);\n    this.passedThroughNonGreedyDecision = config !== null ? this.checkNonGreedyDecision(config, this.state) : false;\n    this.hashCodeForConfigSet = LexerATNConfig.prototype.hashCode;\n    this.equalsForConfigSet = LexerATNConfig.prototype.equals;\n    return this;\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.state.stateNumber, this.alt, this.context, this.semanticContext, this.passedThroughNonGreedyDecision, this.lexerActionExecutor);\n  }\n\n  equals(other) {\n    return this === other || other instanceof LexerATNConfig && this.passedThroughNonGreedyDecision === other.passedThroughNonGreedyDecision && (this.lexerActionExecutor ? this.lexerActionExecutor.equals(other.lexerActionExecutor) : !other.lexerActionExecutor) && super.equals(other);\n  }\n\n  checkNonGreedyDecision(source, target) {\n    return source.passedThroughNonGreedyDecision || target instanceof DecisionState && target.nonGreedy;\n  }\n\n}\n\nmodule.exports.ATNConfig = ATNConfig;\nmodule.exports.LexerATNConfig = LexerATNConfig;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/ATNConfig.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/ATNConfigSet.js":
/*!************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/ATNConfigSet.js ***!
  \************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst ATN = __webpack_require__(/*! ./ATN */ \"./node_modules/antlr4/src/antlr4/atn/ATN.js\");\n\nconst Utils = __webpack_require__(/*! ./../Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n\nconst {\n  SemanticContext\n} = __webpack_require__(/*! ./SemanticContext */ \"./node_modules/antlr4/src/antlr4/atn/SemanticContext.js\");\n\nconst {\n  merge\n} = __webpack_require__(/*! ./../PredictionContext */ \"./node_modules/antlr4/src/antlr4/PredictionContext.js\");\n\nfunction hashATNConfig(c) {\n  return c.hashCodeForConfigSet();\n}\n\nfunction equalATNConfigs(a, b) {\n  if (a === b) {\n    return true;\n  } else if (a === null || b === null) {\n    return false;\n  } else return a.equalsForConfigSet(b);\n}\n/**\n * Specialized {@link Set}{@code <}{@link ATNConfig}{@code >} that can track\n * info about the set, with support for combining similar configurations using a\n * graph-structured stack\n */\n\n\nclass ATNConfigSet {\n  constructor(fullCtx) {\n    /**\n     * The reason that we need this is because we don't want the hash map to use\n     * the standard hash code and equals. We need all configurations with the\n     * same\n     * {@code (s,i,_,semctx)} to be equal. Unfortunately, this key effectively\n     * doubles\n     * the number of objects associated with ATNConfigs. The other solution is\n     * to\n     * use a hash table that lets us specify the equals/hashcode operation.\n     * All configs but hashed by (s, i, _, pi) not including context. Wiped out\n     * when we go readonly as this set becomes a DFA state\n     */\n    this.configLookup = new Utils.Set(hashATNConfig, equalATNConfigs);\n    /**\n     * Indicates that this configuration set is part of a full context\n     * LL prediction. It will be used to determine how to merge $. With SLL\n     * it's a wildcard whereas it is not for LL context merge\n     */\n\n    this.fullCtx = fullCtx === undefined ? true : fullCtx;\n    /**\n     * Indicates that the set of configurations is read-only. Do not\n     * allow any code to manipulate the set; DFA states will point at\n     * the sets and they must not change. This does not protect the other\n     * fields; in particular, conflictingAlts is set after\n     * we've made this readonly\n     */\n\n    this.readOnly = false; // Track the elements as they are added to the set; supports get(i)///\n\n    this.configs = []; // TODO: these fields make me pretty uncomfortable but nice to pack up info\n    // together, saves recomputation\n    // TODO: can we track conflicts as they are added to save scanning configs\n    // later?\n\n    this.uniqueAlt = 0;\n    this.conflictingAlts = null;\n    /**\n     * Used in parser and lexer. In lexer, it indicates we hit a pred\n     * while computing a closure operation. Don't make a DFA state from this\n     */\n\n    this.hasSemanticContext = false;\n    this.dipsIntoOuterContext = false;\n    this.cachedHashCode = -1;\n  }\n  /**\n   * Adding a new config means merging contexts with existing configs for\n   * {@code (s, i, pi, _)}, where {@code s} is the\n   * {@link ATNConfig//state}, {@code i} is the {@link ATNConfig//alt}, and\n   * {@code pi} is the {@link ATNConfig//semanticContext}. We use\n   * {@code (s,i,pi)} as key.\n   *\n   * <p>This method updates {@link //dipsIntoOuterContext} and\n   * {@link //hasSemanticContext} when necessary.</p>\n   */\n\n\n  add(config, mergeCache) {\n    if (mergeCache === undefined) {\n      mergeCache = null;\n    }\n\n    if (this.readOnly) {\n      throw \"This set is readonly\";\n    }\n\n    if (config.semanticContext !== SemanticContext.NONE) {\n      this.hasSemanticContext = true;\n    }\n\n    if (config.reachesIntoOuterContext > 0) {\n      this.dipsIntoOuterContext = true;\n    }\n\n    const existing = this.configLookup.add(config);\n\n    if (existing === config) {\n      this.cachedHashCode = -1;\n      this.configs.push(config); // track order here\n\n      return true;\n    } // a previous (s,i,pi,_), merge with it and save result\n\n\n    const rootIsWildcard = !this.fullCtx;\n    const merged = merge(existing.context, config.context, rootIsWildcard, mergeCache);\n    /**\n     * no need to check for existing.context, config.context in cache\n     * since only way to create new graphs is \"call rule\" and here. We\n     * cache at both places\n     */\n\n    existing.reachesIntoOuterContext = Math.max(existing.reachesIntoOuterContext, config.reachesIntoOuterContext); // make sure to preserve the precedence filter suppression during the merge\n\n    if (config.precedenceFilterSuppressed) {\n      existing.precedenceFilterSuppressed = true;\n    }\n\n    existing.context = merged; // replace context; no need to alt mapping\n\n    return true;\n  }\n\n  getStates() {\n    const states = new Utils.Set();\n\n    for (let i = 0; i < this.configs.length; i++) {\n      states.add(this.configs[i].state);\n    }\n\n    return states;\n  }\n\n  getPredicates() {\n    const preds = [];\n\n    for (let i = 0; i < this.configs.length; i++) {\n      const c = this.configs[i].semanticContext;\n\n      if (c !== SemanticContext.NONE) {\n        preds.push(c.semanticContext);\n      }\n    }\n\n    return preds;\n  }\n\n  optimizeConfigs(interpreter) {\n    if (this.readOnly) {\n      throw \"This set is readonly\";\n    }\n\n    if (this.configLookup.length === 0) {\n      return;\n    }\n\n    for (let i = 0; i < this.configs.length; i++) {\n      const config = this.configs[i];\n      config.context = interpreter.getCachedContext(config.context);\n    }\n  }\n\n  addAll(coll) {\n    for (let i = 0; i < coll.length; i++) {\n      this.add(coll[i]);\n    }\n\n    return false;\n  }\n\n  equals(other) {\n    return this === other || other instanceof ATNConfigSet && Utils.equalArrays(this.configs, other.configs) && this.fullCtx === other.fullCtx && this.uniqueAlt === other.uniqueAlt && this.conflictingAlts === other.conflictingAlts && this.hasSemanticContext === other.hasSemanticContext && this.dipsIntoOuterContext === other.dipsIntoOuterContext;\n  }\n\n  hashCode() {\n    const hash = new Utils.Hash();\n    hash.update(this.configs);\n    return hash.finish();\n  }\n\n  updateHashCode(hash) {\n    if (this.readOnly) {\n      if (this.cachedHashCode === -1) {\n        this.cachedHashCode = this.hashCode();\n      }\n\n      hash.update(this.cachedHashCode);\n    } else {\n      hash.update(this.hashCode());\n    }\n  }\n\n  isEmpty() {\n    return this.configs.length === 0;\n  }\n\n  contains(item) {\n    if (this.configLookup === null) {\n      throw \"This method is not implemented for readonly sets.\";\n    }\n\n    return this.configLookup.contains(item);\n  }\n\n  containsFast(item) {\n    if (this.configLookup === null) {\n      throw \"This method is not implemented for readonly sets.\";\n    }\n\n    return this.configLookup.containsFast(item);\n  }\n\n  clear() {\n    if (this.readOnly) {\n      throw \"This set is readonly\";\n    }\n\n    this.configs = [];\n    this.cachedHashCode = -1;\n    this.configLookup = new Utils.Set();\n  }\n\n  setReadonly(readOnly) {\n    this.readOnly = readOnly;\n\n    if (readOnly) {\n      this.configLookup = null; // can't mod, no need for lookup cache\n    }\n  }\n\n  toString() {\n    return Utils.arrayToString(this.configs) + (this.hasSemanticContext ? \",hasSemanticContext=\" + this.hasSemanticContext : \"\") + (this.uniqueAlt !== ATN.INVALID_ALT_NUMBER ? \",uniqueAlt=\" + this.uniqueAlt : \"\") + (this.conflictingAlts !== null ? \",conflictingAlts=\" + this.conflictingAlts : \"\") + (this.dipsIntoOuterContext ? \",dipsIntoOuterContext\" : \"\");\n  }\n\n  get items() {\n    return this.configs;\n  }\n\n  get length() {\n    return this.configs.length;\n  }\n\n}\n\nclass OrderedATNConfigSet extends ATNConfigSet {\n  constructor() {\n    super();\n    this.configLookup = new Utils.Set();\n  }\n\n}\n\nmodule.exports = {\n  ATNConfigSet,\n  OrderedATNConfigSet\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/ATNConfigSet.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/ATNDeserializationOptions.js":
/*!*************************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/ATNDeserializationOptions.js ***!
  \*************************************************************************/
/***/ ((module) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nclass ATNDeserializationOptions {\n  constructor(copyFrom) {\n    if (copyFrom === undefined) {\n      copyFrom = null;\n    }\n\n    this.readOnly = false;\n    this.verifyATN = copyFrom === null ? true : copyFrom.verifyATN;\n    this.generateRuleBypassTransitions = copyFrom === null ? false : copyFrom.generateRuleBypassTransitions;\n  }\n\n}\n\nATNDeserializationOptions.defaultOptions = new ATNDeserializationOptions();\nATNDeserializationOptions.defaultOptions.readOnly = true; //    def __setattr__(self, key, value):\n//        if key!=\"readOnly\" and self.readOnly:\n//            raise Exception(\"The object is read only.\")\n//        super(type(self), self).__setattr__(key,value)\n\nmodule.exports = ATNDeserializationOptions;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/ATNDeserializationOptions.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/ATNDeserializer.js":
/*!***************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/ATNDeserializer.js ***!
  \***************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Token\n} = __webpack_require__(/*! ./../Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\nconst ATN = __webpack_require__(/*! ./ATN */ \"./node_modules/antlr4/src/antlr4/atn/ATN.js\");\n\nconst ATNType = __webpack_require__(/*! ./ATNType */ \"./node_modules/antlr4/src/antlr4/atn/ATNType.js\");\n\nconst {\n  ATNState,\n  BasicState,\n  DecisionState,\n  BlockStartState,\n  BlockEndState,\n  LoopEndState,\n  RuleStartState,\n  RuleStopState,\n  TokensStartState,\n  PlusLoopbackState,\n  StarLoopbackState,\n  StarLoopEntryState,\n  PlusBlockStartState,\n  StarBlockStartState,\n  BasicBlockStartState\n} = __webpack_require__(/*! ./ATNState */ \"./node_modules/antlr4/src/antlr4/atn/ATNState.js\");\n\nconst {\n  Transition,\n  AtomTransition,\n  SetTransition,\n  NotSetTransition,\n  RuleTransition,\n  RangeTransition,\n  ActionTransition,\n  EpsilonTransition,\n  WildcardTransition,\n  PredicateTransition,\n  PrecedencePredicateTransition\n} = __webpack_require__(/*! ./Transition */ \"./node_modules/antlr4/src/antlr4/atn/Transition.js\");\n\nconst {\n  IntervalSet\n} = __webpack_require__(/*! ./../IntervalSet */ \"./node_modules/antlr4/src/antlr4/IntervalSet.js\");\n\nconst ATNDeserializationOptions = __webpack_require__(/*! ./ATNDeserializationOptions */ \"./node_modules/antlr4/src/antlr4/atn/ATNDeserializationOptions.js\");\n\nconst {\n  LexerActionType,\n  LexerSkipAction,\n  LexerChannelAction,\n  LexerCustomAction,\n  LexerMoreAction,\n  LexerTypeAction,\n  LexerPushModeAction,\n  LexerPopModeAction,\n  LexerModeAction\n} = __webpack_require__(/*! ./LexerAction */ \"./node_modules/antlr4/src/antlr4/atn/LexerAction.js\"); // This is the earliest supported serialized UUID.\n// stick to serialized version for now, we don't need a UUID instance\n\n\nconst BASE_SERIALIZED_UUID = \"AADB8D7E-AEEF-4415-AD2B-8204D6CF042E\"; //\n// This UUID indicates the serialized ATN contains two sets of\n// IntervalSets, where the second set's values are encoded as\n// 32-bit integers to support the full Unicode SMP range up to U+10FFFF.\n//\n\nconst ADDED_UNICODE_SMP = \"59627784-3BE5-417A-B9EB-8131A7286089\"; // This list contains all of the currently supported UUIDs, ordered by when\n// the feature first appeared in this branch.\n\nconst SUPPORTED_UUIDS = [BASE_SERIALIZED_UUID, ADDED_UNICODE_SMP];\nconst SERIALIZED_VERSION = 3; // This is the current serialized UUID.\n\nconst SERIALIZED_UUID = ADDED_UNICODE_SMP;\n\nfunction initArray(length, value) {\n  const tmp = [];\n  tmp[length - 1] = value;\n  return tmp.map(function (i) {\n    return value;\n  });\n}\n\nclass ATNDeserializer {\n  constructor(options) {\n    if (options === undefined || options === null) {\n      options = ATNDeserializationOptions.defaultOptions;\n    }\n\n    this.deserializationOptions = options;\n    this.stateFactories = null;\n    this.actionFactories = null;\n  }\n  /**\n   * Determines if a particular serialized representation of an ATN supports\n   * a particular feature, identified by the {@link UUID} used for serializing\n   * the ATN at the time the feature was first introduced.\n   *\n   * @param feature The {@link UUID} marking the first time the feature was\n   * supported in the serialized ATN.\n   * @param actualUuid The {@link UUID} of the actual serialized ATN which is\n   * currently being deserialized.\n   * @return {@code true} if the {@code actualUuid} value represents a\n   * serialized ATN at or after the feature identified by {@code feature} was\n   * introduced; otherwise, {@code false}.\n  */\n\n\n  isFeatureSupported(feature, actualUuid) {\n    const idx1 = SUPPORTED_UUIDS.indexOf(feature);\n\n    if (idx1 < 0) {\n      return false;\n    }\n\n    const idx2 = SUPPORTED_UUIDS.indexOf(actualUuid);\n    return idx2 >= idx1;\n  }\n\n  deserialize(data) {\n    this.reset(data);\n    this.checkVersion();\n    this.checkUUID();\n    const atn = this.readATN();\n    this.readStates(atn);\n    this.readRules(atn);\n    this.readModes(atn);\n    const sets = []; // First, deserialize sets with 16-bit arguments <= U+FFFF.\n\n    this.readSets(atn, sets, this.readInt.bind(this)); // Next, if the ATN was serialized with the Unicode SMP feature,\n    // deserialize sets with 32-bit arguments <= U+10FFFF.\n\n    if (this.isFeatureSupported(ADDED_UNICODE_SMP, this.uuid)) {\n      this.readSets(atn, sets, this.readInt32.bind(this));\n    }\n\n    this.readEdges(atn, sets);\n    this.readDecisions(atn);\n    this.readLexerActions(atn);\n    this.markPrecedenceDecisions(atn);\n    this.verifyATN(atn);\n\n    if (this.deserializationOptions.generateRuleBypassTransitions && atn.grammarType === ATNType.PARSER) {\n      this.generateRuleBypassTransitions(atn); // re-verify after modification\n\n      this.verifyATN(atn);\n    }\n\n    return atn;\n  }\n\n  reset(data) {\n    const adjust = function (c) {\n      const v = c.charCodeAt(0);\n      return v > 1 ? v - 2 : v + 65534;\n    };\n\n    const temp = data.split(\"\").map(adjust); // don't adjust the first value since that's the version number\n\n    temp[0] = data.charCodeAt(0);\n    this.data = temp;\n    this.pos = 0;\n  }\n\n  checkVersion() {\n    const version = this.readInt();\n\n    if (version !== SERIALIZED_VERSION) {\n      throw \"Could not deserialize ATN with version \" + version + \" (expected \" + SERIALIZED_VERSION + \").\";\n    }\n  }\n\n  checkUUID() {\n    const uuid = this.readUUID();\n\n    if (SUPPORTED_UUIDS.indexOf(uuid) < 0) {\n      throw \"Could not deserialize ATN with UUID: \" + uuid + \" (expected \" + SERIALIZED_UUID + \" or a legacy UUID).\", uuid, SERIALIZED_UUID;\n    }\n\n    this.uuid = uuid;\n  }\n\n  readATN() {\n    const grammarType = this.readInt();\n    const maxTokenType = this.readInt();\n    return new ATN(grammarType, maxTokenType);\n  }\n\n  readStates(atn) {\n    let j, pair, stateNumber;\n    const loopBackStateNumbers = [];\n    const endStateNumbers = [];\n    const nstates = this.readInt();\n\n    for (let i = 0; i < nstates; i++) {\n      const stype = this.readInt(); // ignore bad type of states\n\n      if (stype === ATNState.INVALID_TYPE) {\n        atn.addState(null);\n        continue;\n      }\n\n      let ruleIndex = this.readInt();\n\n      if (ruleIndex === 0xFFFF) {\n        ruleIndex = -1;\n      }\n\n      const s = this.stateFactory(stype, ruleIndex);\n\n      if (stype === ATNState.LOOP_END) {\n        // special case\n        const loopBackStateNumber = this.readInt();\n        loopBackStateNumbers.push([s, loopBackStateNumber]);\n      } else if (s instanceof BlockStartState) {\n        const endStateNumber = this.readInt();\n        endStateNumbers.push([s, endStateNumber]);\n      }\n\n      atn.addState(s);\n    } // delay the assignment of loop back and end states until we know all the\n    // state instances have been initialized\n\n\n    for (j = 0; j < loopBackStateNumbers.length; j++) {\n      pair = loopBackStateNumbers[j];\n      pair[0].loopBackState = atn.states[pair[1]];\n    }\n\n    for (j = 0; j < endStateNumbers.length; j++) {\n      pair = endStateNumbers[j];\n      pair[0].endState = atn.states[pair[1]];\n    }\n\n    let numNonGreedyStates = this.readInt();\n\n    for (j = 0; j < numNonGreedyStates; j++) {\n      stateNumber = this.readInt();\n      atn.states[stateNumber].nonGreedy = true;\n    }\n\n    let numPrecedenceStates = this.readInt();\n\n    for (j = 0; j < numPrecedenceStates; j++) {\n      stateNumber = this.readInt();\n      atn.states[stateNumber].isPrecedenceRule = true;\n    }\n  }\n\n  readRules(atn) {\n    let i;\n    const nrules = this.readInt();\n\n    if (atn.grammarType === ATNType.LEXER) {\n      atn.ruleToTokenType = initArray(nrules, 0);\n    }\n\n    atn.ruleToStartState = initArray(nrules, 0);\n\n    for (i = 0; i < nrules; i++) {\n      const s = this.readInt();\n      atn.ruleToStartState[i] = atn.states[s];\n\n      if (atn.grammarType === ATNType.LEXER) {\n        let tokenType = this.readInt();\n\n        if (tokenType === 0xFFFF) {\n          tokenType = Token.EOF;\n        }\n\n        atn.ruleToTokenType[i] = tokenType;\n      }\n    }\n\n    atn.ruleToStopState = initArray(nrules, 0);\n\n    for (i = 0; i < atn.states.length; i++) {\n      const state = atn.states[i];\n\n      if (!(state instanceof RuleStopState)) {\n        continue;\n      }\n\n      atn.ruleToStopState[state.ruleIndex] = state;\n      atn.ruleToStartState[state.ruleIndex].stopState = state;\n    }\n  }\n\n  readModes(atn) {\n    const nmodes = this.readInt();\n\n    for (let i = 0; i < nmodes; i++) {\n      let s = this.readInt();\n      atn.modeToStartState.push(atn.states[s]);\n    }\n  }\n\n  readSets(atn, sets, readUnicode) {\n    const m = this.readInt();\n\n    for (let i = 0; i < m; i++) {\n      const iset = new IntervalSet();\n      sets.push(iset);\n      const n = this.readInt();\n      const containsEof = this.readInt();\n\n      if (containsEof !== 0) {\n        iset.addOne(-1);\n      }\n\n      for (let j = 0; j < n; j++) {\n        const i1 = readUnicode();\n        const i2 = readUnicode();\n        iset.addRange(i1, i2);\n      }\n    }\n  }\n\n  readEdges(atn, sets) {\n    let i, j, state, trans, target;\n    const nedges = this.readInt();\n\n    for (i = 0; i < nedges; i++) {\n      const src = this.readInt();\n      const trg = this.readInt();\n      const ttype = this.readInt();\n      const arg1 = this.readInt();\n      const arg2 = this.readInt();\n      const arg3 = this.readInt();\n      trans = this.edgeFactory(atn, ttype, src, trg, arg1, arg2, arg3, sets);\n      const srcState = atn.states[src];\n      srcState.addTransition(trans);\n    } // edges for rule stop states can be derived, so they aren't serialized\n\n\n    for (i = 0; i < atn.states.length; i++) {\n      state = atn.states[i];\n\n      for (j = 0; j < state.transitions.length; j++) {\n        const t = state.transitions[j];\n\n        if (!(t instanceof RuleTransition)) {\n          continue;\n        }\n\n        let outermostPrecedenceReturn = -1;\n\n        if (atn.ruleToStartState[t.target.ruleIndex].isPrecedenceRule) {\n          if (t.precedence === 0) {\n            outermostPrecedenceReturn = t.target.ruleIndex;\n          }\n        }\n\n        trans = new EpsilonTransition(t.followState, outermostPrecedenceReturn);\n        atn.ruleToStopState[t.target.ruleIndex].addTransition(trans);\n      }\n    }\n\n    for (i = 0; i < atn.states.length; i++) {\n      state = atn.states[i];\n\n      if (state instanceof BlockStartState) {\n        // we need to know the end state to set its start state\n        if (state.endState === null) {\n          throw \"IllegalState\";\n        } // block end states can only be associated to a single block start\n        // state\n\n\n        if (state.endState.startState !== null) {\n          throw \"IllegalState\";\n        }\n\n        state.endState.startState = state;\n      }\n\n      if (state instanceof PlusLoopbackState) {\n        for (j = 0; j < state.transitions.length; j++) {\n          target = state.transitions[j].target;\n\n          if (target instanceof PlusBlockStartState) {\n            target.loopBackState = state;\n          }\n        }\n      } else if (state instanceof StarLoopbackState) {\n        for (j = 0; j < state.transitions.length; j++) {\n          target = state.transitions[j].target;\n\n          if (target instanceof StarLoopEntryState) {\n            target.loopBackState = state;\n          }\n        }\n      }\n    }\n  }\n\n  readDecisions(atn) {\n    const ndecisions = this.readInt();\n\n    for (let i = 0; i < ndecisions; i++) {\n      const s = this.readInt();\n      const decState = atn.states[s];\n      atn.decisionToState.push(decState);\n      decState.decision = i;\n    }\n  }\n\n  readLexerActions(atn) {\n    if (atn.grammarType === ATNType.LEXER) {\n      const count = this.readInt();\n      atn.lexerActions = initArray(count, null);\n\n      for (let i = 0; i < count; i++) {\n        const actionType = this.readInt();\n        let data1 = this.readInt();\n\n        if (data1 === 0xFFFF) {\n          data1 = -1;\n        }\n\n        let data2 = this.readInt();\n\n        if (data2 === 0xFFFF) {\n          data2 = -1;\n        }\n\n        atn.lexerActions[i] = this.lexerActionFactory(actionType, data1, data2);\n      }\n    }\n  }\n\n  generateRuleBypassTransitions(atn) {\n    let i;\n    const count = atn.ruleToStartState.length;\n\n    for (i = 0; i < count; i++) {\n      atn.ruleToTokenType[i] = atn.maxTokenType + i + 1;\n    }\n\n    for (i = 0; i < count; i++) {\n      this.generateRuleBypassTransition(atn, i);\n    }\n  }\n\n  generateRuleBypassTransition(atn, idx) {\n    let i, state;\n    const bypassStart = new BasicBlockStartState();\n    bypassStart.ruleIndex = idx;\n    atn.addState(bypassStart);\n    const bypassStop = new BlockEndState();\n    bypassStop.ruleIndex = idx;\n    atn.addState(bypassStop);\n    bypassStart.endState = bypassStop;\n    atn.defineDecisionState(bypassStart);\n    bypassStop.startState = bypassStart;\n    let excludeTransition = null;\n    let endState = null;\n\n    if (atn.ruleToStartState[idx].isPrecedenceRule) {\n      // wrap from the beginning of the rule to the StarLoopEntryState\n      endState = null;\n\n      for (i = 0; i < atn.states.length; i++) {\n        state = atn.states[i];\n\n        if (this.stateIsEndStateFor(state, idx)) {\n          endState = state;\n          excludeTransition = state.loopBackState.transitions[0];\n          break;\n        }\n      }\n\n      if (excludeTransition === null) {\n        throw \"Couldn't identify final state of the precedence rule prefix section.\";\n      }\n    } else {\n      endState = atn.ruleToStopState[idx];\n    } // all non-excluded transitions that currently target end state need to\n    // target blockEnd instead\n\n\n    for (i = 0; i < atn.states.length; i++) {\n      state = atn.states[i];\n\n      for (let j = 0; j < state.transitions.length; j++) {\n        const transition = state.transitions[j];\n\n        if (transition === excludeTransition) {\n          continue;\n        }\n\n        if (transition.target === endState) {\n          transition.target = bypassStop;\n        }\n      }\n    } // all transitions leaving the rule start state need to leave blockStart\n    // instead\n\n\n    const ruleToStartState = atn.ruleToStartState[idx];\n    const count = ruleToStartState.transitions.length;\n\n    while (count > 0) {\n      bypassStart.addTransition(ruleToStartState.transitions[count - 1]);\n      ruleToStartState.transitions = ruleToStartState.transitions.slice(-1);\n    } // link the new states\n\n\n    atn.ruleToStartState[idx].addTransition(new EpsilonTransition(bypassStart));\n    bypassStop.addTransition(new EpsilonTransition(endState));\n    const matchState = new BasicState();\n    atn.addState(matchState);\n    matchState.addTransition(new AtomTransition(bypassStop, atn.ruleToTokenType[idx]));\n    bypassStart.addTransition(new EpsilonTransition(matchState));\n  }\n\n  stateIsEndStateFor(state, idx) {\n    if (state.ruleIndex !== idx) {\n      return null;\n    }\n\n    if (!(state instanceof StarLoopEntryState)) {\n      return null;\n    }\n\n    const maybeLoopEndState = state.transitions[state.transitions.length - 1].target;\n\n    if (!(maybeLoopEndState instanceof LoopEndState)) {\n      return null;\n    }\n\n    if (maybeLoopEndState.epsilonOnlyTransitions && maybeLoopEndState.transitions[0].target instanceof RuleStopState) {\n      return state;\n    } else {\n      return null;\n    }\n  }\n  /**\n   * Analyze the {@link StarLoopEntryState} states in the specified ATN to set\n   * the {@link StarLoopEntryState//isPrecedenceDecision} field to the\n   * correct value.\n   * @param atn The ATN.\n   */\n\n\n  markPrecedenceDecisions(atn) {\n    for (let i = 0; i < atn.states.length; i++) {\n      const state = atn.states[i];\n\n      if (!(state instanceof StarLoopEntryState)) {\n        continue;\n      } // We analyze the ATN to determine if this ATN decision state is the\n      // decision for the closure block that determines whether a\n      // precedence rule should continue or complete.\n\n\n      if (atn.ruleToStartState[state.ruleIndex].isPrecedenceRule) {\n        const maybeLoopEndState = state.transitions[state.transitions.length - 1].target;\n\n        if (maybeLoopEndState instanceof LoopEndState) {\n          if (maybeLoopEndState.epsilonOnlyTransitions && maybeLoopEndState.transitions[0].target instanceof RuleStopState) {\n            state.isPrecedenceDecision = true;\n          }\n        }\n      }\n    }\n  }\n\n  verifyATN(atn) {\n    if (!this.deserializationOptions.verifyATN) {\n      return;\n    } // verify assumptions\n\n\n    for (let i = 0; i < atn.states.length; i++) {\n      const state = atn.states[i];\n\n      if (state === null) {\n        continue;\n      }\n\n      this.checkCondition(state.epsilonOnlyTransitions || state.transitions.length <= 1);\n\n      if (state instanceof PlusBlockStartState) {\n        this.checkCondition(state.loopBackState !== null);\n      } else if (state instanceof StarLoopEntryState) {\n        this.checkCondition(state.loopBackState !== null);\n        this.checkCondition(state.transitions.length === 2);\n\n        if (state.transitions[0].target instanceof StarBlockStartState) {\n          this.checkCondition(state.transitions[1].target instanceof LoopEndState);\n          this.checkCondition(!state.nonGreedy);\n        } else if (state.transitions[0].target instanceof LoopEndState) {\n          this.checkCondition(state.transitions[1].target instanceof StarBlockStartState);\n          this.checkCondition(state.nonGreedy);\n        } else {\n          throw \"IllegalState\";\n        }\n      } else if (state instanceof StarLoopbackState) {\n        this.checkCondition(state.transitions.length === 1);\n        this.checkCondition(state.transitions[0].target instanceof StarLoopEntryState);\n      } else if (state instanceof LoopEndState) {\n        this.checkCondition(state.loopBackState !== null);\n      } else if (state instanceof RuleStartState) {\n        this.checkCondition(state.stopState !== null);\n      } else if (state instanceof BlockStartState) {\n        this.checkCondition(state.endState !== null);\n      } else if (state instanceof BlockEndState) {\n        this.checkCondition(state.startState !== null);\n      } else if (state instanceof DecisionState) {\n        this.checkCondition(state.transitions.length <= 1 || state.decision >= 0);\n      } else {\n        this.checkCondition(state.transitions.length <= 1 || state instanceof RuleStopState);\n      }\n    }\n  }\n\n  checkCondition(condition, message) {\n    if (!condition) {\n      if (message === undefined || message === null) {\n        message = \"IllegalState\";\n      }\n\n      throw message;\n    }\n  }\n\n  readInt() {\n    return this.data[this.pos++];\n  }\n\n  readInt32() {\n    const low = this.readInt();\n    const high = this.readInt();\n    return low | high << 16;\n  }\n\n  readLong() {\n    const low = this.readInt32();\n    const high = this.readInt32();\n    return low & 0x00000000FFFFFFFF | high << 32;\n  }\n\n  readUUID() {\n    const bb = [];\n\n    for (let i = 7; i >= 0; i--) {\n      const int = this.readInt();\n      /* jshint bitwise: false */\n\n      bb[2 * i + 1] = int & 0xFF;\n      bb[2 * i] = int >> 8 & 0xFF;\n    }\n\n    return byteToHex[bb[0]] + byteToHex[bb[1]] + byteToHex[bb[2]] + byteToHex[bb[3]] + '-' + byteToHex[bb[4]] + byteToHex[bb[5]] + '-' + byteToHex[bb[6]] + byteToHex[bb[7]] + '-' + byteToHex[bb[8]] + byteToHex[bb[9]] + '-' + byteToHex[bb[10]] + byteToHex[bb[11]] + byteToHex[bb[12]] + byteToHex[bb[13]] + byteToHex[bb[14]] + byteToHex[bb[15]];\n  }\n\n  edgeFactory(atn, type, src, trg, arg1, arg2, arg3, sets) {\n    const target = atn.states[trg];\n\n    switch (type) {\n      case Transition.EPSILON:\n        return new EpsilonTransition(target);\n\n      case Transition.RANGE:\n        return arg3 !== 0 ? new RangeTransition(target, Token.EOF, arg2) : new RangeTransition(target, arg1, arg2);\n\n      case Transition.RULE:\n        return new RuleTransition(atn.states[arg1], arg2, arg3, target);\n\n      case Transition.PREDICATE:\n        return new PredicateTransition(target, arg1, arg2, arg3 !== 0);\n\n      case Transition.PRECEDENCE:\n        return new PrecedencePredicateTransition(target, arg1);\n\n      case Transition.ATOM:\n        return arg3 !== 0 ? new AtomTransition(target, Token.EOF) : new AtomTransition(target, arg1);\n\n      case Transition.ACTION:\n        return new ActionTransition(target, arg1, arg2, arg3 !== 0);\n\n      case Transition.SET:\n        return new SetTransition(target, sets[arg1]);\n\n      case Transition.NOT_SET:\n        return new NotSetTransition(target, sets[arg1]);\n\n      case Transition.WILDCARD:\n        return new WildcardTransition(target);\n\n      default:\n        throw \"The specified transition type: \" + type + \" is not valid.\";\n    }\n  }\n\n  stateFactory(type, ruleIndex) {\n    if (this.stateFactories === null) {\n      const sf = [];\n      sf[ATNState.INVALID_TYPE] = null;\n\n      sf[ATNState.BASIC] = () => new BasicState();\n\n      sf[ATNState.RULE_START] = () => new RuleStartState();\n\n      sf[ATNState.BLOCK_START] = () => new BasicBlockStartState();\n\n      sf[ATNState.PLUS_BLOCK_START] = () => new PlusBlockStartState();\n\n      sf[ATNState.STAR_BLOCK_START] = () => new StarBlockStartState();\n\n      sf[ATNState.TOKEN_START] = () => new TokensStartState();\n\n      sf[ATNState.RULE_STOP] = () => new RuleStopState();\n\n      sf[ATNState.BLOCK_END] = () => new BlockEndState();\n\n      sf[ATNState.STAR_LOOP_BACK] = () => new StarLoopbackState();\n\n      sf[ATNState.STAR_LOOP_ENTRY] = () => new StarLoopEntryState();\n\n      sf[ATNState.PLUS_LOOP_BACK] = () => new PlusLoopbackState();\n\n      sf[ATNState.LOOP_END] = () => new LoopEndState();\n\n      this.stateFactories = sf;\n    }\n\n    if (type > this.stateFactories.length || this.stateFactories[type] === null) {\n      throw \"The specified state type \" + type + \" is not valid.\";\n    } else {\n      const s = this.stateFactories[type]();\n\n      if (s !== null) {\n        s.ruleIndex = ruleIndex;\n        return s;\n      }\n    }\n  }\n\n  lexerActionFactory(type, data1, data2) {\n    if (this.actionFactories === null) {\n      const af = [];\n\n      af[LexerActionType.CHANNEL] = (data1, data2) => new LexerChannelAction(data1);\n\n      af[LexerActionType.CUSTOM] = (data1, data2) => new LexerCustomAction(data1, data2);\n\n      af[LexerActionType.MODE] = (data1, data2) => new LexerModeAction(data1);\n\n      af[LexerActionType.MORE] = (data1, data2) => LexerMoreAction.INSTANCE;\n\n      af[LexerActionType.POP_MODE] = (data1, data2) => LexerPopModeAction.INSTANCE;\n\n      af[LexerActionType.PUSH_MODE] = (data1, data2) => new LexerPushModeAction(data1);\n\n      af[LexerActionType.SKIP] = (data1, data2) => LexerSkipAction.INSTANCE;\n\n      af[LexerActionType.TYPE] = (data1, data2) => new LexerTypeAction(data1);\n\n      this.actionFactories = af;\n    }\n\n    if (type > this.actionFactories.length || this.actionFactories[type] === null) {\n      throw \"The specified lexer action type \" + type + \" is not valid.\";\n    } else {\n      return this.actionFactories[type](data1, data2);\n    }\n  }\n\n}\n\nfunction createByteToHex() {\n  const bth = [];\n\n  for (let i = 0; i < 256; i++) {\n    bth[i] = (i + 0x100).toString(16).substr(1).toUpperCase();\n  }\n\n  return bth;\n}\n\nconst byteToHex = createByteToHex();\nmodule.exports = ATNDeserializer;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/ATNDeserializer.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/ATNSimulator.js":
/*!************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/ATNSimulator.js ***!
  \************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  DFAState\n} = __webpack_require__(/*! ./../dfa/DFAState */ \"./node_modules/antlr4/src/antlr4/dfa/DFAState.js\");\n\nconst {\n  ATNConfigSet\n} = __webpack_require__(/*! ./ATNConfigSet */ \"./node_modules/antlr4/src/antlr4/atn/ATNConfigSet.js\");\n\nconst {\n  getCachedPredictionContext\n} = __webpack_require__(/*! ./../PredictionContext */ \"./node_modules/antlr4/src/antlr4/PredictionContext.js\");\n\nconst {\n  Map\n} = __webpack_require__(/*! ./../Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n\nclass ATNSimulator {\n  constructor(atn, sharedContextCache) {\n    /**\n     * The context cache maps all PredictionContext objects that are ==\n     * to a single cached copy. This cache is shared across all contexts\n     * in all ATNConfigs in all DFA states.  We rebuild each ATNConfigSet\n     * to use only cached nodes/graphs in addDFAState(). We don't want to\n     * fill this during closure() since there are lots of contexts that\n     * pop up but are not used ever again. It also greatly slows down closure().\n     *\n     * <p>This cache makes a huge difference in memory and a little bit in speed.\n     * For the Java grammar on java.*, it dropped the memory requirements\n     * at the end from 25M to 16M. We don't store any of the full context\n     * graphs in the DFA because they are limited to local context only,\n     * but apparently there's a lot of repetition there as well. We optimize\n     * the config contexts before storing the config set in the DFA states\n     * by literally rebuilding them with cached subgraphs only.</p>\n     *\n     * <p>I tried a cache for use during closure operations, that was\n     * whacked after each adaptivePredict(). It cost a little bit\n     * more time I think and doesn't save on the overall footprint\n     * so it's not worth the complexity.</p>\n     */\n    this.atn = atn;\n    this.sharedContextCache = sharedContextCache;\n    return this;\n  }\n\n  getCachedContext(context) {\n    if (this.sharedContextCache === null) {\n      return context;\n    }\n\n    const visited = new Map();\n    return getCachedPredictionContext(context, this.sharedContextCache, visited);\n  }\n\n} // Must distinguish between missing edge and edge we know leads nowhere///\n\n\nATNSimulator.ERROR = new DFAState(0x7FFFFFFF, new ATNConfigSet());\nmodule.exports = ATNSimulator;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/ATNSimulator.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/ATNState.js":
/*!********************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/ATNState.js ***!
  \********************************************************/
/***/ ((module) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst INITIAL_NUM_TRANSITIONS = 4;\n/**\n * The following images show the relation of states and\n * {@link ATNState//transitions} for various grammar constructs.\n *\n * <ul>\n *\n * <li>Solid edges marked with an &//0949; indicate a required\n * {@link EpsilonTransition}.</li>\n *\n * <li>Dashed edges indicate locations where any transition derived from\n * {@link Transition} might appear.</li>\n *\n * <li>Dashed nodes are place holders for either a sequence of linked\n * {@link BasicState} states or the inclusion of a block representing a nested\n * construct in one of the forms below.</li>\n *\n * <li>Nodes showing multiple outgoing alternatives with a {@code ...} support\n * any number of alternatives (one or more). Nodes without the {@code ...} only\n * support the exact number of alternatives shown in the diagram.</li>\n *\n * </ul>\n *\n * <h2>Basic Blocks</h2>\n *\n * <h3>Rule</h3>\n *\n * <embed src=\"images/Rule.svg\" type=\"image/svg+xml\"/>\n *\n * <h3>Block of 1 or more alternatives</h3>\n *\n * <embed src=\"images/Block.svg\" type=\"image/svg+xml\"/>\n *\n * <h2>Greedy Loops</h2>\n *\n * <h3>Greedy Closure: {@code (...)*}</h3>\n *\n * <embed src=\"images/ClosureGreedy.svg\" type=\"image/svg+xml\"/>\n *\n * <h3>Greedy Positive Closure: {@code (...)+}</h3>\n *\n * <embed src=\"images/PositiveClosureGreedy.svg\" type=\"image/svg+xml\"/>\n *\n * <h3>Greedy Optional: {@code (...)?}</h3>\n *\n * <embed src=\"images/OptionalGreedy.svg\" type=\"image/svg+xml\"/>\n *\n * <h2>Non-Greedy Loops</h2>\n *\n * <h3>Non-Greedy Closure: {@code (...)*?}</h3>\n *\n * <embed src=\"images/ClosureNonGreedy.svg\" type=\"image/svg+xml\"/>\n *\n * <h3>Non-Greedy Positive Closure: {@code (...)+?}</h3>\n *\n * <embed src=\"images/PositiveClosureNonGreedy.svg\" type=\"image/svg+xml\"/>\n *\n * <h3>Non-Greedy Optional: {@code (...)??}</h3>\n *\n * <embed src=\"images/OptionalNonGreedy.svg\" type=\"image/svg+xml\"/>\n */\n\nclass ATNState {\n  constructor() {\n    // Which ATN are we in?\n    this.atn = null;\n    this.stateNumber = ATNState.INVALID_STATE_NUMBER;\n    this.stateType = null;\n    this.ruleIndex = 0; // at runtime, we don't have Rule objects\n\n    this.epsilonOnlyTransitions = false; // Track the transitions emanating from this ATN state.\n\n    this.transitions = []; // Used to cache lookahead during parsing, not used during construction\n\n    this.nextTokenWithinRule = null;\n  }\n\n  toString() {\n    return this.stateNumber;\n  }\n\n  equals(other) {\n    if (other instanceof ATNState) {\n      return this.stateNumber === other.stateNumber;\n    } else {\n      return false;\n    }\n  }\n\n  isNonGreedyExitState() {\n    return false;\n  }\n\n  addTransition(trans, index) {\n    if (index === undefined) {\n      index = -1;\n    }\n\n    if (this.transitions.length === 0) {\n      this.epsilonOnlyTransitions = trans.isEpsilon;\n    } else if (this.epsilonOnlyTransitions !== trans.isEpsilon) {\n      this.epsilonOnlyTransitions = false;\n    }\n\n    if (index === -1) {\n      this.transitions.push(trans);\n    } else {\n      this.transitions.splice(index, 1, trans);\n    }\n  }\n\n} // constants for serialization\n\n\nATNState.INVALID_TYPE = 0;\nATNState.BASIC = 1;\nATNState.RULE_START = 2;\nATNState.BLOCK_START = 3;\nATNState.PLUS_BLOCK_START = 4;\nATNState.STAR_BLOCK_START = 5;\nATNState.TOKEN_START = 6;\nATNState.RULE_STOP = 7;\nATNState.BLOCK_END = 8;\nATNState.STAR_LOOP_BACK = 9;\nATNState.STAR_LOOP_ENTRY = 10;\nATNState.PLUS_LOOP_BACK = 11;\nATNState.LOOP_END = 12;\nATNState.serializationNames = [\"INVALID\", \"BASIC\", \"RULE_START\", \"BLOCK_START\", \"PLUS_BLOCK_START\", \"STAR_BLOCK_START\", \"TOKEN_START\", \"RULE_STOP\", \"BLOCK_END\", \"STAR_LOOP_BACK\", \"STAR_LOOP_ENTRY\", \"PLUS_LOOP_BACK\", \"LOOP_END\"];\nATNState.INVALID_STATE_NUMBER = -1;\n\nclass BasicState extends ATNState {\n  constructor() {\n    super();\n    this.stateType = ATNState.BASIC;\n  }\n\n}\n\nclass DecisionState extends ATNState {\n  constructor() {\n    super();\n    this.decision = -1;\n    this.nonGreedy = false;\n    return this;\n  }\n\n}\n/**\n *  The start of a regular {@code (...)} block\n */\n\n\nclass BlockStartState extends DecisionState {\n  constructor() {\n    super();\n    this.endState = null;\n    return this;\n  }\n\n}\n\nclass BasicBlockStartState extends BlockStartState {\n  constructor() {\n    super();\n    this.stateType = ATNState.BLOCK_START;\n    return this;\n  }\n\n}\n/**\n * Terminal node of a simple {@code (a|b|c)} block\n */\n\n\nclass BlockEndState extends ATNState {\n  constructor() {\n    super();\n    this.stateType = ATNState.BLOCK_END;\n    this.startState = null;\n    return this;\n  }\n\n}\n/**\n * The last node in the ATN for a rule, unless that rule is the start symbol.\n * In that case, there is one transition to EOF. Later, we might encode\n * references to all calls to this rule to compute FOLLOW sets for\n * error handling\n */\n\n\nclass RuleStopState extends ATNState {\n  constructor() {\n    super();\n    this.stateType = ATNState.RULE_STOP;\n    return this;\n  }\n\n}\n\nclass RuleStartState extends ATNState {\n  constructor() {\n    super();\n    this.stateType = ATNState.RULE_START;\n    this.stopState = null;\n    this.isPrecedenceRule = false;\n    return this;\n  }\n\n}\n/**\n * Decision state for {@code A+} and {@code (A|B)+}.  It has two transitions:\n * one to the loop back to start of the block and one to exit.\n */\n\n\nclass PlusLoopbackState extends DecisionState {\n  constructor() {\n    super();\n    this.stateType = ATNState.PLUS_LOOP_BACK;\n    return this;\n  }\n\n}\n/**\n * Start of {@code (A|B|...)+} loop. Technically a decision state, but\n * we don't use for code generation; somebody might need it, so I'm defining\n * it for completeness. In reality, the {@link PlusLoopbackState} node is the\n * real decision-making note for {@code A+}\n */\n\n\nclass PlusBlockStartState extends BlockStartState {\n  constructor() {\n    super();\n    this.stateType = ATNState.PLUS_BLOCK_START;\n    this.loopBackState = null;\n    return this;\n  }\n\n}\n/**\n * The block that begins a closure loop\n */\n\n\nclass StarBlockStartState extends BlockStartState {\n  constructor() {\n    super();\n    this.stateType = ATNState.STAR_BLOCK_START;\n    return this;\n  }\n\n}\n\nclass StarLoopbackState extends ATNState {\n  constructor() {\n    super();\n    this.stateType = ATNState.STAR_LOOP_BACK;\n    return this;\n  }\n\n}\n\nclass StarLoopEntryState extends DecisionState {\n  constructor() {\n    super();\n    this.stateType = ATNState.STAR_LOOP_ENTRY;\n    this.loopBackState = null; // Indicates whether this state can benefit from a precedence DFA during SLL decision making.\n\n    this.isPrecedenceDecision = null;\n    return this;\n  }\n\n}\n/**\n * Mark the end of a * or + loop\n */\n\n\nclass LoopEndState extends ATNState {\n  constructor() {\n    super();\n    this.stateType = ATNState.LOOP_END;\n    this.loopBackState = null;\n    return this;\n  }\n\n}\n/**\n * The Tokens rule start state linking to each lexer rule start state\n */\n\n\nclass TokensStartState extends DecisionState {\n  constructor() {\n    super();\n    this.stateType = ATNState.TOKEN_START;\n    return this;\n  }\n\n}\n\nmodule.exports = {\n  ATNState,\n  BasicState,\n  DecisionState,\n  BlockStartState,\n  BlockEndState,\n  LoopEndState,\n  RuleStartState,\n  RuleStopState,\n  TokensStartState,\n  PlusLoopbackState,\n  StarLoopbackState,\n  StarLoopEntryState,\n  PlusBlockStartState,\n  StarBlockStartState,\n  BasicBlockStartState\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/ATNState.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/ATNType.js":
/*!*******************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/ATNType.js ***!
  \*******************************************************/
/***/ ((module) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\n\n/**\n * Represents the type of recognizer an ATN applies to\n */\nmodule.exports = {\n  LEXER: 0,\n  PARSER: 1\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/ATNType.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/LexerATNSimulator.js":
/*!*****************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/LexerATNSimulator.js ***!
  \*****************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Token\n} = __webpack_require__(/*! ./../Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\nconst Lexer = __webpack_require__(/*! ./../Lexer */ \"./node_modules/antlr4/src/antlr4/Lexer.js\");\n\nconst ATN = __webpack_require__(/*! ./ATN */ \"./node_modules/antlr4/src/antlr4/atn/ATN.js\");\n\nconst ATNSimulator = __webpack_require__(/*! ./ATNSimulator */ \"./node_modules/antlr4/src/antlr4/atn/ATNSimulator.js\");\n\nconst {\n  DFAState\n} = __webpack_require__(/*! ./../dfa/DFAState */ \"./node_modules/antlr4/src/antlr4/dfa/DFAState.js\");\n\nconst {\n  OrderedATNConfigSet\n} = __webpack_require__(/*! ./ATNConfigSet */ \"./node_modules/antlr4/src/antlr4/atn/ATNConfigSet.js\");\n\nconst {\n  PredictionContext\n} = __webpack_require__(/*! ./../PredictionContext */ \"./node_modules/antlr4/src/antlr4/PredictionContext.js\");\n\nconst {\n  SingletonPredictionContext\n} = __webpack_require__(/*! ./../PredictionContext */ \"./node_modules/antlr4/src/antlr4/PredictionContext.js\");\n\nconst {\n  RuleStopState\n} = __webpack_require__(/*! ./ATNState */ \"./node_modules/antlr4/src/antlr4/atn/ATNState.js\");\n\nconst {\n  LexerATNConfig\n} = __webpack_require__(/*! ./ATNConfig */ \"./node_modules/antlr4/src/antlr4/atn/ATNConfig.js\");\n\nconst {\n  Transition\n} = __webpack_require__(/*! ./Transition */ \"./node_modules/antlr4/src/antlr4/atn/Transition.js\");\n\nconst LexerActionExecutor = __webpack_require__(/*! ./LexerActionExecutor */ \"./node_modules/antlr4/src/antlr4/atn/LexerActionExecutor.js\");\n\nconst {\n  LexerNoViableAltException\n} = __webpack_require__(/*! ./../error/Errors */ \"./node_modules/antlr4/src/antlr4/error/Errors.js\");\n\nfunction resetSimState(sim) {\n  sim.index = -1;\n  sim.line = 0;\n  sim.column = -1;\n  sim.dfaState = null;\n}\n\nclass SimState {\n  constructor() {\n    resetSimState(this);\n  }\n\n  reset() {\n    resetSimState(this);\n  }\n\n}\n\nclass LexerATNSimulator extends ATNSimulator {\n  /**\n   * When we hit an accept state in either the DFA or the ATN, we\n   * have to notify the character stream to start buffering characters\n   * via {@link IntStream//mark} and record the current state. The current sim state\n   * includes the current index into the input, the current line,\n   * and current character position in that line. Note that the Lexer is\n   * tracking the starting line and characterization of the token. These\n   * variables track the \"state\" of the simulator when it hits an accept state.\n   *\n   * <p>We track these variables separately for the DFA and ATN simulation\n   * because the DFA simulation often has to fail over to the ATN\n   * simulation. If the ATN simulation fails, we need the DFA to fall\n   * back to its previously accepted state, if any. If the ATN succeeds,\n   * then the ATN does the accept and the DFA simulator that invoked it\n   * can simply return the predicted token type.</p>\n   */\n  constructor(recog, atn, decisionToDFA, sharedContextCache) {\n    super(atn, sharedContextCache);\n    this.decisionToDFA = decisionToDFA;\n    this.recog = recog;\n    /**\n     * The current token's starting index into the character stream.\n     * Shared across DFA to ATN simulation in case the ATN fails and the\n     * DFA did not have a previous accept state. In this case, we use the\n     * ATN-generated exception object\n     */\n\n    this.startIndex = -1; // line number 1..n within the input///\n\n    this.line = 1;\n    /**\n     * The index of the character relative to the beginning of the line\n     * 0..n-1\n     */\n\n    this.column = 0;\n    this.mode = Lexer.DEFAULT_MODE;\n    /**\n     * Used during DFA/ATN exec to record the most recent accept configuration\n     * info\n     */\n\n    this.prevAccept = new SimState();\n  }\n\n  copyState(simulator) {\n    this.column = simulator.column;\n    this.line = simulator.line;\n    this.mode = simulator.mode;\n    this.startIndex = simulator.startIndex;\n  }\n\n  match(input, mode) {\n    this.match_calls += 1;\n    this.mode = mode;\n    const mark = input.mark();\n\n    try {\n      this.startIndex = input.index;\n      this.prevAccept.reset();\n      const dfa = this.decisionToDFA[mode];\n\n      if (dfa.s0 === null) {\n        return this.matchATN(input);\n      } else {\n        return this.execATN(input, dfa.s0);\n      }\n    } finally {\n      input.release(mark);\n    }\n  }\n\n  reset() {\n    this.prevAccept.reset();\n    this.startIndex = -1;\n    this.line = 1;\n    this.column = 0;\n    this.mode = Lexer.DEFAULT_MODE;\n  }\n\n  matchATN(input) {\n    const startState = this.atn.modeToStartState[this.mode];\n\n    if (LexerATNSimulator.debug) {\n      console.log(\"matchATN mode \" + this.mode + \" start: \" + startState);\n    }\n\n    const old_mode = this.mode;\n    const s0_closure = this.computeStartState(input, startState);\n    const suppressEdge = s0_closure.hasSemanticContext;\n    s0_closure.hasSemanticContext = false;\n    const next = this.addDFAState(s0_closure);\n\n    if (!suppressEdge) {\n      this.decisionToDFA[this.mode].s0 = next;\n    }\n\n    const predict = this.execATN(input, next);\n\n    if (LexerATNSimulator.debug) {\n      console.log(\"DFA after matchATN: \" + this.decisionToDFA[old_mode].toLexerString());\n    }\n\n    return predict;\n  }\n\n  execATN(input, ds0) {\n    if (LexerATNSimulator.debug) {\n      console.log(\"start state closure=\" + ds0.configs);\n    }\n\n    if (ds0.isAcceptState) {\n      // allow zero-length tokens\n      this.captureSimState(this.prevAccept, input, ds0);\n    }\n\n    let t = input.LA(1);\n    let s = ds0; // s is current/from DFA state\n\n    while (true) {\n      // while more work\n      if (LexerATNSimulator.debug) {\n        console.log(\"execATN loop starting closure: \" + s.configs);\n      }\n      /**\n       * As we move src->trg, src->trg, we keep track of the previous trg to\n       * avoid looking up the DFA state again, which is expensive.\n       * If the previous target was already part of the DFA, we might\n       * be able to avoid doing a reach operation upon t. If s!=null,\n       * it means that semantic predicates didn't prevent us from\n       * creating a DFA state. Once we know s!=null, we check to see if\n       * the DFA state has an edge already for t. If so, we can just reuse\n       * it's configuration set; there's no point in re-computing it.\n       * This is kind of like doing DFA simulation within the ATN\n       * simulation because DFA simulation is really just a way to avoid\n       * computing reach/closure sets. Technically, once we know that\n       * we have a previously added DFA state, we could jump over to\n       * the DFA simulator. But, that would mean popping back and forth\n       * a lot and making things more complicated algorithmically.\n       * This optimization makes a lot of sense for loops within DFA.\n       * A character will take us back to an existing DFA state\n       * that already has lots of edges out of it. e.g., .* in comments.\n       * print(\"Target for:\" + str(s) + \" and:\" + str(t))\n       */\n\n\n      let target = this.getExistingTargetState(s, t); // print(\"Existing:\" + str(target))\n\n      if (target === null) {\n        target = this.computeTargetState(input, s, t); // print(\"Computed:\" + str(target))\n      }\n\n      if (target === ATNSimulator.ERROR) {\n        break;\n      } // If this is a consumable input element, make sure to consume before\n      // capturing the accept state so the input index, line, and char\n      // position accurately reflect the state of the interpreter at the\n      // end of the token.\n\n\n      if (t !== Token.EOF) {\n        this.consume(input);\n      }\n\n      if (target.isAcceptState) {\n        this.captureSimState(this.prevAccept, input, target);\n\n        if (t === Token.EOF) {\n          break;\n        }\n      }\n\n      t = input.LA(1);\n      s = target; // flip; current DFA target becomes new src/from state\n    }\n\n    return this.failOrAccept(this.prevAccept, input, s.configs, t);\n  }\n  /**\n   * Get an existing target state for an edge in the DFA. If the target state\n   * for the edge has not yet been computed or is otherwise not available,\n   * this method returns {@code null}.\n   *\n   * @param s The current DFA state\n   * @param t The next input symbol\n   * @return The existing target DFA state for the given input symbol\n   * {@code t}, or {@code null} if the target state for this edge is not\n   * already cached\n   */\n\n\n  getExistingTargetState(s, t) {\n    if (s.edges === null || t < LexerATNSimulator.MIN_DFA_EDGE || t > LexerATNSimulator.MAX_DFA_EDGE) {\n      return null;\n    }\n\n    let target = s.edges[t - LexerATNSimulator.MIN_DFA_EDGE];\n\n    if (target === undefined) {\n      target = null;\n    }\n\n    if (LexerATNSimulator.debug && target !== null) {\n      console.log(\"reuse state \" + s.stateNumber + \" edge to \" + target.stateNumber);\n    }\n\n    return target;\n  }\n  /**\n   * Compute a target state for an edge in the DFA, and attempt to add the\n   * computed state and corresponding edge to the DFA.\n   *\n   * @param input The input stream\n   * @param s The current DFA state\n   * @param t The next input symbol\n   *\n   * @return The computed target DFA state for the given input symbol\n   * {@code t}. If {@code t} does not lead to a valid DFA state, this method\n   * returns {@link //ERROR}.\n   */\n\n\n  computeTargetState(input, s, t) {\n    const reach = new OrderedATNConfigSet(); // if we don't find an existing DFA state\n    // Fill reach starting from closure, following t transitions\n\n    this.getReachableConfigSet(input, s.configs, reach, t);\n\n    if (reach.items.length === 0) {\n      // we got nowhere on t from s\n      if (!reach.hasSemanticContext) {\n        // we got nowhere on t, don't throw out this knowledge; it'd\n        // cause a failover from DFA later.\n        this.addDFAEdge(s, t, ATNSimulator.ERROR);\n      } // stop when we can't match any more char\n\n\n      return ATNSimulator.ERROR;\n    } // Add an edge from s to target DFA found/created for reach\n\n\n    return this.addDFAEdge(s, t, null, reach);\n  }\n\n  failOrAccept(prevAccept, input, reach, t) {\n    if (this.prevAccept.dfaState !== null) {\n      const lexerActionExecutor = prevAccept.dfaState.lexerActionExecutor;\n      this.accept(input, lexerActionExecutor, this.startIndex, prevAccept.index, prevAccept.line, prevAccept.column);\n      return prevAccept.dfaState.prediction;\n    } else {\n      // if no accept and EOF is first char, return EOF\n      if (t === Token.EOF && input.index === this.startIndex) {\n        return Token.EOF;\n      }\n\n      throw new LexerNoViableAltException(this.recog, input, this.startIndex, reach);\n    }\n  }\n  /**\n   * Given a starting configuration set, figure out all ATN configurations\n   * we can reach upon input {@code t}. Parameter {@code reach} is a return\n   * parameter.\n   */\n\n\n  getReachableConfigSet(input, closure, reach, t) {\n    // this is used to skip processing for configs which have a lower priority\n    // than a config that already reached an accept state for the same rule\n    let skipAlt = ATN.INVALID_ALT_NUMBER;\n\n    for (let i = 0; i < closure.items.length; i++) {\n      const cfg = closure.items[i];\n      const currentAltReachedAcceptState = cfg.alt === skipAlt;\n\n      if (currentAltReachedAcceptState && cfg.passedThroughNonGreedyDecision) {\n        continue;\n      }\n\n      if (LexerATNSimulator.debug) {\n        console.log(\"testing %s at %s\\n\", this.getTokenName(t), cfg.toString(this.recog, true));\n      }\n\n      for (let j = 0; j < cfg.state.transitions.length; j++) {\n        const trans = cfg.state.transitions[j]; // for each transition\n\n        const target = this.getReachableTarget(trans, t);\n\n        if (target !== null) {\n          let lexerActionExecutor = cfg.lexerActionExecutor;\n\n          if (lexerActionExecutor !== null) {\n            lexerActionExecutor = lexerActionExecutor.fixOffsetBeforeMatch(input.index - this.startIndex);\n          }\n\n          const treatEofAsEpsilon = t === Token.EOF;\n          const config = new LexerATNConfig({\n            state: target,\n            lexerActionExecutor: lexerActionExecutor\n          }, cfg);\n\n          if (this.closure(input, config, reach, currentAltReachedAcceptState, true, treatEofAsEpsilon)) {\n            // any remaining configs for this alt have a lower priority\n            // than the one that just reached an accept state.\n            skipAlt = cfg.alt;\n          }\n        }\n      }\n    }\n  }\n\n  accept(input, lexerActionExecutor, startIndex, index, line, charPos) {\n    if (LexerATNSimulator.debug) {\n      console.log(\"ACTION %s\\n\", lexerActionExecutor);\n    } // seek to after last char in token\n\n\n    input.seek(index);\n    this.line = line;\n    this.column = charPos;\n\n    if (lexerActionExecutor !== null && this.recog !== null) {\n      lexerActionExecutor.execute(this.recog, input, startIndex);\n    }\n  }\n\n  getReachableTarget(trans, t) {\n    if (trans.matches(t, 0, Lexer.MAX_CHAR_VALUE)) {\n      return trans.target;\n    } else {\n      return null;\n    }\n  }\n\n  computeStartState(input, p) {\n    const initialContext = PredictionContext.EMPTY;\n    const configs = new OrderedATNConfigSet();\n\n    for (let i = 0; i < p.transitions.length; i++) {\n      const target = p.transitions[i].target;\n      const cfg = new LexerATNConfig({\n        state: target,\n        alt: i + 1,\n        context: initialContext\n      }, null);\n      this.closure(input, cfg, configs, false, false, false);\n    }\n\n    return configs;\n  }\n  /**\n   * Since the alternatives within any lexer decision are ordered by\n   * preference, this method stops pursuing the closure as soon as an accept\n   * state is reached. After the first accept state is reached by depth-first\n   * search from {@code config}, all other (potentially reachable) states for\n   * this rule would have a lower priority.\n   *\n   * @return {Boolean} {@code true} if an accept state is reached, otherwise\n   * {@code false}.\n   */\n\n\n  closure(input, config, configs, currentAltReachedAcceptState, speculative, treatEofAsEpsilon) {\n    let cfg = null;\n\n    if (LexerATNSimulator.debug) {\n      console.log(\"closure(\" + config.toString(this.recog, true) + \")\");\n    }\n\n    if (config.state instanceof RuleStopState) {\n      if (LexerATNSimulator.debug) {\n        if (this.recog !== null) {\n          console.log(\"closure at %s rule stop %s\\n\", this.recog.ruleNames[config.state.ruleIndex], config);\n        } else {\n          console.log(\"closure at rule stop %s\\n\", config);\n        }\n      }\n\n      if (config.context === null || config.context.hasEmptyPath()) {\n        if (config.context === null || config.context.isEmpty()) {\n          configs.add(config);\n          return true;\n        } else {\n          configs.add(new LexerATNConfig({\n            state: config.state,\n            context: PredictionContext.EMPTY\n          }, config));\n          currentAltReachedAcceptState = true;\n        }\n      }\n\n      if (config.context !== null && !config.context.isEmpty()) {\n        for (let i = 0; i < config.context.length; i++) {\n          if (config.context.getReturnState(i) !== PredictionContext.EMPTY_RETURN_STATE) {\n            const newContext = config.context.getParent(i); // \"pop\" return state\n\n            const returnState = this.atn.states[config.context.getReturnState(i)];\n            cfg = new LexerATNConfig({\n              state: returnState,\n              context: newContext\n            }, config);\n            currentAltReachedAcceptState = this.closure(input, cfg, configs, currentAltReachedAcceptState, speculative, treatEofAsEpsilon);\n          }\n        }\n      }\n\n      return currentAltReachedAcceptState;\n    } // optimization\n\n\n    if (!config.state.epsilonOnlyTransitions) {\n      if (!currentAltReachedAcceptState || !config.passedThroughNonGreedyDecision) {\n        configs.add(config);\n      }\n    }\n\n    for (let j = 0; j < config.state.transitions.length; j++) {\n      const trans = config.state.transitions[j];\n      cfg = this.getEpsilonTarget(input, config, trans, configs, speculative, treatEofAsEpsilon);\n\n      if (cfg !== null) {\n        currentAltReachedAcceptState = this.closure(input, cfg, configs, currentAltReachedAcceptState, speculative, treatEofAsEpsilon);\n      }\n    }\n\n    return currentAltReachedAcceptState;\n  } // side-effect: can alter configs.hasSemanticContext\n\n\n  getEpsilonTarget(input, config, trans, configs, speculative, treatEofAsEpsilon) {\n    let cfg = null;\n\n    if (trans.serializationType === Transition.RULE) {\n      const newContext = SingletonPredictionContext.create(config.context, trans.followState.stateNumber);\n      cfg = new LexerATNConfig({\n        state: trans.target,\n        context: newContext\n      }, config);\n    } else if (trans.serializationType === Transition.PRECEDENCE) {\n      throw \"Precedence predicates are not supported in lexers.\";\n    } else if (trans.serializationType === Transition.PREDICATE) {\n      // Track traversing semantic predicates. If we traverse,\n      // we cannot add a DFA state for this \"reach\" computation\n      // because the DFA would not test the predicate again in the\n      // future. Rather than creating collections of semantic predicates\n      // like v3 and testing them on prediction, v4 will test them on the\n      // fly all the time using the ATN not the DFA. This is slower but\n      // semantically it's not used that often. One of the key elements to\n      // this predicate mechanism is not adding DFA states that see\n      // predicates immediately afterwards in the ATN. For example,\n      // a : ID {p1}? | ID {p2}? ;\n      // should create the start state for rule 'a' (to save start state\n      // competition), but should not create target of ID state. The\n      // collection of ATN states the following ID references includes\n      // states reached by traversing predicates. Since this is when we\n      // test them, we cannot cash the DFA state target of ID.\n      if (LexerATNSimulator.debug) {\n        console.log(\"EVAL rule \" + trans.ruleIndex + \":\" + trans.predIndex);\n      }\n\n      configs.hasSemanticContext = true;\n\n      if (this.evaluatePredicate(input, trans.ruleIndex, trans.predIndex, speculative)) {\n        cfg = new LexerATNConfig({\n          state: trans.target\n        }, config);\n      }\n    } else if (trans.serializationType === Transition.ACTION) {\n      if (config.context === null || config.context.hasEmptyPath()) {\n        // execute actions anywhere in the start rule for a token.\n        //\n        // TODO: if the entry rule is invoked recursively, some\n        // actions may be executed during the recursive call. The\n        // problem can appear when hasEmptyPath() is true but\n        // isEmpty() is false. In this case, the config needs to be\n        // split into two contexts - one with just the empty path\n        // and another with everything but the empty path.\n        // Unfortunately, the current algorithm does not allow\n        // getEpsilonTarget to return two configurations, so\n        // additional modifications are needed before we can support\n        // the split operation.\n        const lexerActionExecutor = LexerActionExecutor.append(config.lexerActionExecutor, this.atn.lexerActions[trans.actionIndex]);\n        cfg = new LexerATNConfig({\n          state: trans.target,\n          lexerActionExecutor: lexerActionExecutor\n        }, config);\n      } else {\n        // ignore actions in referenced rules\n        cfg = new LexerATNConfig({\n          state: trans.target\n        }, config);\n      }\n    } else if (trans.serializationType === Transition.EPSILON) {\n      cfg = new LexerATNConfig({\n        state: trans.target\n      }, config);\n    } else if (trans.serializationType === Transition.ATOM || trans.serializationType === Transition.RANGE || trans.serializationType === Transition.SET) {\n      if (treatEofAsEpsilon) {\n        if (trans.matches(Token.EOF, 0, Lexer.MAX_CHAR_VALUE)) {\n          cfg = new LexerATNConfig({\n            state: trans.target\n          }, config);\n        }\n      }\n    }\n\n    return cfg;\n  }\n  /**\n   * Evaluate a predicate specified in the lexer.\n   *\n   * <p>If {@code speculative} is {@code true}, this method was called before\n   * {@link //consume} for the matched character. This method should call\n   * {@link //consume} before evaluating the predicate to ensure position\n   * sensitive values, including {@link Lexer//getText}, {@link Lexer//getLine},\n   * and {@link Lexer//getcolumn}, properly reflect the current\n   * lexer state. This method should restore {@code input} and the simulator\n   * to the original state before returning (i.e. undo the actions made by the\n   * call to {@link //consume}.</p>\n   *\n   * @param input The input stream.\n   * @param ruleIndex The rule containing the predicate.\n   * @param predIndex The index of the predicate within the rule.\n   * @param speculative {@code true} if the current index in {@code input} is\n   * one character before the predicate's location.\n   *\n   * @return {@code true} if the specified predicate evaluates to\n   * {@code true}.\n   */\n\n\n  evaluatePredicate(input, ruleIndex, predIndex, speculative) {\n    // assume true if no recognizer was provided\n    if (this.recog === null) {\n      return true;\n    }\n\n    if (!speculative) {\n      return this.recog.sempred(null, ruleIndex, predIndex);\n    }\n\n    const savedcolumn = this.column;\n    const savedLine = this.line;\n    const index = input.index;\n    const marker = input.mark();\n\n    try {\n      this.consume(input);\n      return this.recog.sempred(null, ruleIndex, predIndex);\n    } finally {\n      this.column = savedcolumn;\n      this.line = savedLine;\n      input.seek(index);\n      input.release(marker);\n    }\n  }\n\n  captureSimState(settings, input, dfaState) {\n    settings.index = input.index;\n    settings.line = this.line;\n    settings.column = this.column;\n    settings.dfaState = dfaState;\n  }\n\n  addDFAEdge(from_, tk, to, cfgs) {\n    if (to === undefined) {\n      to = null;\n    }\n\n    if (cfgs === undefined) {\n      cfgs = null;\n    }\n\n    if (to === null && cfgs !== null) {\n      // leading to this call, ATNConfigSet.hasSemanticContext is used as a\n      // marker indicating dynamic predicate evaluation makes this edge\n      // dependent on the specific input sequence, so the static edge in the\n      // DFA should be omitted. The target DFAState is still created since\n      // execATN has the ability to resynchronize with the DFA state cache\n      // following the predicate evaluation step.\n      //\n      // TJP notes: next time through the DFA, we see a pred again and eval.\n      // If that gets us to a previously created (but dangling) DFA\n      // state, we can continue in pure DFA mode from there.\n      // /\n      const suppressEdge = cfgs.hasSemanticContext;\n      cfgs.hasSemanticContext = false;\n      to = this.addDFAState(cfgs);\n\n      if (suppressEdge) {\n        return to;\n      }\n    } // add the edge\n\n\n    if (tk < LexerATNSimulator.MIN_DFA_EDGE || tk > LexerATNSimulator.MAX_DFA_EDGE) {\n      // Only track edges within the DFA bounds\n      return to;\n    }\n\n    if (LexerATNSimulator.debug) {\n      console.log(\"EDGE \" + from_ + \" -> \" + to + \" upon \" + tk);\n    }\n\n    if (from_.edges === null) {\n      // make room for tokens 1..n and -1 masquerading as index 0\n      from_.edges = [];\n    }\n\n    from_.edges[tk - LexerATNSimulator.MIN_DFA_EDGE] = to; // connect\n\n    return to;\n  }\n  /**\n   * Add a new DFA state if there isn't one with this set of\n   * configurations already. This method also detects the first\n   * configuration containing an ATN rule stop state. Later, when\n   * traversing the DFA, we will know which rule to accept.\n   */\n\n\n  addDFAState(configs) {\n    const proposed = new DFAState(null, configs);\n    let firstConfigWithRuleStopState = null;\n\n    for (let i = 0; i < configs.items.length; i++) {\n      const cfg = configs.items[i];\n\n      if (cfg.state instanceof RuleStopState) {\n        firstConfigWithRuleStopState = cfg;\n        break;\n      }\n    }\n\n    if (firstConfigWithRuleStopState !== null) {\n      proposed.isAcceptState = true;\n      proposed.lexerActionExecutor = firstConfigWithRuleStopState.lexerActionExecutor;\n      proposed.prediction = this.atn.ruleToTokenType[firstConfigWithRuleStopState.state.ruleIndex];\n    }\n\n    const dfa = this.decisionToDFA[this.mode];\n    const existing = dfa.states.get(proposed);\n\n    if (existing !== null) {\n      return existing;\n    }\n\n    const newState = proposed;\n    newState.stateNumber = dfa.states.length;\n    configs.setReadonly(true);\n    newState.configs = configs;\n    dfa.states.add(newState);\n    return newState;\n  }\n\n  getDFA(mode) {\n    return this.decisionToDFA[mode];\n  } // Get the text matched so far for the current token.\n\n\n  getText(input) {\n    // index is first lookahead char, don't include.\n    return input.getText(this.startIndex, input.index - 1);\n  }\n\n  consume(input) {\n    const curChar = input.LA(1);\n\n    if (curChar === \"\\n\".charCodeAt(0)) {\n      this.line += 1;\n      this.column = 0;\n    } else {\n      this.column += 1;\n    }\n\n    input.consume();\n  }\n\n  getTokenName(tt) {\n    if (tt === -1) {\n      return \"EOF\";\n    } else {\n      return \"'\" + String.fromCharCode(tt) + \"'\";\n    }\n  }\n\n}\n\nLexerATNSimulator.debug = false;\nLexerATNSimulator.dfa_debug = false;\nLexerATNSimulator.MIN_DFA_EDGE = 0;\nLexerATNSimulator.MAX_DFA_EDGE = 127; // forces unicode to stay in ATN\n\nLexerATNSimulator.match_calls = 0;\nmodule.exports = LexerATNSimulator;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/LexerATNSimulator.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/LexerAction.js":
/*!***********************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/LexerAction.js ***!
  \***********************************************************/
/***/ ((module) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst LexerActionType = {\n  // The type of a {@link LexerChannelAction} action.\n  CHANNEL: 0,\n  // The type of a {@link LexerCustomAction} action\n  CUSTOM: 1,\n  // The type of a {@link LexerModeAction} action.\n  MODE: 2,\n  //The type of a {@link LexerMoreAction} action.\n  MORE: 3,\n  //The type of a {@link LexerPopModeAction} action.\n  POP_MODE: 4,\n  //The type of a {@link LexerPushModeAction} action.\n  PUSH_MODE: 5,\n  //The type of a {@link LexerSkipAction} action.\n  SKIP: 6,\n  //The type of a {@link LexerTypeAction} action.\n  TYPE: 7\n};\n\nclass LexerAction {\n  constructor(action) {\n    this.actionType = action;\n    this.isPositionDependent = false;\n  }\n\n  hashCode() {\n    const hash = new Hash();\n    this.updateHashCode(hash);\n    return hash.finish();\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.actionType);\n  }\n\n  equals(other) {\n    return this === other;\n  }\n\n}\n/**\n * Implements the {@code skip} lexer action by calling {@link Lexer//skip}.\n *\n * <p>The {@code skip} command does not have any parameters, so this action is\n * implemented as a singleton instance exposed by {@link //INSTANCE}.</p>\n */\n\n\nclass LexerSkipAction extends LexerAction {\n  constructor() {\n    super(LexerActionType.SKIP);\n  }\n\n  execute(lexer) {\n    lexer.skip();\n  }\n\n  toString() {\n    return \"skip\";\n  }\n\n} // Provides a singleton instance of this parameterless lexer action.\n\n\nLexerSkipAction.INSTANCE = new LexerSkipAction();\n/**\n * Implements the {@code type} lexer action by calling {@link Lexer//setType}\n * with the assigned type\n */\n\nclass LexerTypeAction extends LexerAction {\n  constructor(type) {\n    super(LexerActionType.TYPE);\n    this.type = type;\n  }\n\n  execute(lexer) {\n    lexer.type = this.type;\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.actionType, this.type);\n  }\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof LexerTypeAction)) {\n      return false;\n    } else {\n      return this.type === other.type;\n    }\n  }\n\n  toString() {\n    return \"type(\" + this.type + \")\";\n  }\n\n}\n/**\n * Implements the {@code pushMode} lexer action by calling\n * {@link Lexer//pushMode} with the assigned mode\n */\n\n\nclass LexerPushModeAction extends LexerAction {\n  constructor(mode) {\n    super(LexerActionType.PUSH_MODE);\n    this.mode = mode;\n  }\n  /**\n   * <p>This action is implemented by calling {@link Lexer//pushMode} with the\n   * value provided by {@link //getMode}.</p>\n   */\n\n\n  execute(lexer) {\n    lexer.pushMode(this.mode);\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.actionType, this.mode);\n  }\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof LexerPushModeAction)) {\n      return false;\n    } else {\n      return this.mode === other.mode;\n    }\n  }\n\n  toString() {\n    return \"pushMode(\" + this.mode + \")\";\n  }\n\n}\n/**\n * Implements the {@code popMode} lexer action by calling {@link Lexer//popMode}.\n *\n * <p>The {@code popMode} command does not have any parameters, so this action is\n * implemented as a singleton instance exposed by {@link //INSTANCE}.</p>\n */\n\n\nclass LexerPopModeAction extends LexerAction {\n  constructor() {\n    super(LexerActionType.POP_MODE);\n  }\n  /**\n   * <p>This action is implemented by calling {@link Lexer//popMode}.</p>\n   */\n\n\n  execute(lexer) {\n    lexer.popMode();\n  }\n\n  toString() {\n    return \"popMode\";\n  }\n\n}\n\nLexerPopModeAction.INSTANCE = new LexerPopModeAction();\n/**\n * Implements the {@code more} lexer action by calling {@link Lexer//more}.\n *\n * <p>The {@code more} command does not have any parameters, so this action is\n * implemented as a singleton instance exposed by {@link //INSTANCE}.</p>\n */\n\nclass LexerMoreAction extends LexerAction {\n  constructor() {\n    super(LexerActionType.MORE);\n  }\n  /**\n   * <p>This action is implemented by calling {@link Lexer//popMode}.</p>\n   */\n\n\n  execute(lexer) {\n    lexer.more();\n  }\n\n  toString() {\n    return \"more\";\n  }\n\n}\n\nLexerMoreAction.INSTANCE = new LexerMoreAction();\n/**\n * Implements the {@code mode} lexer action by calling {@link Lexer//mode} with\n * the assigned mode\n */\n\nclass LexerModeAction extends LexerAction {\n  constructor(mode) {\n    super(LexerActionType.MODE);\n    this.mode = mode;\n  }\n  /**\n   * <p>This action is implemented by calling {@link Lexer//mode} with the\n   * value provided by {@link //getMode}.</p>\n   */\n\n\n  execute(lexer) {\n    lexer.mode(this.mode);\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.actionType, this.mode);\n  }\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof LexerModeAction)) {\n      return false;\n    } else {\n      return this.mode === other.mode;\n    }\n  }\n\n  toString() {\n    return \"mode(\" + this.mode + \")\";\n  }\n\n}\n/**\n * Executes a custom lexer action by calling {@link Recognizer//action} with the\n * rule and action indexes assigned to the custom action. The implementation of\n * a custom action is added to the generated code for the lexer in an override\n * of {@link Recognizer//action} when the grammar is compiled.\n *\n * <p>This class may represent embedded actions created with the <code>{...}</code>\n * syntax in ANTLR 4, as well as actions created for lexer commands where the\n * command argument could not be evaluated when the grammar was compiled.</p>\n */\n\n\nclass LexerCustomAction extends LexerAction {\n  /**\n   * Constructs a custom lexer action with the specified rule and action\n   * indexes.\n   *\n   * @param ruleIndex The rule index to use for calls to\n   * {@link Recognizer//action}.\n   * @param actionIndex The action index to use for calls to\n   * {@link Recognizer//action}.\n   */\n  constructor(ruleIndex, actionIndex) {\n    super(LexerActionType.CUSTOM);\n    this.ruleIndex = ruleIndex;\n    this.actionIndex = actionIndex;\n    this.isPositionDependent = true;\n  }\n  /**\n   * <p>Custom actions are implemented by calling {@link Lexer//action} with the\n   * appropriate rule and action indexes.</p>\n   */\n\n\n  execute(lexer) {\n    lexer.action(null, this.ruleIndex, this.actionIndex);\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.actionType, this.ruleIndex, this.actionIndex);\n  }\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof LexerCustomAction)) {\n      return false;\n    } else {\n      return this.ruleIndex === other.ruleIndex && this.actionIndex === other.actionIndex;\n    }\n  }\n\n}\n/**\n * Implements the {@code channel} lexer action by calling\n * {@link Lexer//setChannel} with the assigned channel.\n * Constructs a new {@code channel} action with the specified channel value.\n * @param channel The channel value to pass to {@link Lexer//setChannel}\n */\n\n\nclass LexerChannelAction extends LexerAction {\n  constructor(channel) {\n    super(LexerActionType.CHANNEL);\n    this.channel = channel;\n  }\n  /**\n   * <p>This action is implemented by calling {@link Lexer//setChannel} with the\n   * value provided by {@link //getChannel}.</p>\n   */\n\n\n  execute(lexer) {\n    lexer._channel = this.channel;\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.actionType, this.channel);\n  }\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof LexerChannelAction)) {\n      return false;\n    } else {\n      return this.channel === other.channel;\n    }\n  }\n\n  toString() {\n    return \"channel(\" + this.channel + \")\";\n  }\n\n}\n/**\n * This implementation of {@link LexerAction} is used for tracking input offsets\n * for position-dependent actions within a {@link LexerActionExecutor}.\n *\n * <p>This action is not serialized as part of the ATN, and is only required for\n * position-dependent lexer actions which appear at a location other than the\n * end of a rule. For more information about DFA optimizations employed for\n * lexer actions, see {@link LexerActionExecutor//append} and\n * {@link LexerActionExecutor//fixOffsetBeforeMatch}.</p>\n *\n * Constructs a new indexed custom action by associating a character offset\n * with a {@link LexerAction}.\n *\n * <p>Note: This class is only required for lexer actions for which\n * {@link LexerAction//isPositionDependent} returns {@code true}.</p>\n *\n * @param offset The offset into the input {@link CharStream}, relative to\n * the token start index, at which the specified lexer action should be\n * executed.\n * @param action The lexer action to execute at a particular offset in the\n * input {@link CharStream}.\n */\n\n\nclass LexerIndexedCustomAction extends LexerAction {\n  constructor(offset, action) {\n    super(action.actionType);\n    this.offset = offset;\n    this.action = action;\n    this.isPositionDependent = true;\n  }\n  /**\n   * <p>This method calls {@link //execute} on the result of {@link //getAction}\n   * using the provided {@code lexer}.</p>\n   */\n\n\n  execute(lexer) {\n    // assume the input stream position was properly set by the calling code\n    this.action.execute(lexer);\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.actionType, this.offset, this.action);\n  }\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof LexerIndexedCustomAction)) {\n      return false;\n    } else {\n      return this.offset === other.offset && this.action === other.action;\n    }\n  }\n\n}\n\nmodule.exports = {\n  LexerActionType,\n  LexerSkipAction,\n  LexerChannelAction,\n  LexerCustomAction,\n  LexerIndexedCustomAction,\n  LexerMoreAction,\n  LexerTypeAction,\n  LexerPushModeAction,\n  LexerPopModeAction,\n  LexerModeAction\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/LexerAction.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/LexerActionExecutor.js":
/*!*******************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/LexerActionExecutor.js ***!
  \*******************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  hashStuff\n} = __webpack_require__(/*! ../Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n\nconst {\n  LexerIndexedCustomAction\n} = __webpack_require__(/*! ./LexerAction */ \"./node_modules/antlr4/src/antlr4/atn/LexerAction.js\");\n\nclass LexerActionExecutor {\n  /**\n   * Represents an executor for a sequence of lexer actions which traversed during\n   * the matching operation of a lexer rule (token).\n   *\n   * <p>The executor tracks position information for position-dependent lexer actions\n   * efficiently, ensuring that actions appearing only at the end of the rule do\n   * not cause bloating of the {@link DFA} created for the lexer.</p>\n   */\n  constructor(lexerActions) {\n    this.lexerActions = lexerActions === null ? [] : lexerActions;\n    /**\n     * Caches the result of {@link //hashCode} since the hash code is an element\n     * of the performance-critical {@link LexerATNConfig//hashCode} operation\n     */\n\n    this.cachedHashCode = hashStuff(lexerActions); // \"\".join([str(la) for la in\n    // lexerActions]))\n\n    return this;\n  }\n  /**\n   * Creates a {@link LexerActionExecutor} which encodes the current offset\n   * for position-dependent lexer actions.\n   *\n   * <p>Normally, when the executor encounters lexer actions where\n   * {@link LexerAction//isPositionDependent} returns {@code true}, it calls\n   * {@link IntStream//seek} on the input {@link CharStream} to set the input\n   * position to the <em>end</em> of the current token. This behavior provides\n   * for efficient DFA representation of lexer actions which appear at the end\n   * of a lexer rule, even when the lexer rule matches a variable number of\n   * characters.</p>\n   *\n   * <p>Prior to traversing a match transition in the ATN, the current offset\n   * from the token start index is assigned to all position-dependent lexer\n   * actions which have not already been assigned a fixed offset. By storing\n   * the offsets relative to the token start index, the DFA representation of\n   * lexer actions which appear in the middle of tokens remains efficient due\n   * to sharing among tokens of the same length, regardless of their absolute\n   * position in the input stream.</p>\n   *\n   * <p>If the current executor already has offsets assigned to all\n   * position-dependent lexer actions, the method returns {@code this}.</p>\n   *\n   * @param offset The current offset to assign to all position-dependent\n   * lexer actions which do not already have offsets assigned.\n   *\n   * @return {LexerActionExecutor} A {@link LexerActionExecutor} which stores input stream offsets\n   * for all position-dependent lexer actions.\n   */\n\n\n  fixOffsetBeforeMatch(offset) {\n    let updatedLexerActions = null;\n\n    for (let i = 0; i < this.lexerActions.length; i++) {\n      if (this.lexerActions[i].isPositionDependent && !(this.lexerActions[i] instanceof LexerIndexedCustomAction)) {\n        if (updatedLexerActions === null) {\n          updatedLexerActions = this.lexerActions.concat([]);\n        }\n\n        updatedLexerActions[i] = new LexerIndexedCustomAction(offset, this.lexerActions[i]);\n      }\n    }\n\n    if (updatedLexerActions === null) {\n      return this;\n    } else {\n      return new LexerActionExecutor(updatedLexerActions);\n    }\n  }\n  /**\n   * Execute the actions encapsulated by this executor within the context of a\n   * particular {@link Lexer}.\n   *\n   * <p>This method calls {@link IntStream//seek} to set the position of the\n   * {@code input} {@link CharStream} prior to calling\n   * {@link LexerAction//execute} on a position-dependent action. Before the\n   * method returns, the input position will be restored to the same position\n   * it was in when the method was invoked.</p>\n   *\n   * @param lexer The lexer instance.\n   * @param input The input stream which is the source for the current token.\n   * When this method is called, the current {@link IntStream//index} for\n   * {@code input} should be the start of the following token, i.e. 1\n   * character past the end of the current token.\n   * @param startIndex The token start index. This value may be passed to\n   * {@link IntStream//seek} to set the {@code input} position to the beginning\n   * of the token.\n   */\n\n\n  execute(lexer, input, startIndex) {\n    let requiresSeek = false;\n    const stopIndex = input.index;\n\n    try {\n      for (let i = 0; i < this.lexerActions.length; i++) {\n        let lexerAction = this.lexerActions[i];\n\n        if (lexerAction instanceof LexerIndexedCustomAction) {\n          const offset = lexerAction.offset;\n          input.seek(startIndex + offset);\n          lexerAction = lexerAction.action;\n          requiresSeek = startIndex + offset !== stopIndex;\n        } else if (lexerAction.isPositionDependent) {\n          input.seek(stopIndex);\n          requiresSeek = false;\n        }\n\n        lexerAction.execute(lexer);\n      }\n    } finally {\n      if (requiresSeek) {\n        input.seek(stopIndex);\n      }\n    }\n  }\n\n  hashCode() {\n    return this.cachedHashCode;\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.cachedHashCode);\n  }\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof LexerActionExecutor)) {\n      return false;\n    } else if (this.cachedHashCode != other.cachedHashCode) {\n      return false;\n    } else if (this.lexerActions.length != other.lexerActions.length) {\n      return false;\n    } else {\n      const numActions = this.lexerActions.length;\n\n      for (let idx = 0; idx < numActions; ++idx) {\n        if (!this.lexerActions[idx].equals(other.lexerActions[idx])) {\n          return false;\n        }\n      }\n\n      return true;\n    }\n  }\n  /**\n   * Creates a {@link LexerActionExecutor} which executes the actions for\n   * the input {@code lexerActionExecutor} followed by a specified\n   * {@code lexerAction}.\n   *\n   * @param lexerActionExecutor The executor for actions already traversed by\n   * the lexer while matching a token within a particular\n   * {@link LexerATNConfig}. If this is {@code null}, the method behaves as\n   * though it were an empty executor.\n   * @param lexerAction The lexer action to execute after the actions\n   * specified in {@code lexerActionExecutor}.\n   *\n   * @return {LexerActionExecutor} A {@link LexerActionExecutor} for executing the combine actions\n   * of {@code lexerActionExecutor} and {@code lexerAction}.\n   */\n\n\n  static append(lexerActionExecutor, lexerAction) {\n    if (lexerActionExecutor === null) {\n      return new LexerActionExecutor([lexerAction]);\n    }\n\n    const lexerActions = lexerActionExecutor.lexerActions.concat([lexerAction]);\n    return new LexerActionExecutor(lexerActions);\n  }\n\n}\n\nmodule.exports = LexerActionExecutor;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/LexerActionExecutor.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/ParserATNSimulator.js":
/*!******************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/ParserATNSimulator.js ***!
  \******************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst Utils = __webpack_require__(/*! ./../Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n\nconst {\n  Set,\n  BitSet,\n  DoubleDict\n} = Utils;\n\nconst ATN = __webpack_require__(/*! ./ATN */ \"./node_modules/antlr4/src/antlr4/atn/ATN.js\");\n\nconst {\n  ATNState,\n  RuleStopState\n} = __webpack_require__(/*! ./ATNState */ \"./node_modules/antlr4/src/antlr4/atn/ATNState.js\");\n\nconst {\n  ATNConfig\n} = __webpack_require__(/*! ./ATNConfig */ \"./node_modules/antlr4/src/antlr4/atn/ATNConfig.js\");\n\nconst {\n  ATNConfigSet\n} = __webpack_require__(/*! ./ATNConfigSet */ \"./node_modules/antlr4/src/antlr4/atn/ATNConfigSet.js\");\n\nconst {\n  Token\n} = __webpack_require__(/*! ./../Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\nconst {\n  DFAState,\n  PredPrediction\n} = __webpack_require__(/*! ./../dfa/DFAState */ \"./node_modules/antlr4/src/antlr4/dfa/DFAState.js\");\n\nconst ATNSimulator = __webpack_require__(/*! ./ATNSimulator */ \"./node_modules/antlr4/src/antlr4/atn/ATNSimulator.js\");\n\nconst PredictionMode = __webpack_require__(/*! ./PredictionMode */ \"./node_modules/antlr4/src/antlr4/atn/PredictionMode.js\");\n\nconst RuleContext = __webpack_require__(/*! ./../RuleContext */ \"./node_modules/antlr4/src/antlr4/RuleContext.js\");\n\nconst ParserRuleContext = __webpack_require__(/*! ./../ParserRuleContext */ \"./node_modules/antlr4/src/antlr4/ParserRuleContext.js\");\n\nconst {\n  SemanticContext\n} = __webpack_require__(/*! ./SemanticContext */ \"./node_modules/antlr4/src/antlr4/atn/SemanticContext.js\");\n\nconst {\n  PredictionContext\n} = __webpack_require__(/*! ./../PredictionContext */ \"./node_modules/antlr4/src/antlr4/PredictionContext.js\");\n\nconst {\n  Interval\n} = __webpack_require__(/*! ./../IntervalSet */ \"./node_modules/antlr4/src/antlr4/IntervalSet.js\");\n\nconst {\n  Transition,\n  SetTransition,\n  NotSetTransition,\n  RuleTransition,\n  ActionTransition\n} = __webpack_require__(/*! ./Transition */ \"./node_modules/antlr4/src/antlr4/atn/Transition.js\");\n\nconst {\n  NoViableAltException\n} = __webpack_require__(/*! ./../error/Errors */ \"./node_modules/antlr4/src/antlr4/error/Errors.js\");\n\nconst {\n  SingletonPredictionContext,\n  predictionContextFromRuleContext\n} = __webpack_require__(/*! ./../PredictionContext */ \"./node_modules/antlr4/src/antlr4/PredictionContext.js\");\n/**\n * The embodiment of the adaptive LL(*), ALL(*), parsing strategy.\n *\n * <p>\n * The basic complexity of the adaptive strategy makes it harder to understand.\n * We begin with ATN simulation to build paths in a DFA. Subsequent prediction\n * requests go through the DFA first. If they reach a state without an edge for\n * the current symbol, the algorithm fails over to the ATN simulation to\n * complete the DFA path for the current input (until it finds a conflict state\n * or uniquely predicting state).</p>\n *\n * <p>\n * All of that is done without using the outer context because we want to create\n * a DFA that is not dependent upon the rule invocation stack when we do a\n * prediction. One DFA works in all contexts. We avoid using context not\n * necessarily because it's slower, although it can be, but because of the DFA\n * caching problem. The closure routine only considers the rule invocation stack\n * created during prediction beginning in the decision rule. For example, if\n * prediction occurs without invoking another rule's ATN, there are no context\n * stacks in the configurations. When lack of context leads to a conflict, we\n * don't know if it's an ambiguity or a weakness in the strong LL(*) parsing\n * strategy (versus full LL(*)).</p>\n *\n * <p>\n * When SLL yields a configuration set with conflict, we rewind the input and\n * retry the ATN simulation, this time using full outer context without adding\n * to the DFA. Configuration context stacks will be the full invocation stacks\n * from the start rule. If we get a conflict using full context, then we can\n * definitively say we have a true ambiguity for that input sequence. If we\n * don't get a conflict, it implies that the decision is sensitive to the outer\n * context. (It is not context-sensitive in the sense of context-sensitive\n * grammars.)</p>\n *\n * <p>\n * The next time we reach this DFA state with an SLL conflict, through DFA\n * simulation, we will again retry the ATN simulation using full context mode.\n * This is slow because we can't save the results and have to \"interpret\" the\n * ATN each time we get that input.</p>\n *\n * <p>\n * <strong>CACHING FULL CONTEXT PREDICTIONS</strong></p>\n *\n * <p>\n * We could cache results from full context to predicted alternative easily and\n * that saves a lot of time but doesn't work in presence of predicates. The set\n * of visible predicates from the ATN start state changes depending on the\n * context, because closure can fall off the end of a rule. I tried to cache\n * tuples (stack context, semantic context, predicted alt) but it was slower\n * than interpreting and much more complicated. Also required a huge amount of\n * memory. The goal is not to create the world's fastest parser anyway. I'd like\n * to keep this algorithm simple. By launching multiple threads, we can improve\n * the speed of parsing across a large number of files.</p>\n *\n * <p>\n * There is no strict ordering between the amount of input used by SLL vs LL,\n * which makes it really hard to build a cache for full context. Let's say that\n * we have input A B C that leads to an SLL conflict with full context X. That\n * implies that using X we might only use A B but we could also use A B C D to\n * resolve conflict. Input A B C D could predict alternative 1 in one position\n * in the input and A B C E could predict alternative 2 in another position in\n * input. The conflicting SLL configurations could still be non-unique in the\n * full context prediction, which would lead us to requiring more input than the\n * original A B C.\tTo make a\tprediction cache work, we have to track\tthe exact\n * input\tused during the previous prediction. That amounts to a cache that maps\n * X to a specific DFA for that context.</p>\n *\n * <p>\n * Something should be done for left-recursive expression predictions. They are\n * likely LL(1) + pred eval. Easier to do the whole SLL unless error and retry\n * with full LL thing Sam does.</p>\n *\n * <p>\n * <strong>AVOIDING FULL CONTEXT PREDICTION</strong></p>\n *\n * <p>\n * We avoid doing full context retry when the outer context is empty, we did not\n * dip into the outer context by falling off the end of the decision state rule,\n * or when we force SLL mode.</p>\n *\n * <p>\n * As an example of the not dip into outer context case, consider as super\n * constructor calls versus function calls. One grammar might look like\n * this:</p>\n *\n * <pre>\n * ctorBody\n *   : '{' superCall? stat* '}'\n *   ;\n * </pre>\n *\n * <p>\n * Or, you might see something like</p>\n *\n * <pre>\n * stat\n *   : superCall ';'\n *   | expression ';'\n *   | ...\n *   ;\n * </pre>\n *\n * <p>\n * In both cases I believe that no closure operations will dip into the outer\n * context. In the first case ctorBody in the worst case will stop at the '}'.\n * In the 2nd case it should stop at the ';'. Both cases should stay within the\n * entry rule and not dip into the outer context.</p>\n *\n * <p>\n * <strong>PREDICATES</strong></p>\n *\n * <p>\n * Predicates are always evaluated if present in either SLL or LL both. SLL and\n * LL simulation deals with predicates differently. SLL collects predicates as\n * it performs closure operations like ANTLR v3 did. It delays predicate\n * evaluation until it reaches and accept state. This allows us to cache the SLL\n * ATN simulation whereas, if we had evaluated predicates on-the-fly during\n * closure, the DFA state configuration sets would be different and we couldn't\n * build up a suitable DFA.</p>\n *\n * <p>\n * When building a DFA accept state during ATN simulation, we evaluate any\n * predicates and return the sole semantically valid alternative. If there is\n * more than 1 alternative, we report an ambiguity. If there are 0 alternatives,\n * we throw an exception. Alternatives without predicates act like they have\n * true predicates. The simple way to think about it is to strip away all\n * alternatives with false predicates and choose the minimum alternative that\n * remains.</p>\n *\n * <p>\n * When we start in the DFA and reach an accept state that's predicated, we test\n * those and return the minimum semantically viable alternative. If no\n * alternatives are viable, we throw an exception.</p>\n *\n * <p>\n * During full LL ATN simulation, closure always evaluates predicates and\n * on-the-fly. This is crucial to reducing the configuration set size during\n * closure. It hits a landmine when parsing with the Java grammar, for example,\n * without this on-the-fly evaluation.</p>\n *\n * <p>\n * <strong>SHARING DFA</strong></p>\n *\n * <p>\n * All instances of the same parser share the same decision DFAs through a\n * static field. Each instance gets its own ATN simulator but they share the\n * same {@link //decisionToDFA} field. They also share a\n * {@link PredictionContextCache} object that makes sure that all\n * {@link PredictionContext} objects are shared among the DFA states. This makes\n * a big size difference.</p>\n *\n * <p>\n * <strong>THREAD SAFETY</strong></p>\n *\n * <p>\n * The {@link ParserATNSimulator} locks on the {@link //decisionToDFA} field when\n * it adds a new DFA object to that array. {@link //addDFAEdge}\n * locks on the DFA for the current decision when setting the\n * {@link DFAState//edges} field. {@link //addDFAState} locks on\n * the DFA for the current decision when looking up a DFA state to see if it\n * already exists. We must make sure that all requests to add DFA states that\n * are equivalent result in the same shared DFA object. This is because lots of\n * threads will be trying to update the DFA at once. The\n * {@link //addDFAState} method also locks inside the DFA lock\n * but this time on the shared context cache when it rebuilds the\n * configurations' {@link PredictionContext} objects using cached\n * subgraphs/nodes. No other locking occurs, even during DFA simulation. This is\n * safe as long as we can guarantee that all threads referencing\n * {@code s.edge[t]} get the same physical target {@link DFAState}, or\n * {@code null}. Once into the DFA, the DFA simulation does not reference the\n * {@link DFA//states} map. It follows the {@link DFAState//edges} field to new\n * targets. The DFA simulator will either find {@link DFAState//edges} to be\n * {@code null}, to be non-{@code null} and {@code dfa.edges[t]} null, or\n * {@code dfa.edges[t]} to be non-null. The\n * {@link //addDFAEdge} method could be racing to set the field\n * but in either case the DFA simulator works; if {@code null}, and requests ATN\n * simulation. It could also race trying to get {@code dfa.edges[t]}, but either\n * way it will work because it's not doing a test and set operation.</p>\n *\n * <p>\n * <strong>Starting with SLL then failing to combined SLL/LL (Two-Stage\n * Parsing)</strong></p>\n *\n * <p>\n * Sam pointed out that if SLL does not give a syntax error, then there is no\n * point in doing full LL, which is slower. We only have to try LL if we get a\n * syntax error. For maximum speed, Sam starts the parser set to pure SLL\n * mode with the {@link BailErrorStrategy}:</p>\n *\n * <pre>\n * parser.{@link Parser//getInterpreter() getInterpreter()}.{@link //setPredictionMode setPredictionMode}{@code (}{@link PredictionMode//SLL}{@code )};\n * parser.{@link Parser//setErrorHandler setErrorHandler}(new {@link BailErrorStrategy}());\n * </pre>\n *\n * <p>\n * If it does not get a syntax error, then we're done. If it does get a syntax\n * error, we need to retry with the combined SLL/LL strategy.</p>\n *\n * <p>\n * The reason this works is as follows. If there are no SLL conflicts, then the\n * grammar is SLL (at least for that input set). If there is an SLL conflict,\n * the full LL analysis must yield a set of viable alternatives which is a\n * subset of the alternatives reported by SLL. If the LL set is a singleton,\n * then the grammar is LL but not SLL. If the LL set is the same size as the SLL\n * set, the decision is SLL. If the LL set has size &gt; 1, then that decision\n * is truly ambiguous on the current input. If the LL set is smaller, then the\n * SLL conflict resolution might choose an alternative that the full LL would\n * rule out as a possibility based upon better context information. If that's\n * the case, then the SLL parse will definitely get an error because the full LL\n * analysis says it's not viable. If SLL conflict resolution chooses an\n * alternative within the LL set, them both SLL and LL would choose the same\n * alternative because they both choose the minimum of multiple conflicting\n * alternatives.</p>\n *\n * <p>\n * Let's say we have a set of SLL conflicting alternatives {@code {1, 2, 3}} and\n * a smaller LL set called <em>s</em>. If <em>s</em> is {@code {2, 3}}, then SLL\n * parsing will get an error because SLL will pursue alternative 1. If\n * <em>s</em> is {@code {1, 2}} or {@code {1, 3}} then both SLL and LL will\n * choose the same alternative because alternative one is the minimum of either\n * set. If <em>s</em> is {@code {2}} or {@code {3}} then SLL will get a syntax\n * error. If <em>s</em> is {@code {1}} then SLL will succeed.</p>\n *\n * <p>\n * Of course, if the input is invalid, then we will get an error for sure in\n * both SLL and LL parsing. Erroneous input will therefore require 2 passes over\n * the input.</p>\n */\n\n\nclass ParserATNSimulator extends ATNSimulator {\n  constructor(parser, atn, decisionToDFA, sharedContextCache) {\n    super(atn, sharedContextCache);\n    this.parser = parser;\n    this.decisionToDFA = decisionToDFA; // SLL, LL, or LL + exact ambig detection?//\n\n    this.predictionMode = PredictionMode.LL; // LAME globals to avoid parameters!!!!! I need these down deep in predTransition\n\n    this._input = null;\n    this._startIndex = 0;\n    this._outerContext = null;\n    this._dfa = null;\n    /**\n     * Each prediction operation uses a cache for merge of prediction contexts.\n     *  Don't keep around as it wastes huge amounts of memory. DoubleKeyMap\n     *  isn't synchronized but we're ok since two threads shouldn't reuse same\n     *  parser/atnsim object because it can only handle one input at a time.\n     *  This maps graphs a and b to merged result c. (a,b)&rarr;c. We can avoid\n     *  the merge if we ever see a and b again.  Note that (b,a)&rarr;c should\n     *  also be examined during cache lookup.\n     */\n\n    this.mergeCache = null;\n    this.debug = false;\n    this.debug_closure = false;\n    this.debug_add = false;\n    this.debug_list_atn_decisions = false;\n    this.dfa_debug = false;\n    this.retry_debug = false;\n  }\n\n  reset() {}\n\n  adaptivePredict(input, decision, outerContext) {\n    if (this.debug || this.debug_list_atn_decisions) {\n      console.log(\"adaptivePredict decision \" + decision + \" exec LA(1)==\" + this.getLookaheadName(input) + \" line \" + input.LT(1).line + \":\" + input.LT(1).column);\n    }\n\n    this._input = input;\n    this._startIndex = input.index;\n    this._outerContext = outerContext;\n    const dfa = this.decisionToDFA[decision];\n    this._dfa = dfa;\n    const m = input.mark();\n    const index = input.index; // Now we are certain to have a specific decision's DFA\n    // But, do we still need an initial state?\n\n    try {\n      let s0;\n\n      if (dfa.precedenceDfa) {\n        // the start state for a precedence DFA depends on the current\n        // parser precedence, and is provided by a DFA method.\n        s0 = dfa.getPrecedenceStartState(this.parser.getPrecedence());\n      } else {\n        // the start state for a \"regular\" DFA is just s0\n        s0 = dfa.s0;\n      }\n\n      if (s0 === null) {\n        if (outerContext === null) {\n          outerContext = RuleContext.EMPTY;\n        }\n\n        if (this.debug || this.debug_list_atn_decisions) {\n          console.log(\"predictATN decision \" + dfa.decision + \" exec LA(1)==\" + this.getLookaheadName(input) + \", outerContext=\" + outerContext.toString(this.parser.ruleNames));\n        }\n\n        const fullCtx = false;\n        let s0_closure = this.computeStartState(dfa.atnStartState, RuleContext.EMPTY, fullCtx);\n\n        if (dfa.precedenceDfa) {\n          // If this is a precedence DFA, we use applyPrecedenceFilter\n          // to convert the computed start state to a precedence start\n          // state. We then use DFA.setPrecedenceStartState to set the\n          // appropriate start state for the precedence level rather\n          // than simply setting DFA.s0.\n          //\n          dfa.s0.configs = s0_closure; // not used for prediction but useful to know start configs anyway\n\n          s0_closure = this.applyPrecedenceFilter(s0_closure);\n          s0 = this.addDFAState(dfa, new DFAState(null, s0_closure));\n          dfa.setPrecedenceStartState(this.parser.getPrecedence(), s0);\n        } else {\n          s0 = this.addDFAState(dfa, new DFAState(null, s0_closure));\n          dfa.s0 = s0;\n        }\n      }\n\n      const alt = this.execATN(dfa, s0, input, index, outerContext);\n\n      if (this.debug) {\n        console.log(\"DFA after predictATN: \" + dfa.toString(this.parser.literalNames, this.parser.symbolicNames));\n      }\n\n      return alt;\n    } finally {\n      this._dfa = null;\n      this.mergeCache = null; // wack cache after each prediction\n\n      input.seek(index);\n      input.release(m);\n    }\n  }\n  /**\n   * Performs ATN simulation to compute a predicted alternative based\n   *  upon the remaining input, but also updates the DFA cache to avoid\n   *  having to traverse the ATN again for the same input sequence.\n   *\n   * There are some key conditions we're looking for after computing a new\n   * set of ATN configs (proposed DFA state):\n   *       if the set is empty, there is no viable alternative for current symbol\n   *       does the state uniquely predict an alternative?\n   *       does the state have a conflict that would prevent us from\n   *         putting it on the work list?\n   *\n   * We also have some key operations to do:\n   *       add an edge from previous DFA state to potentially new DFA state, D,\n   *         upon current symbol but only if adding to work list, which means in all\n   *         cases except no viable alternative (and possibly non-greedy decisions?)\n   *       collecting predicates and adding semantic context to DFA accept states\n   *       adding rule context to context-sensitive DFA accept states\n   *       consuming an input symbol\n   *       reporting a conflict\n   *       reporting an ambiguity\n   *       reporting a context sensitivity\n   *       reporting insufficient predicates\n   *\n   * cover these cases:\n   *    dead end\n   *    single alt\n   *    single alt + preds\n   *    conflict\n   *    conflict + preds\n   *\n   */\n\n\n  execATN(dfa, s0, input, startIndex, outerContext) {\n    if (this.debug || this.debug_list_atn_decisions) {\n      console.log(\"execATN decision \" + dfa.decision + \" exec LA(1)==\" + this.getLookaheadName(input) + \" line \" + input.LT(1).line + \":\" + input.LT(1).column);\n    }\n\n    let alt;\n    let previousD = s0;\n\n    if (this.debug) {\n      console.log(\"s0 = \" + s0);\n    }\n\n    let t = input.LA(1);\n\n    while (true) {\n      // while more work\n      let D = this.getExistingTargetState(previousD, t);\n\n      if (D === null) {\n        D = this.computeTargetState(dfa, previousD, t);\n      }\n\n      if (D === ATNSimulator.ERROR) {\n        // if any configs in previous dipped into outer context, that\n        // means that input up to t actually finished entry rule\n        // at least for SLL decision. Full LL doesn't dip into outer\n        // so don't need special case.\n        // We will get an error no matter what so delay until after\n        // decision; better error message. Also, no reachable target\n        // ATN states in SLL implies LL will also get nowhere.\n        // If conflict in states that dip out, choose min since we\n        // will get error no matter what.\n        const e = this.noViableAlt(input, outerContext, previousD.configs, startIndex);\n        input.seek(startIndex);\n        alt = this.getSynValidOrSemInvalidAltThatFinishedDecisionEntryRule(previousD.configs, outerContext);\n\n        if (alt !== ATN.INVALID_ALT_NUMBER) {\n          return alt;\n        } else {\n          throw e;\n        }\n      }\n\n      if (D.requiresFullContext && this.predictionMode !== PredictionMode.SLL) {\n        // IF PREDS, MIGHT RESOLVE TO SINGLE ALT => SLL (or syntax error)\n        let conflictingAlts = null;\n\n        if (D.predicates !== null) {\n          if (this.debug) {\n            console.log(\"DFA state has preds in DFA sim LL failover\");\n          }\n\n          const conflictIndex = input.index;\n\n          if (conflictIndex !== startIndex) {\n            input.seek(startIndex);\n          }\n\n          conflictingAlts = this.evalSemanticContext(D.predicates, outerContext, true);\n\n          if (conflictingAlts.length === 1) {\n            if (this.debug) {\n              console.log(\"Full LL avoided\");\n            }\n\n            return conflictingAlts.minValue();\n          }\n\n          if (conflictIndex !== startIndex) {\n            // restore the index so reporting the fallback to full\n            // context occurs with the index at the correct spot\n            input.seek(conflictIndex);\n          }\n        }\n\n        if (this.dfa_debug) {\n          console.log(\"ctx sensitive state \" + outerContext + \" in \" + D);\n        }\n\n        const fullCtx = true;\n        const s0_closure = this.computeStartState(dfa.atnStartState, outerContext, fullCtx);\n        this.reportAttemptingFullContext(dfa, conflictingAlts, D.configs, startIndex, input.index);\n        alt = this.execATNWithFullContext(dfa, D, s0_closure, input, startIndex, outerContext);\n        return alt;\n      }\n\n      if (D.isAcceptState) {\n        if (D.predicates === null) {\n          return D.prediction;\n        }\n\n        const stopIndex = input.index;\n        input.seek(startIndex);\n        const alts = this.evalSemanticContext(D.predicates, outerContext, true);\n\n        if (alts.length === 0) {\n          throw this.noViableAlt(input, outerContext, D.configs, startIndex);\n        } else if (alts.length === 1) {\n          return alts.minValue();\n        } else {\n          // report ambiguity after predicate evaluation to make sure the correct set of ambig alts is reported.\n          this.reportAmbiguity(dfa, D, startIndex, stopIndex, false, alts, D.configs);\n          return alts.minValue();\n        }\n      }\n\n      previousD = D;\n\n      if (t !== Token.EOF) {\n        input.consume();\n        t = input.LA(1);\n      }\n    }\n  }\n  /**\n   * Get an existing target state for an edge in the DFA. If the target state\n   * for the edge has not yet been computed or is otherwise not available,\n   * this method returns {@code null}.\n   *\n   * @param previousD The current DFA state\n   * @param t The next input symbol\n   * @return The existing target DFA state for the given input symbol\n   * {@code t}, or {@code null} if the target state for this edge is not\n   * already cached\n   */\n\n\n  getExistingTargetState(previousD, t) {\n    const edges = previousD.edges;\n\n    if (edges === null) {\n      return null;\n    } else {\n      return edges[t + 1] || null;\n    }\n  }\n  /**\n   * Compute a target state for an edge in the DFA, and attempt to add the\n   * computed state and corresponding edge to the DFA.\n   *\n   * @param dfa The DFA\n   * @param previousD The current DFA state\n   * @param t The next input symbol\n   *\n   * @return The computed target DFA state for the given input symbol\n   * {@code t}. If {@code t} does not lead to a valid DFA state, this method\n   * returns {@link //ERROR\n   */\n\n\n  computeTargetState(dfa, previousD, t) {\n    const reach = this.computeReachSet(previousD.configs, t, false);\n\n    if (reach === null) {\n      this.addDFAEdge(dfa, previousD, t, ATNSimulator.ERROR);\n      return ATNSimulator.ERROR;\n    } // create new target state; we'll add to DFA after it's complete\n\n\n    let D = new DFAState(null, reach);\n    const predictedAlt = this.getUniqueAlt(reach);\n\n    if (this.debug) {\n      const altSubSets = PredictionMode.getConflictingAltSubsets(reach);\n      console.log(\"SLL altSubSets=\" + Utils.arrayToString(altSubSets) +\n      /*\", previous=\" + previousD.configs + */\n      \", configs=\" + reach + \", predict=\" + predictedAlt + \", allSubsetsConflict=\" + PredictionMode.allSubsetsConflict(altSubSets) + \", conflictingAlts=\" + this.getConflictingAlts(reach));\n    }\n\n    if (predictedAlt !== ATN.INVALID_ALT_NUMBER) {\n      // NO CONFLICT, UNIQUELY PREDICTED ALT\n      D.isAcceptState = true;\n      D.configs.uniqueAlt = predictedAlt;\n      D.prediction = predictedAlt;\n    } else if (PredictionMode.hasSLLConflictTerminatingPrediction(this.predictionMode, reach)) {\n      // MORE THAN ONE VIABLE ALTERNATIVE\n      D.configs.conflictingAlts = this.getConflictingAlts(reach);\n      D.requiresFullContext = true; // in SLL-only mode, we will stop at this state and return the minimum alt\n\n      D.isAcceptState = true;\n      D.prediction = D.configs.conflictingAlts.minValue();\n    }\n\n    if (D.isAcceptState && D.configs.hasSemanticContext) {\n      this.predicateDFAState(D, this.atn.getDecisionState(dfa.decision));\n\n      if (D.predicates !== null) {\n        D.prediction = ATN.INVALID_ALT_NUMBER;\n      }\n    } // all adds to dfa are done after we've created full D state\n\n\n    D = this.addDFAEdge(dfa, previousD, t, D);\n    return D;\n  }\n\n  predicateDFAState(dfaState, decisionState) {\n    // We need to test all predicates, even in DFA states that\n    // uniquely predict alternative.\n    const nalts = decisionState.transitions.length; // Update DFA so reach becomes accept state with (predicate,alt)\n    // pairs if preds found for conflicting alts\n\n    const altsToCollectPredsFrom = this.getConflictingAltsOrUniqueAlt(dfaState.configs);\n    const altToPred = this.getPredsForAmbigAlts(altsToCollectPredsFrom, dfaState.configs, nalts);\n\n    if (altToPred !== null) {\n      dfaState.predicates = this.getPredicatePredictions(altsToCollectPredsFrom, altToPred);\n      dfaState.prediction = ATN.INVALID_ALT_NUMBER; // make sure we use preds\n    } else {\n      // There are preds in configs but they might go away\n      // when OR'd together like {p}? || NONE == NONE. If neither\n      // alt has preds, resolve to min alt\n      dfaState.prediction = altsToCollectPredsFrom.minValue();\n    }\n  } // comes back with reach.uniqueAlt set to a valid alt\n\n\n  execATNWithFullContext(dfa, D, // how far we got before failing over\n  s0, input, startIndex, outerContext) {\n    if (this.debug || this.debug_list_atn_decisions) {\n      console.log(\"execATNWithFullContext \" + s0);\n    }\n\n    const fullCtx = true;\n    let foundExactAmbig = false;\n    let reach;\n    let previous = s0;\n    input.seek(startIndex);\n    let t = input.LA(1);\n    let predictedAlt = -1;\n\n    while (true) {\n      // while more work\n      reach = this.computeReachSet(previous, t, fullCtx);\n\n      if (reach === null) {\n        // if any configs in previous dipped into outer context, that\n        // means that input up to t actually finished entry rule\n        // at least for LL decision. Full LL doesn't dip into outer\n        // so don't need special case.\n        // We will get an error no matter what so delay until after\n        // decision; better error message. Also, no reachable target\n        // ATN states in SLL implies LL will also get nowhere.\n        // If conflict in states that dip out, choose min since we\n        // will get error no matter what.\n        const e = this.noViableAlt(input, outerContext, previous, startIndex);\n        input.seek(startIndex);\n        const alt = this.getSynValidOrSemInvalidAltThatFinishedDecisionEntryRule(previous, outerContext);\n\n        if (alt !== ATN.INVALID_ALT_NUMBER) {\n          return alt;\n        } else {\n          throw e;\n        }\n      }\n\n      const altSubSets = PredictionMode.getConflictingAltSubsets(reach);\n\n      if (this.debug) {\n        console.log(\"LL altSubSets=\" + altSubSets + \", predict=\" + PredictionMode.getUniqueAlt(altSubSets) + \", resolvesToJustOneViableAlt=\" + PredictionMode.resolvesToJustOneViableAlt(altSubSets));\n      }\n\n      reach.uniqueAlt = this.getUniqueAlt(reach); // unique prediction?\n\n      if (reach.uniqueAlt !== ATN.INVALID_ALT_NUMBER) {\n        predictedAlt = reach.uniqueAlt;\n        break;\n      } else if (this.predictionMode !== PredictionMode.LL_EXACT_AMBIG_DETECTION) {\n        predictedAlt = PredictionMode.resolvesToJustOneViableAlt(altSubSets);\n\n        if (predictedAlt !== ATN.INVALID_ALT_NUMBER) {\n          break;\n        }\n      } else {\n        // In exact ambiguity mode, we never try to terminate early.\n        // Just keeps scarfing until we know what the conflict is\n        if (PredictionMode.allSubsetsConflict(altSubSets) && PredictionMode.allSubsetsEqual(altSubSets)) {\n          foundExactAmbig = true;\n          predictedAlt = PredictionMode.getSingleViableAlt(altSubSets);\n          break;\n        } // else there are multiple non-conflicting subsets or\n        // we're not sure what the ambiguity is yet.\n        // So, keep going.\n\n      }\n\n      previous = reach;\n\n      if (t !== Token.EOF) {\n        input.consume();\n        t = input.LA(1);\n      }\n    } // If the configuration set uniquely predicts an alternative,\n    // without conflict, then we know that it's a full LL decision\n    // not SLL.\n\n\n    if (reach.uniqueAlt !== ATN.INVALID_ALT_NUMBER) {\n      this.reportContextSensitivity(dfa, predictedAlt, reach, startIndex, input.index);\n      return predictedAlt;\n    } // We do not check predicates here because we have checked them\n    // on-the-fly when doing full context prediction.\n    //\n    // In non-exact ambiguity detection mode, we might\tactually be able to\n    // detect an exact ambiguity, but I'm not going to spend the cycles\n    // needed to check. We only emit ambiguity warnings in exact ambiguity\n    // mode.\n    //\n    // For example, we might know that we have conflicting configurations.\n    // But, that does not mean that there is no way forward without a\n    // conflict. It's possible to have nonconflicting alt subsets as in:\n    // altSubSets=[{1, 2}, {1, 2}, {1}, {1, 2}]\n    // from\n    //\n    //    [(17,1,[5 $]), (13,1,[5 10 $]), (21,1,[5 10 $]), (11,1,[$]),\n    //     (13,2,[5 10 $]), (21,2,[5 10 $]), (11,2,[$])]\n    //\n    // In this case, (17,1,[5 $]) indicates there is some next sequence that\n    // would resolve this without conflict to alternative 1. Any other viable\n    // next sequence, however, is associated with a conflict.  We stop\n    // looking for input because no amount of further lookahead will alter\n    // the fact that we should predict alternative 1.  We just can't say for\n    // sure that there is an ambiguity without looking further.\n\n\n    this.reportAmbiguity(dfa, D, startIndex, input.index, foundExactAmbig, null, reach);\n    return predictedAlt;\n  }\n\n  computeReachSet(closure, t, fullCtx) {\n    if (this.debug) {\n      console.log(\"in computeReachSet, starting closure: \" + closure);\n    }\n\n    if (this.mergeCache === null) {\n      this.mergeCache = new DoubleDict();\n    }\n\n    const intermediate = new ATNConfigSet(fullCtx); // Configurations already in a rule stop state indicate reaching the end\n    // of the decision rule (local context) or end of the start rule (full\n    // context). Once reached, these configurations are never updated by a\n    // closure operation, so they are handled separately for the performance\n    // advantage of having a smaller intermediate set when calling closure.\n    //\n    // For full-context reach operations, separate handling is required to\n    // ensure that the alternative matching the longest overall sequence is\n    // chosen when multiple such configurations can match the input.\n\n    let skippedStopStates = null; // First figure out where we can reach on input t\n\n    for (let i = 0; i < closure.items.length; i++) {\n      const c = closure.items[i];\n\n      if (this.debug) {\n        console.log(\"testing \" + this.getTokenName(t) + \" at \" + c);\n      }\n\n      if (c.state instanceof RuleStopState) {\n        if (fullCtx || t === Token.EOF) {\n          if (skippedStopStates === null) {\n            skippedStopStates = [];\n          }\n\n          skippedStopStates.push(c);\n\n          if (this.debug_add) {\n            console.log(\"added \" + c + \" to skippedStopStates\");\n          }\n        }\n\n        continue;\n      }\n\n      for (let j = 0; j < c.state.transitions.length; j++) {\n        const trans = c.state.transitions[j];\n        const target = this.getReachableTarget(trans, t);\n\n        if (target !== null) {\n          const cfg = new ATNConfig({\n            state: target\n          }, c);\n          intermediate.add(cfg, this.mergeCache);\n\n          if (this.debug_add) {\n            console.log(\"added \" + cfg + \" to intermediate\");\n          }\n        }\n      }\n    } // Now figure out where the reach operation can take us...\n\n\n    let reach = null; // This block optimizes the reach operation for intermediate sets which\n    // trivially indicate a termination state for the overall\n    // adaptivePredict operation.\n    //\n    // The conditions assume that intermediate\n    // contains all configurations relevant to the reach set, but this\n    // condition is not true when one or more configurations have been\n    // withheld in skippedStopStates, or when the current symbol is EOF.\n    //\n\n    if (skippedStopStates === null && t !== Token.EOF) {\n      if (intermediate.items.length === 1) {\n        // Don't pursue the closure if there is just one state.\n        // It can only have one alternative; just add to result\n        // Also don't pursue the closure if there is unique alternative\n        // among the configurations.\n        reach = intermediate;\n      } else if (this.getUniqueAlt(intermediate) !== ATN.INVALID_ALT_NUMBER) {\n        // Also don't pursue the closure if there is unique alternative\n        // among the configurations.\n        reach = intermediate;\n      }\n    } // If the reach set could not be trivially determined, perform a closure\n    // operation on the intermediate set to compute its initial value.\n    //\n\n\n    if (reach === null) {\n      reach = new ATNConfigSet(fullCtx);\n      const closureBusy = new Set();\n      const treatEofAsEpsilon = t === Token.EOF;\n\n      for (let k = 0; k < intermediate.items.length; k++) {\n        this.closure(intermediate.items[k], reach, closureBusy, false, fullCtx, treatEofAsEpsilon);\n      }\n    }\n\n    if (t === Token.EOF) {\n      // After consuming EOF no additional input is possible, so we are\n      // only interested in configurations which reached the end of the\n      // decision rule (local context) or end of the start rule (full\n      // context). Update reach to contain only these configurations. This\n      // handles both explicit EOF transitions in the grammar and implicit\n      // EOF transitions following the end of the decision or start rule.\n      //\n      // When reach==intermediate, no closure operation was performed. In\n      // this case, removeAllConfigsNotInRuleStopState needs to check for\n      // reachable rule stop states as well as configurations already in\n      // a rule stop state.\n      //\n      // This is handled before the configurations in skippedStopStates,\n      // because any configurations potentially added from that list are\n      // already guaranteed to meet this condition whether or not it's\n      // required.\n      //\n      reach = this.removeAllConfigsNotInRuleStopState(reach, reach === intermediate);\n    } // If skippedStopStates!==null, then it contains at least one\n    // configuration. For full-context reach operations, these\n    // configurations reached the end of the start rule, in which case we\n    // only add them back to reach if no configuration during the current\n    // closure operation reached such a state. This ensures adaptivePredict\n    // chooses an alternative matching the longest overall sequence when\n    // multiple alternatives are viable.\n    //\n\n\n    if (skippedStopStates !== null && (!fullCtx || !PredictionMode.hasConfigInRuleStopState(reach))) {\n      for (let l = 0; l < skippedStopStates.length; l++) {\n        reach.add(skippedStopStates[l], this.mergeCache);\n      }\n    }\n\n    if (reach.items.length === 0) {\n      return null;\n    } else {\n      return reach;\n    }\n  }\n  /**\n   * Return a configuration set containing only the configurations from\n   * {@code configs} which are in a {@link RuleStopState}. If all\n   * configurations in {@code configs} are already in a rule stop state, this\n   * method simply returns {@code configs}.\n   *\n   * <p>When {@code lookToEndOfRule} is true, this method uses\n   * {@link ATN//nextTokens} for each configuration in {@code configs} which is\n   * not already in a rule stop state to see if a rule stop state is reachable\n   * from the configuration via epsilon-only transitions.</p>\n   *\n   * @param configs the configuration set to update\n   * @param lookToEndOfRule when true, this method checks for rule stop states\n   * reachable by epsilon-only transitions from each configuration in\n   * {@code configs}.\n   *\n   * @return {@code configs} if all configurations in {@code configs} are in a\n   * rule stop state, otherwise return a new configuration set containing only\n   * the configurations from {@code configs} which are in a rule stop state\n   */\n\n\n  removeAllConfigsNotInRuleStopState(configs, lookToEndOfRule) {\n    if (PredictionMode.allConfigsInRuleStopStates(configs)) {\n      return configs;\n    }\n\n    const result = new ATNConfigSet(configs.fullCtx);\n\n    for (let i = 0; i < configs.items.length; i++) {\n      const config = configs.items[i];\n\n      if (config.state instanceof RuleStopState) {\n        result.add(config, this.mergeCache);\n        continue;\n      }\n\n      if (lookToEndOfRule && config.state.epsilonOnlyTransitions) {\n        const nextTokens = this.atn.nextTokens(config.state);\n\n        if (nextTokens.contains(Token.EPSILON)) {\n          const endOfRuleState = this.atn.ruleToStopState[config.state.ruleIndex];\n          result.add(new ATNConfig({\n            state: endOfRuleState\n          }, config), this.mergeCache);\n        }\n      }\n    }\n\n    return result;\n  }\n\n  computeStartState(p, ctx, fullCtx) {\n    // always at least the implicit call to start rule\n    const initialContext = predictionContextFromRuleContext(this.atn, ctx);\n    const configs = new ATNConfigSet(fullCtx);\n\n    for (let i = 0; i < p.transitions.length; i++) {\n      const target = p.transitions[i].target;\n      const c = new ATNConfig({\n        state: target,\n        alt: i + 1,\n        context: initialContext\n      }, null);\n      const closureBusy = new Set();\n      this.closure(c, configs, closureBusy, true, fullCtx, false);\n    }\n\n    return configs;\n  }\n  /**\n   * This method transforms the start state computed by\n   * {@link //computeStartState} to the special start state used by a\n   * precedence DFA for a particular precedence value. The transformation\n   * process applies the following changes to the start state's configuration\n   * set.\n   *\n   * <ol>\n   * <li>Evaluate the precedence predicates for each configuration using\n   * {@link SemanticContext//evalPrecedence}.</li>\n   * <li>Remove all configurations which predict an alternative greater than\n   * 1, for which another configuration that predicts alternative 1 is in the\n   * same ATN state with the same prediction context. This transformation is\n   * valid for the following reasons:\n   * <ul>\n   * <li>The closure block cannot contain any epsilon transitions which bypass\n   * the body of the closure, so all states reachable via alternative 1 are\n   * part of the precedence alternatives of the transformed left-recursive\n   * rule.</li>\n   * <li>The \"primary\" portion of a left recursive rule cannot contain an\n   * epsilon transition, so the only way an alternative other than 1 can exist\n   * in a state that is also reachable via alternative 1 is by nesting calls\n   * to the left-recursive rule, with the outer calls not being at the\n   * preferred precedence level.</li>\n   * </ul>\n   * </li>\n   * </ol>\n   *\n   * <p>\n   * The prediction context must be considered by this filter to address\n   * situations like the following.\n   * </p>\n   * <code>\n   * <pre>\n   * grammar TA;\n   * prog: statement* EOF;\n   * statement: letterA | statement letterA 'b' ;\n   * letterA: 'a';\n   * </pre>\n   * </code>\n   * <p>\n   * If the above grammar, the ATN state immediately before the token\n   * reference {@code 'a'} in {@code letterA} is reachable from the left edge\n   * of both the primary and closure blocks of the left-recursive rule\n   * {@code statement}. The prediction context associated with each of these\n   * configurations distinguishes between them, and prevents the alternative\n   * which stepped out to {@code prog} (and then back in to {@code statement}\n   * from being eliminated by the filter.\n   * </p>\n   *\n   * @param configs The configuration set computed by\n   * {@link //computeStartState} as the start state for the DFA.\n   * @return The transformed configuration set representing the start state\n   * for a precedence DFA at a particular precedence level (determined by\n   * calling {@link Parser//getPrecedence})\n   */\n\n\n  applyPrecedenceFilter(configs) {\n    let config;\n    const statesFromAlt1 = [];\n    const configSet = new ATNConfigSet(configs.fullCtx);\n\n    for (let i = 0; i < configs.items.length; i++) {\n      config = configs.items[i]; // handle alt 1 first\n\n      if (config.alt !== 1) {\n        continue;\n      }\n\n      const updatedContext = config.semanticContext.evalPrecedence(this.parser, this._outerContext);\n\n      if (updatedContext === null) {\n        // the configuration was eliminated\n        continue;\n      }\n\n      statesFromAlt1[config.state.stateNumber] = config.context;\n\n      if (updatedContext !== config.semanticContext) {\n        configSet.add(new ATNConfig({\n          semanticContext: updatedContext\n        }, config), this.mergeCache);\n      } else {\n        configSet.add(config, this.mergeCache);\n      }\n    }\n\n    for (let i = 0; i < configs.items.length; i++) {\n      config = configs.items[i];\n\n      if (config.alt === 1) {\n        // already handled\n        continue;\n      } // In the future, this elimination step could be updated to also\n      // filter the prediction context for alternatives predicting alt>1\n      // (basically a graph subtraction algorithm).\n\n\n      if (!config.precedenceFilterSuppressed) {\n        const context = statesFromAlt1[config.state.stateNumber] || null;\n\n        if (context !== null && context.equals(config.context)) {\n          // eliminated\n          continue;\n        }\n      }\n\n      configSet.add(config, this.mergeCache);\n    }\n\n    return configSet;\n  }\n\n  getReachableTarget(trans, ttype) {\n    if (trans.matches(ttype, 0, this.atn.maxTokenType)) {\n      return trans.target;\n    } else {\n      return null;\n    }\n  }\n\n  getPredsForAmbigAlts(ambigAlts, configs, nalts) {\n    // REACH=[1|1|[]|0:0, 1|2|[]|0:1]\n    // altToPred starts as an array of all null contexts. The entry at index i\n    // corresponds to alternative i. altToPred[i] may have one of three values:\n    //   1. null: no ATNConfig c is found such that c.alt==i\n    //   2. SemanticContext.NONE: At least one ATNConfig c exists such that\n    //      c.alt==i and c.semanticContext==SemanticContext.NONE. In other words,\n    //      alt i has at least one unpredicated config.\n    //   3. Non-NONE Semantic Context: There exists at least one, and for all\n    //      ATNConfig c such that c.alt==i, c.semanticContext!=SemanticContext.NONE.\n    //\n    // From this, it is clear that NONE||anything==NONE.\n    //\n    let altToPred = [];\n\n    for (let i = 0; i < configs.items.length; i++) {\n      const c = configs.items[i];\n\n      if (ambigAlts.contains(c.alt)) {\n        altToPred[c.alt] = SemanticContext.orContext(altToPred[c.alt] || null, c.semanticContext);\n      }\n    }\n\n    let nPredAlts = 0;\n\n    for (let i = 1; i < nalts + 1; i++) {\n      const pred = altToPred[i] || null;\n\n      if (pred === null) {\n        altToPred[i] = SemanticContext.NONE;\n      } else if (pred !== SemanticContext.NONE) {\n        nPredAlts += 1;\n      }\n    } // nonambig alts are null in altToPred\n\n\n    if (nPredAlts === 0) {\n      altToPred = null;\n    }\n\n    if (this.debug) {\n      console.log(\"getPredsForAmbigAlts result \" + Utils.arrayToString(altToPred));\n    }\n\n    return altToPred;\n  }\n\n  getPredicatePredictions(ambigAlts, altToPred) {\n    const pairs = [];\n    let containsPredicate = false;\n\n    for (let i = 1; i < altToPred.length; i++) {\n      const pred = altToPred[i]; // unpredicated is indicated by SemanticContext.NONE\n\n      if (ambigAlts !== null && ambigAlts.contains(i)) {\n        pairs.push(new PredPrediction(pred, i));\n      }\n\n      if (pred !== SemanticContext.NONE) {\n        containsPredicate = true;\n      }\n    }\n\n    if (!containsPredicate) {\n      return null;\n    }\n\n    return pairs;\n  }\n  /**\n   * This method is used to improve the localization of error messages by\n   * choosing an alternative rather than throwing a\n   * {@link NoViableAltException} in particular prediction scenarios where the\n   * {@link //ERROR} state was reached during ATN simulation.\n   *\n   * <p>\n   * The default implementation of this method uses the following\n   * algorithm to identify an ATN configuration which successfully parsed the\n   * decision entry rule. Choosing such an alternative ensures that the\n   * {@link ParserRuleContext} returned by the calling rule will be complete\n   * and valid, and the syntax error will be reported later at a more\n   * localized location.</p>\n   *\n   * <ul>\n   * <li>If a syntactically valid path or paths reach the end of the decision rule and\n   * they are semantically valid if predicated, return the min associated alt.</li>\n   * <li>Else, if a semantically invalid but syntactically valid path exist\n   * or paths exist, return the minimum associated alt.\n   * </li>\n   * <li>Otherwise, return {@link ATN//INVALID_ALT_NUMBER}.</li>\n   * </ul>\n   *\n   * <p>\n   * In some scenarios, the algorithm described above could predict an\n   * alternative which will result in a {@link FailedPredicateException} in\n   * the parser. Specifically, this could occur if the <em>only</em> configuration\n   * capable of successfully parsing to the end of the decision rule is\n   * blocked by a semantic predicate. By choosing this alternative within\n   * {@link //adaptivePredict} instead of throwing a\n   * {@link NoViableAltException}, the resulting\n   * {@link FailedPredicateException} in the parser will identify the specific\n   * predicate which is preventing the parser from successfully parsing the\n   * decision rule, which helps developers identify and correct logic errors\n   * in semantic predicates.\n   * </p>\n   *\n   * @param configs The ATN configurations which were valid immediately before\n   * the {@link //ERROR} state was reached\n   * @param outerContext The is the \\gamma_0 initial parser context from the paper\n   * or the parser stack at the instant before prediction commences.\n   *\n   * @return The value to return from {@link //adaptivePredict}, or\n   * {@link ATN//INVALID_ALT_NUMBER} if a suitable alternative was not\n   * identified and {@link //adaptivePredict} should report an error instead\n   */\n\n\n  getSynValidOrSemInvalidAltThatFinishedDecisionEntryRule(configs, outerContext) {\n    const cfgs = this.splitAccordingToSemanticValidity(configs, outerContext);\n    const semValidConfigs = cfgs[0];\n    const semInvalidConfigs = cfgs[1];\n    let alt = this.getAltThatFinishedDecisionEntryRule(semValidConfigs);\n\n    if (alt !== ATN.INVALID_ALT_NUMBER) {\n      // semantically/syntactically viable path exists\n      return alt;\n    } // Is there a syntactically valid path with a failed pred?\n\n\n    if (semInvalidConfigs.items.length > 0) {\n      alt = this.getAltThatFinishedDecisionEntryRule(semInvalidConfigs);\n\n      if (alt !== ATN.INVALID_ALT_NUMBER) {\n        // syntactically viable path exists\n        return alt;\n      }\n    }\n\n    return ATN.INVALID_ALT_NUMBER;\n  }\n\n  getAltThatFinishedDecisionEntryRule(configs) {\n    const alts = [];\n\n    for (let i = 0; i < configs.items.length; i++) {\n      const c = configs.items[i];\n\n      if (c.reachesIntoOuterContext > 0 || c.state instanceof RuleStopState && c.context.hasEmptyPath()) {\n        if (alts.indexOf(c.alt) < 0) {\n          alts.push(c.alt);\n        }\n      }\n    }\n\n    if (alts.length === 0) {\n      return ATN.INVALID_ALT_NUMBER;\n    } else {\n      return Math.min.apply(null, alts);\n    }\n  }\n  /**\n   * Walk the list of configurations and split them according to\n   * those that have preds evaluating to true/false.  If no pred, assume\n   * true pred and include in succeeded set.  Returns Pair of sets.\n   *\n   * Create a new set so as not to alter the incoming parameter.\n   *\n   * Assumption: the input stream has been restored to the starting point\n   * prediction, which is where predicates need to evaluate.*/\n\n\n  splitAccordingToSemanticValidity(configs, outerContext) {\n    const succeeded = new ATNConfigSet(configs.fullCtx);\n    const failed = new ATNConfigSet(configs.fullCtx);\n\n    for (let i = 0; i < configs.items.length; i++) {\n      const c = configs.items[i];\n\n      if (c.semanticContext !== SemanticContext.NONE) {\n        const predicateEvaluationResult = c.semanticContext.evaluate(this.parser, outerContext);\n\n        if (predicateEvaluationResult) {\n          succeeded.add(c);\n        } else {\n          failed.add(c);\n        }\n      } else {\n        succeeded.add(c);\n      }\n    }\n\n    return [succeeded, failed];\n  }\n  /**\n   * Look through a list of predicate/alt pairs, returning alts for the\n   * pairs that win. A {@code NONE} predicate indicates an alt containing an\n   * unpredicated config which behaves as \"always true.\" If !complete\n   * then we stop at the first predicate that evaluates to true. This\n   * includes pairs with null predicates.\n   */\n\n\n  evalSemanticContext(predPredictions, outerContext, complete) {\n    const predictions = new BitSet();\n\n    for (let i = 0; i < predPredictions.length; i++) {\n      const pair = predPredictions[i];\n\n      if (pair.pred === SemanticContext.NONE) {\n        predictions.add(pair.alt);\n\n        if (!complete) {\n          break;\n        }\n\n        continue;\n      }\n\n      const predicateEvaluationResult = pair.pred.evaluate(this.parser, outerContext);\n\n      if (this.debug || this.dfa_debug) {\n        console.log(\"eval pred \" + pair + \"=\" + predicateEvaluationResult);\n      }\n\n      if (predicateEvaluationResult) {\n        if (this.debug || this.dfa_debug) {\n          console.log(\"PREDICT \" + pair.alt);\n        }\n\n        predictions.add(pair.alt);\n\n        if (!complete) {\n          break;\n        }\n      }\n    }\n\n    return predictions;\n  } // TODO: If we are doing predicates, there is no point in pursuing\n  //     closure operations if we reach a DFA state that uniquely predicts\n  //     alternative. We will not be caching that DFA state and it is a\n  //     waste to pursue the closure. Might have to advance when we do\n  //     ambig detection thought :(\n  //\n\n\n  closure(config, configs, closureBusy, collectPredicates, fullCtx, treatEofAsEpsilon) {\n    const initialDepth = 0;\n    this.closureCheckingStopState(config, configs, closureBusy, collectPredicates, fullCtx, initialDepth, treatEofAsEpsilon);\n  }\n\n  closureCheckingStopState(config, configs, closureBusy, collectPredicates, fullCtx, depth, treatEofAsEpsilon) {\n    if (this.debug || this.debug_closure) {\n      console.log(\"closure(\" + config.toString(this.parser, true) + \")\"); // console.log(\"configs(\" + configs.toString() + \")\");\n\n      if (config.reachesIntoOuterContext > 50) {\n        throw \"problem\";\n      }\n    }\n\n    if (config.state instanceof RuleStopState) {\n      // We hit rule end. If we have context info, use it\n      // run thru all possible stack tops in ctx\n      if (!config.context.isEmpty()) {\n        for (let i = 0; i < config.context.length; i++) {\n          if (config.context.getReturnState(i) === PredictionContext.EMPTY_RETURN_STATE) {\n            if (fullCtx) {\n              configs.add(new ATNConfig({\n                state: config.state,\n                context: PredictionContext.EMPTY\n              }, config), this.mergeCache);\n              continue;\n            } else {\n              // we have no context info, just chase follow links (if greedy)\n              if (this.debug) {\n                console.log(\"FALLING off rule \" + this.getRuleName(config.state.ruleIndex));\n              }\n\n              this.closure_(config, configs, closureBusy, collectPredicates, fullCtx, depth, treatEofAsEpsilon);\n            }\n\n            continue;\n          }\n\n          const returnState = this.atn.states[config.context.getReturnState(i)];\n          const newContext = config.context.getParent(i); // \"pop\" return state\n\n          const parms = {\n            state: returnState,\n            alt: config.alt,\n            context: newContext,\n            semanticContext: config.semanticContext\n          };\n          const c = new ATNConfig(parms, null); // While we have context to pop back from, we may have\n          // gotten that context AFTER having falling off a rule.\n          // Make sure we track that we are now out of context.\n\n          c.reachesIntoOuterContext = config.reachesIntoOuterContext;\n          this.closureCheckingStopState(c, configs, closureBusy, collectPredicates, fullCtx, depth - 1, treatEofAsEpsilon);\n        }\n\n        return;\n      } else if (fullCtx) {\n        // reached end of start rule\n        configs.add(config, this.mergeCache);\n        return;\n      } else {\n        // else if we have no context info, just chase follow links (if greedy)\n        if (this.debug) {\n          console.log(\"FALLING off rule \" + this.getRuleName(config.state.ruleIndex));\n        }\n      }\n    }\n\n    this.closure_(config, configs, closureBusy, collectPredicates, fullCtx, depth, treatEofAsEpsilon);\n  } // Do the actual work of walking epsilon edges//\n\n\n  closure_(config, configs, closureBusy, collectPredicates, fullCtx, depth, treatEofAsEpsilon) {\n    const p = config.state; // optimization\n\n    if (!p.epsilonOnlyTransitions) {\n      configs.add(config, this.mergeCache); // make sure to not return here, because EOF transitions can act as\n      // both epsilon transitions and non-epsilon transitions.\n    }\n\n    for (let i = 0; i < p.transitions.length; i++) {\n      if (i === 0 && this.canDropLoopEntryEdgeInLeftRecursiveRule(config)) continue;\n      const t = p.transitions[i];\n      const continueCollecting = collectPredicates && !(t instanceof ActionTransition);\n      const c = this.getEpsilonTarget(config, t, continueCollecting, depth === 0, fullCtx, treatEofAsEpsilon);\n\n      if (c !== null) {\n        let newDepth = depth;\n\n        if (config.state instanceof RuleStopState) {\n          // target fell off end of rule; mark resulting c as having dipped into outer context\n          // We can't get here if incoming config was rule stop and we had context\n          // track how far we dip into outer context.  Might\n          // come in handy and we avoid evaluating context dependent\n          // preds if this is > 0.\n          if (this._dfa !== null && this._dfa.precedenceDfa) {\n            if (t.outermostPrecedenceReturn === this._dfa.atnStartState.ruleIndex) {\n              c.precedenceFilterSuppressed = true;\n            }\n          }\n\n          c.reachesIntoOuterContext += 1;\n\n          if (closureBusy.add(c) !== c) {\n            // avoid infinite recursion for right-recursive rules\n            continue;\n          }\n\n          configs.dipsIntoOuterContext = true; // TODO: can remove? only care when we add to set per middle of this method\n\n          newDepth -= 1;\n\n          if (this.debug) {\n            console.log(\"dips into outer ctx: \" + c);\n          }\n        } else {\n          if (!t.isEpsilon && closureBusy.add(c) !== c) {\n            // avoid infinite recursion for EOF* and EOF+\n            continue;\n          }\n\n          if (t instanceof RuleTransition) {\n            // latch when newDepth goes negative - once we step out of the entry context we can't return\n            if (newDepth >= 0) {\n              newDepth += 1;\n            }\n          }\n        }\n\n        this.closureCheckingStopState(c, configs, closureBusy, continueCollecting, fullCtx, newDepth, treatEofAsEpsilon);\n      }\n    }\n  }\n\n  canDropLoopEntryEdgeInLeftRecursiveRule(config) {\n    // return False\n    const p = config.state; // First check to see if we are in StarLoopEntryState generated during\n    // left-recursion elimination. For efficiency, also check if\n    // the context has an empty stack case. If so, it would mean\n    // global FOLLOW so we can't perform optimization\n    // Are we the special loop entry/exit state? or SLL wildcard\n\n    if (p.stateType !== ATNState.STAR_LOOP_ENTRY) return false;\n    if (p.stateType !== ATNState.STAR_LOOP_ENTRY || !p.isPrecedenceDecision || config.context.isEmpty() || config.context.hasEmptyPath()) return false; // Require all return states to return back to the same rule that p is in.\n\n    const numCtxs = config.context.length;\n\n    for (let i = 0; i < numCtxs; i++) {\n      // for each stack context\n      const returnState = this.atn.states[config.context.getReturnState(i)];\n      if (returnState.ruleIndex !== p.ruleIndex) return false;\n    }\n\n    const decisionStartState = p.transitions[0].target;\n    const blockEndStateNum = decisionStartState.endState.stateNumber;\n    const blockEndState = this.atn.states[blockEndStateNum]; // Verify that the top of each stack context leads to loop entry/exit\n    // state through epsilon edges and w/o leaving rule.\n\n    for (let i = 0; i < numCtxs; i++) {\n      // for each stack context\n      const returnStateNumber = config.context.getReturnState(i);\n      const returnState = this.atn.states[returnStateNumber]; // all states must have single outgoing epsilon edge\n\n      if (returnState.transitions.length !== 1 || !returnState.transitions[0].isEpsilon) return false; // Look for prefix op case like 'not expr', (' type ')' expr\n\n      const returnStateTarget = returnState.transitions[0].target;\n      if (returnState.stateType === ATNState.BLOCK_END && returnStateTarget === p) continue; // Look for 'expr op expr' or case where expr's return state is block end\n      // of (...)* internal block; the block end points to loop back\n      // which points to p but we don't need to check that\n\n      if (returnState === blockEndState) continue; // Look for ternary expr ? expr : expr. The return state points at block end,\n      // which points at loop entry state\n\n      if (returnStateTarget === blockEndState) continue; // Look for complex prefix 'between expr and expr' case where 2nd expr's\n      // return state points at block end state of (...)* internal block\n\n      if (returnStateTarget.stateType === ATNState.BLOCK_END && returnStateTarget.transitions.length === 1 && returnStateTarget.transitions[0].isEpsilon && returnStateTarget.transitions[0].target === p) continue; // anything else ain't conforming\n\n      return false;\n    }\n\n    return true;\n  }\n\n  getRuleName(index) {\n    if (this.parser !== null && index >= 0) {\n      return this.parser.ruleNames[index];\n    } else {\n      return \"<rule \" + index + \">\";\n    }\n  }\n\n  getEpsilonTarget(config, t, collectPredicates, inContext, fullCtx, treatEofAsEpsilon) {\n    switch (t.serializationType) {\n      case Transition.RULE:\n        return this.ruleTransition(config, t);\n\n      case Transition.PRECEDENCE:\n        return this.precedenceTransition(config, t, collectPredicates, inContext, fullCtx);\n\n      case Transition.PREDICATE:\n        return this.predTransition(config, t, collectPredicates, inContext, fullCtx);\n\n      case Transition.ACTION:\n        return this.actionTransition(config, t);\n\n      case Transition.EPSILON:\n        return new ATNConfig({\n          state: t.target\n        }, config);\n\n      case Transition.ATOM:\n      case Transition.RANGE:\n      case Transition.SET:\n        // EOF transitions act like epsilon transitions after the first EOF\n        // transition is traversed\n        if (treatEofAsEpsilon) {\n          if (t.matches(Token.EOF, 0, 1)) {\n            return new ATNConfig({\n              state: t.target\n            }, config);\n          }\n        }\n\n        return null;\n\n      default:\n        return null;\n    }\n  }\n\n  actionTransition(config, t) {\n    if (this.debug) {\n      const index = t.actionIndex === -1 ? 65535 : t.actionIndex;\n      console.log(\"ACTION edge \" + t.ruleIndex + \":\" + index);\n    }\n\n    return new ATNConfig({\n      state: t.target\n    }, config);\n  }\n\n  precedenceTransition(config, pt, collectPredicates, inContext, fullCtx) {\n    if (this.debug) {\n      console.log(\"PRED (collectPredicates=\" + collectPredicates + \") \" + pt.precedence + \">=_p, ctx dependent=true\");\n\n      if (this.parser !== null) {\n        console.log(\"context surrounding pred is \" + Utils.arrayToString(this.parser.getRuleInvocationStack()));\n      }\n    }\n\n    let c = null;\n\n    if (collectPredicates && inContext) {\n      if (fullCtx) {\n        // In full context mode, we can evaluate predicates on-the-fly\n        // during closure, which dramatically reduces the size of\n        // the config sets. It also obviates the need to test predicates\n        // later during conflict resolution.\n        const currentPosition = this._input.index;\n\n        this._input.seek(this._startIndex);\n\n        const predSucceeds = pt.getPredicate().evaluate(this.parser, this._outerContext);\n\n        this._input.seek(currentPosition);\n\n        if (predSucceeds) {\n          c = new ATNConfig({\n            state: pt.target\n          }, config); // no pred context\n        }\n      } else {\n        const newSemCtx = SemanticContext.andContext(config.semanticContext, pt.getPredicate());\n        c = new ATNConfig({\n          state: pt.target,\n          semanticContext: newSemCtx\n        }, config);\n      }\n    } else {\n      c = new ATNConfig({\n        state: pt.target\n      }, config);\n    }\n\n    if (this.debug) {\n      console.log(\"config from pred transition=\" + c);\n    }\n\n    return c;\n  }\n\n  predTransition(config, pt, collectPredicates, inContext, fullCtx) {\n    if (this.debug) {\n      console.log(\"PRED (collectPredicates=\" + collectPredicates + \") \" + pt.ruleIndex + \":\" + pt.predIndex + \", ctx dependent=\" + pt.isCtxDependent);\n\n      if (this.parser !== null) {\n        console.log(\"context surrounding pred is \" + Utils.arrayToString(this.parser.getRuleInvocationStack()));\n      }\n    }\n\n    let c = null;\n\n    if (collectPredicates && (pt.isCtxDependent && inContext || !pt.isCtxDependent)) {\n      if (fullCtx) {\n        // In full context mode, we can evaluate predicates on-the-fly\n        // during closure, which dramatically reduces the size of\n        // the config sets. It also obviates the need to test predicates\n        // later during conflict resolution.\n        const currentPosition = this._input.index;\n\n        this._input.seek(this._startIndex);\n\n        const predSucceeds = pt.getPredicate().evaluate(this.parser, this._outerContext);\n\n        this._input.seek(currentPosition);\n\n        if (predSucceeds) {\n          c = new ATNConfig({\n            state: pt.target\n          }, config); // no pred context\n        }\n      } else {\n        const newSemCtx = SemanticContext.andContext(config.semanticContext, pt.getPredicate());\n        c = new ATNConfig({\n          state: pt.target,\n          semanticContext: newSemCtx\n        }, config);\n      }\n    } else {\n      c = new ATNConfig({\n        state: pt.target\n      }, config);\n    }\n\n    if (this.debug) {\n      console.log(\"config from pred transition=\" + c);\n    }\n\n    return c;\n  }\n\n  ruleTransition(config, t) {\n    if (this.debug) {\n      console.log(\"CALL rule \" + this.getRuleName(t.target.ruleIndex) + \", ctx=\" + config.context);\n    }\n\n    const returnState = t.followState;\n    const newContext = SingletonPredictionContext.create(config.context, returnState.stateNumber);\n    return new ATNConfig({\n      state: t.target,\n      context: newContext\n    }, config);\n  }\n\n  getConflictingAlts(configs) {\n    const altsets = PredictionMode.getConflictingAltSubsets(configs);\n    return PredictionMode.getAlts(altsets);\n  }\n  /**\n   * Sam pointed out a problem with the previous definition, v3, of\n   * ambiguous states. If we have another state associated with conflicting\n   * alternatives, we should keep going. For example, the following grammar\n   *\n   * s : (ID | ID ID?) ';' ;\n   *\n   * When the ATN simulation reaches the state before ';', it has a DFA\n   * state that looks like: [12|1|[], 6|2|[], 12|2|[]]. Naturally\n   * 12|1|[] and 12|2|[] conflict, but we cannot stop processing this node\n   * because alternative to has another way to continue, via [6|2|[]].\n   * The key is that we have a single state that has config's only associated\n   * with a single alternative, 2, and crucially the state transitions\n   * among the configurations are all non-epsilon transitions. That means\n   * we don't consider any conflicts that include alternative 2. So, we\n   * ignore the conflict between alts 1 and 2. We ignore a set of\n   * conflicting alts when there is an intersection with an alternative\n   * associated with a single alt state in the state&rarr;config-list map.\n   *\n   * It's also the case that we might have two conflicting configurations but\n   * also a 3rd nonconflicting configuration for a different alternative:\n   * [1|1|[], 1|2|[], 8|3|[]]. This can come about from grammar:\n   *\n   * a : A | A | A B ;\n   *\n   * After matching input A, we reach the stop state for rule A, state 1.\n   * State 8 is the state right before B. Clearly alternatives 1 and 2\n   * conflict and no amount of further lookahead will separate the two.\n   * However, alternative 3 will be able to continue and so we do not\n   * stop working on this state. In the previous example, we're concerned\n   * with states associated with the conflicting alternatives. Here alt\n   * 3 is not associated with the conflicting configs, but since we can continue\n   * looking for input reasonably, I don't declare the state done. We\n   * ignore a set of conflicting alts when we have an alternative\n   * that we still need to pursue\n   */\n\n\n  getConflictingAltsOrUniqueAlt(configs) {\n    let conflictingAlts = null;\n\n    if (configs.uniqueAlt !== ATN.INVALID_ALT_NUMBER) {\n      conflictingAlts = new BitSet();\n      conflictingAlts.add(configs.uniqueAlt);\n    } else {\n      conflictingAlts = configs.conflictingAlts;\n    }\n\n    return conflictingAlts;\n  }\n\n  getTokenName(t) {\n    if (t === Token.EOF) {\n      return \"EOF\";\n    }\n\n    if (this.parser !== null && this.parser.literalNames !== null) {\n      if (t >= this.parser.literalNames.length && t >= this.parser.symbolicNames.length) {\n        console.log(\"\" + t + \" ttype out of range: \" + this.parser.literalNames);\n        console.log(\"\" + this.parser.getInputStream().getTokens());\n      } else {\n        const name = this.parser.literalNames[t] || this.parser.symbolicNames[t];\n        return name + \"<\" + t + \">\";\n      }\n    }\n\n    return \"\" + t;\n  }\n\n  getLookaheadName(input) {\n    return this.getTokenName(input.LA(1));\n  }\n  /**\n   * Used for debugging in adaptivePredict around execATN but I cut\n   * it out for clarity now that alg. works well. We can leave this\n   * \"dead\" code for a bit\n   */\n\n\n  dumpDeadEndConfigs(nvae) {\n    console.log(\"dead end configs: \");\n    const decs = nvae.getDeadEndConfigs();\n\n    for (let i = 0; i < decs.length; i++) {\n      const c = decs[i];\n      let trans = \"no edges\";\n\n      if (c.state.transitions.length > 0) {\n        const t = c.state.transitions[0];\n\n        if (t instanceof AtomTransition) {\n          trans = \"Atom \" + this.getTokenName(t.label);\n        } else if (t instanceof SetTransition) {\n          const neg = t instanceof NotSetTransition;\n          trans = (neg ? \"~\" : \"\") + \"Set \" + t.set;\n        }\n      }\n\n      console.error(c.toString(this.parser, true) + \":\" + trans);\n    }\n  }\n\n  noViableAlt(input, outerContext, configs, startIndex) {\n    return new NoViableAltException(this.parser, input, input.get(startIndex), input.LT(1), configs, outerContext);\n  }\n\n  getUniqueAlt(configs) {\n    let alt = ATN.INVALID_ALT_NUMBER;\n\n    for (let i = 0; i < configs.items.length; i++) {\n      const c = configs.items[i];\n\n      if (alt === ATN.INVALID_ALT_NUMBER) {\n        alt = c.alt; // found first alt\n      } else if (c.alt !== alt) {\n        return ATN.INVALID_ALT_NUMBER;\n      }\n    }\n\n    return alt;\n  }\n  /**\n   * Add an edge to the DFA, if possible. This method calls\n   * {@link //addDFAState} to ensure the {@code to} state is present in the\n   * DFA. If {@code from} is {@code null}, or if {@code t} is outside the\n   * range of edges that can be represented in the DFA tables, this method\n   * returns without adding the edge to the DFA.\n   *\n   * <p>If {@code to} is {@code null}, this method returns {@code null}.\n   * Otherwise, this method returns the {@link DFAState} returned by calling\n   * {@link //addDFAState} for the {@code to} state.</p>\n   *\n   * @param dfa The DFA\n   * @param from_ The source state for the edge\n   * @param t The input symbol\n   * @param to The target state for the edge\n   *\n   * @return If {@code to} is {@code null}, this method returns {@code null};\n   * otherwise this method returns the result of calling {@link //addDFAState}\n   * on {@code to}\n   */\n\n\n  addDFAEdge(dfa, from_, t, to) {\n    if (this.debug) {\n      console.log(\"EDGE \" + from_ + \" -> \" + to + \" upon \" + this.getTokenName(t));\n    }\n\n    if (to === null) {\n      return null;\n    }\n\n    to = this.addDFAState(dfa, to); // used existing if possible not incoming\n\n    if (from_ === null || t < -1 || t > this.atn.maxTokenType) {\n      return to;\n    }\n\n    if (from_.edges === null) {\n      from_.edges = [];\n    }\n\n    from_.edges[t + 1] = to; // connect\n\n    if (this.debug) {\n      const literalNames = this.parser === null ? null : this.parser.literalNames;\n      const symbolicNames = this.parser === null ? null : this.parser.symbolicNames;\n      console.log(\"DFA=\\n\" + dfa.toString(literalNames, symbolicNames));\n    }\n\n    return to;\n  }\n  /**\n   * Add state {@code D} to the DFA if it is not already present, and return\n   * the actual instance stored in the DFA. If a state equivalent to {@code D}\n   * is already in the DFA, the existing state is returned. Otherwise this\n   * method returns {@code D} after adding it to the DFA.\n   *\n   * <p>If {@code D} is {@link //ERROR}, this method returns {@link //ERROR} and\n   * does not change the DFA.</p>\n   *\n   * @param dfa The dfa\n   * @param D The DFA state to add\n   * @return The state stored in the DFA. This will be either the existing\n   * state if {@code D} is already in the DFA, or {@code D} itself if the\n   * state was not already present\n   */\n\n\n  addDFAState(dfa, D) {\n    if (D === ATNSimulator.ERROR) {\n      return D;\n    }\n\n    const existing = dfa.states.get(D);\n\n    if (existing !== null) {\n      return existing;\n    }\n\n    D.stateNumber = dfa.states.length;\n\n    if (!D.configs.readOnly) {\n      D.configs.optimizeConfigs(this);\n      D.configs.setReadonly(true);\n    }\n\n    dfa.states.add(D);\n\n    if (this.debug) {\n      console.log(\"adding new DFA state: \" + D);\n    }\n\n    return D;\n  }\n\n  reportAttemptingFullContext(dfa, conflictingAlts, configs, startIndex, stopIndex) {\n    if (this.debug || this.retry_debug) {\n      const interval = new Interval(startIndex, stopIndex + 1);\n      console.log(\"reportAttemptingFullContext decision=\" + dfa.decision + \":\" + configs + \", input=\" + this.parser.getTokenStream().getText(interval));\n    }\n\n    if (this.parser !== null) {\n      this.parser.getErrorListenerDispatch().reportAttemptingFullContext(this.parser, dfa, startIndex, stopIndex, conflictingAlts, configs);\n    }\n  }\n\n  reportContextSensitivity(dfa, prediction, configs, startIndex, stopIndex) {\n    if (this.debug || this.retry_debug) {\n      const interval = new Interval(startIndex, stopIndex + 1);\n      console.log(\"reportContextSensitivity decision=\" + dfa.decision + \":\" + configs + \", input=\" + this.parser.getTokenStream().getText(interval));\n    }\n\n    if (this.parser !== null) {\n      this.parser.getErrorListenerDispatch().reportContextSensitivity(this.parser, dfa, startIndex, stopIndex, prediction, configs);\n    }\n  } // If context sensitive parsing, we know it's ambiguity not conflict//\n\n\n  reportAmbiguity(dfa, D, startIndex, stopIndex, exact, ambigAlts, configs) {\n    if (this.debug || this.retry_debug) {\n      const interval = new Interval(startIndex, stopIndex + 1);\n      console.log(\"reportAmbiguity \" + ambigAlts + \":\" + configs + \", input=\" + this.parser.getTokenStream().getText(interval));\n    }\n\n    if (this.parser !== null) {\n      this.parser.getErrorListenerDispatch().reportAmbiguity(this.parser, dfa, startIndex, stopIndex, exact, ambigAlts, configs);\n    }\n  }\n\n}\n\nmodule.exports = ParserATNSimulator;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/ParserATNSimulator.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/PredictionMode.js":
/*!**************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/PredictionMode.js ***!
  \**************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Map,\n  BitSet,\n  AltDict,\n  hashStuff\n} = __webpack_require__(/*! ./../Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n\nconst ATN = __webpack_require__(/*! ./ATN */ \"./node_modules/antlr4/src/antlr4/atn/ATN.js\");\n\nconst {\n  RuleStopState\n} = __webpack_require__(/*! ./ATNState */ \"./node_modules/antlr4/src/antlr4/atn/ATNState.js\");\n\nconst {\n  ATNConfigSet\n} = __webpack_require__(/*! ./ATNConfigSet */ \"./node_modules/antlr4/src/antlr4/atn/ATNConfigSet.js\");\n\nconst {\n  ATNConfig\n} = __webpack_require__(/*! ./ATNConfig */ \"./node_modules/antlr4/src/antlr4/atn/ATNConfig.js\");\n\nconst {\n  SemanticContext\n} = __webpack_require__(/*! ./SemanticContext */ \"./node_modules/antlr4/src/antlr4/atn/SemanticContext.js\");\n/**\n * This enumeration defines the prediction modes available in ANTLR 4 along with\n * utility methods for analyzing configuration sets for conflicts and/or\n * ambiguities.\n */\n\n\nconst PredictionMode = {\n  /**\n   * The SLL(*) prediction mode. This prediction mode ignores the current\n   * parser context when making predictions. This is the fastest prediction\n   * mode, and provides correct results for many grammars. This prediction\n   * mode is more powerful than the prediction mode provided by ANTLR 3, but\n   * may result in syntax errors for grammar and input combinations which are\n   * not SLL.\n   *\n   * <p>\n   * When using this prediction mode, the parser will either return a correct\n   * parse tree (i.e. the same parse tree that would be returned with the\n   * {@link //LL} prediction mode), or it will report a syntax error. If a\n   * syntax error is encountered when using the {@link //SLL} prediction mode,\n   * it may be due to either an actual syntax error in the input or indicate\n   * that the particular combination of grammar and input requires the more\n   * powerful {@link //LL} prediction abilities to complete successfully.</p>\n   *\n   * <p>\n   * This prediction mode does not provide any guarantees for prediction\n   * behavior for syntactically-incorrect inputs.</p>\n   */\n  SLL: 0,\n\n  /**\n   * The LL(*) prediction mode. This prediction mode allows the current parser\n   * context to be used for resolving SLL conflicts that occur during\n   * prediction. This is the fastest prediction mode that guarantees correct\n   * parse results for all combinations of grammars with syntactically correct\n   * inputs.\n   *\n   * <p>\n   * When using this prediction mode, the parser will make correct decisions\n   * for all syntactically-correct grammar and input combinations. However, in\n   * cases where the grammar is truly ambiguous this prediction mode might not\n   * report a precise answer for <em>exactly which</em> alternatives are\n   * ambiguous.</p>\n   *\n   * <p>\n   * This prediction mode does not provide any guarantees for prediction\n   * behavior for syntactically-incorrect inputs.</p>\n   */\n  LL: 1,\n\n  /**\n   *\n   * The LL(*) prediction mode with exact ambiguity detection. In addition to\n   * the correctness guarantees provided by the {@link //LL} prediction mode,\n   * this prediction mode instructs the prediction algorithm to determine the\n   * complete and exact set of ambiguous alternatives for every ambiguous\n   * decision encountered while parsing.\n   *\n   * <p>\n   * This prediction mode may be used for diagnosing ambiguities during\n   * grammar development. Due to the performance overhead of calculating sets\n   * of ambiguous alternatives, this prediction mode should be avoided when\n   * the exact results are not necessary.</p>\n   *\n   * <p>\n   * This prediction mode does not provide any guarantees for prediction\n   * behavior for syntactically-incorrect inputs.</p>\n   */\n  LL_EXACT_AMBIG_DETECTION: 2,\n\n  /**\n   *\n   * Computes the SLL prediction termination condition.\n   *\n   * <p>\n   * This method computes the SLL prediction termination condition for both of\n   * the following cases.</p>\n   *\n   * <ul>\n   * <li>The usual SLL+LL fallback upon SLL conflict</li>\n   * <li>Pure SLL without LL fallback</li>\n   * </ul>\n   *\n   * <p><strong>COMBINED SLL+LL PARSING</strong></p>\n   *\n   * <p>When LL-fallback is enabled upon SLL conflict, correct predictions are\n   * ensured regardless of how the termination condition is computed by this\n   * method. Due to the substantially higher cost of LL prediction, the\n   * prediction should only fall back to LL when the additional lookahead\n   * cannot lead to a unique SLL prediction.</p>\n   *\n   * <p>Assuming combined SLL+LL parsing, an SLL configuration set with only\n   * conflicting subsets should fall back to full LL, even if the\n   * configuration sets don't resolve to the same alternative (e.g.\n   * {@code {1,2}} and {@code {3,4}}. If there is at least one non-conflicting\n   * configuration, SLL could continue with the hopes that more lookahead will\n   * resolve via one of those non-conflicting configurations.</p>\n   *\n   * <p>Here's the prediction termination rule them: SLL (for SLL+LL parsing)\n   * stops when it sees only conflicting configuration subsets. In contrast,\n   * full LL keeps going when there is uncertainty.</p>\n   *\n   * <p><strong>HEURISTIC</strong></p>\n   *\n   * <p>As a heuristic, we stop prediction when we see any conflicting subset\n   * unless we see a state that only has one alternative associated with it.\n   * The single-alt-state thing lets prediction continue upon rules like\n   * (otherwise, it would admit defeat too soon):</p>\n   *\n   * <p>{@code [12|1|[], 6|2|[], 12|2|[]]. s : (ID | ID ID?) ';' ;}</p>\n   *\n   * <p>When the ATN simulation reaches the state before {@code ';'}, it has a\n   * DFA state that looks like: {@code [12|1|[], 6|2|[], 12|2|[]]}. Naturally\n   * {@code 12|1|[]} and {@code 12|2|[]} conflict, but we cannot stop\n   * processing this node because alternative to has another way to continue,\n   * via {@code [6|2|[]]}.</p>\n   *\n   * <p>It also let's us continue for this rule:</p>\n   *\n   * <p>{@code [1|1|[], 1|2|[], 8|3|[]] a : A | A | A B ;}</p>\n   *\n   * <p>After matching input A, we reach the stop state for rule A, state 1.\n   * State 8 is the state right before B. Clearly alternatives 1 and 2\n   * conflict and no amount of further lookahead will separate the two.\n   * However, alternative 3 will be able to continue and so we do not stop\n   * working on this state. In the previous example, we're concerned with\n   * states associated with the conflicting alternatives. Here alt 3 is not\n   * associated with the conflicting configs, but since we can continue\n   * looking for input reasonably, don't declare the state done.</p>\n   *\n   * <p><strong>PURE SLL PARSING</strong></p>\n   *\n   * <p>To handle pure SLL parsing, all we have to do is make sure that we\n   * combine stack contexts for configurations that differ only by semantic\n   * predicate. From there, we can do the usual SLL termination heuristic.</p>\n   *\n   * <p><strong>PREDICATES IN SLL+LL PARSING</strong></p>\n   *\n   * <p>SLL decisions don't evaluate predicates until after they reach DFA stop\n   * states because they need to create the DFA cache that works in all\n   * semantic situations. In contrast, full LL evaluates predicates collected\n   * during start state computation so it can ignore predicates thereafter.\n   * This means that SLL termination detection can totally ignore semantic\n   * predicates.</p>\n   *\n   * <p>Implementation-wise, {@link ATNConfigSet} combines stack contexts but not\n   * semantic predicate contexts so we might see two configurations like the\n   * following.</p>\n   *\n   * <p>{@code (s, 1, x, {}), (s, 1, x', {p})}</p>\n   *\n   * <p>Before testing these configurations against others, we have to merge\n   * {@code x} and {@code x'} (without modifying the existing configurations).\n   * For example, we test {@code (x+x')==x''} when looking for conflicts in\n   * the following configurations.</p>\n   *\n   * <p>{@code (s, 1, x, {}), (s, 1, x', {p}), (s, 2, x'', {})}</p>\n   *\n   * <p>If the configuration set has predicates (as indicated by\n   * {@link ATNConfigSet//hasSemanticContext}), this algorithm makes a copy of\n   * the configurations to strip out all of the predicates so that a standard\n   * {@link ATNConfigSet} will merge everything ignoring predicates.</p>\n   */\n  hasSLLConflictTerminatingPrediction: function (mode, configs) {\n    // Configs in rule stop states indicate reaching the end of the decision\n    // rule (local context) or end of start rule (full context). If all\n    // configs meet this condition, then none of the configurations is able\n    // to match additional input so we terminate prediction.\n    //\n    if (PredictionMode.allConfigsInRuleStopStates(configs)) {\n      return true;\n    } // pure SLL mode parsing\n\n\n    if (mode === PredictionMode.SLL) {\n      // Don't bother with combining configs from different semantic\n      // contexts if we can fail over to full LL; costs more time\n      // since we'll often fail over anyway.\n      if (configs.hasSemanticContext) {\n        // dup configs, tossing out semantic predicates\n        const dup = new ATNConfigSet();\n\n        for (let i = 0; i < configs.items.length; i++) {\n          let c = configs.items[i];\n          c = new ATNConfig({\n            semanticContext: SemanticContext.NONE\n          }, c);\n          dup.add(c);\n        }\n\n        configs = dup;\n      } // now we have combined contexts for configs with dissimilar preds\n\n    } // pure SLL or combined SLL+LL mode parsing\n\n\n    const altsets = PredictionMode.getConflictingAltSubsets(configs);\n    return PredictionMode.hasConflictingAltSet(altsets) && !PredictionMode.hasStateAssociatedWithOneAlt(configs);\n  },\n\n  /**\n   * Checks if any configuration in {@code configs} is in a\n   * {@link RuleStopState}. Configurations meeting this condition have reached\n   * the end of the decision rule (local context) or end of start rule (full\n   * context).\n   *\n   * @param configs the configuration set to test\n   * @return {@code true} if any configuration in {@code configs} is in a\n   * {@link RuleStopState}, otherwise {@code false}\n   */\n  hasConfigInRuleStopState: function (configs) {\n    for (let i = 0; i < configs.items.length; i++) {\n      const c = configs.items[i];\n\n      if (c.state instanceof RuleStopState) {\n        return true;\n      }\n    }\n\n    return false;\n  },\n\n  /**\n   * Checks if all configurations in {@code configs} are in a\n   * {@link RuleStopState}. Configurations meeting this condition have reached\n   * the end of the decision rule (local context) or end of start rule (full\n   * context).\n   *\n   * @param configs the configuration set to test\n   * @return {@code true} if all configurations in {@code configs} are in a\n   * {@link RuleStopState}, otherwise {@code false}\n   */\n  allConfigsInRuleStopStates: function (configs) {\n    for (let i = 0; i < configs.items.length; i++) {\n      const c = configs.items[i];\n\n      if (!(c.state instanceof RuleStopState)) {\n        return false;\n      }\n    }\n\n    return true;\n  },\n\n  /**\n   *\n   * Full LL prediction termination.\n   *\n   * <p>Can we stop looking ahead during ATN simulation or is there some\n   * uncertainty as to which alternative we will ultimately pick, after\n   * consuming more input? Even if there are partial conflicts, we might know\n   * that everything is going to resolve to the same minimum alternative. That\n   * means we can stop since no more lookahead will change that fact. On the\n   * other hand, there might be multiple conflicts that resolve to different\n   * minimums. That means we need more look ahead to decide which of those\n   * alternatives we should predict.</p>\n   *\n   * <p>The basic idea is to split the set of configurations {@code C}, into\n   * conflicting subsets {@code (s, _, ctx, _)} and singleton subsets with\n   * non-conflicting configurations. Two configurations conflict if they have\n   * identical {@link ATNConfig//state} and {@link ATNConfig//context} values\n   * but different {@link ATNConfig//alt} value, e.g. {@code (s, i, ctx, _)}\n   * and {@code (s, j, ctx, _)} for {@code i!=j}.</p>\n   *\n   * <p>Reduce these configuration subsets to the set of possible alternatives.\n   * You can compute the alternative subsets in one pass as follows:</p>\n   *\n   * <p>{@code A_s,ctx = {i | (s, i, ctx, _)}} for each configuration in\n   * {@code C} holding {@code s} and {@code ctx} fixed.</p>\n   *\n   * <p>Or in pseudo-code, for each configuration {@code c} in {@code C}:</p>\n   *\n   * <pre>\n   * map[c] U= c.{@link ATNConfig//alt alt} // map hash/equals uses s and x, not\n   * alt and not pred\n   * </pre>\n   *\n   * <p>The values in {@code map} are the set of {@code A_s,ctx} sets.</p>\n   *\n   * <p>If {@code |A_s,ctx|=1} then there is no conflict associated with\n   * {@code s} and {@code ctx}.</p>\n   *\n   * <p>Reduce the subsets to singletons by choosing a minimum of each subset. If\n   * the union of these alternative subsets is a singleton, then no amount of\n   * more lookahead will help us. We will always pick that alternative. If,\n   * however, there is more than one alternative, then we are uncertain which\n   * alternative to predict and must continue looking for resolution. We may\n   * or may not discover an ambiguity in the future, even if there are no\n   * conflicting subsets this round.</p>\n   *\n   * <p>The biggest sin is to terminate early because it means we've made a\n   * decision but were uncertain as to the eventual outcome. We haven't used\n   * enough lookahead. On the other hand, announcing a conflict too late is no\n   * big deal; you will still have the conflict. It's just inefficient. It\n   * might even look until the end of file.</p>\n   *\n   * <p>No special consideration for semantic predicates is required because\n   * predicates are evaluated on-the-fly for full LL prediction, ensuring that\n   * no configuration contains a semantic context during the termination\n   * check.</p>\n   *\n   * <p><strong>CONFLICTING CONFIGS</strong></p>\n   *\n   * <p>Two configurations {@code (s, i, x)} and {@code (s, j, x')}, conflict\n   * when {@code i!=j} but {@code x=x'}. Because we merge all\n   * {@code (s, i, _)} configurations together, that means that there are at\n   * most {@code n} configurations associated with state {@code s} for\n   * {@code n} possible alternatives in the decision. The merged stacks\n   * complicate the comparison of configuration contexts {@code x} and\n   * {@code x'}. Sam checks to see if one is a subset of the other by calling\n   * merge and checking to see if the merged result is either {@code x} or\n   * {@code x'}. If the {@code x} associated with lowest alternative {@code i}\n   * is the superset, then {@code i} is the only possible prediction since the\n   * others resolve to {@code min(i)} as well. However, if {@code x} is\n   * associated with {@code j>i} then at least one stack configuration for\n   * {@code j} is not in conflict with alternative {@code i}. The algorithm\n   * should keep going, looking for more lookahead due to the uncertainty.</p>\n   *\n   * <p>For simplicity, I'm doing a equality check between {@code x} and\n   * {@code x'} that lets the algorithm continue to consume lookahead longer\n   * than necessary. The reason I like the equality is of course the\n   * simplicity but also because that is the test you need to detect the\n   * alternatives that are actually in conflict.</p>\n   *\n   * <p><strong>CONTINUE/STOP RULE</strong></p>\n   *\n   * <p>Continue if union of resolved alternative sets from non-conflicting and\n   * conflicting alternative subsets has more than one alternative. We are\n   * uncertain about which alternative to predict.</p>\n   *\n   * <p>The complete set of alternatives, {@code [i for (_,i,_)]}, tells us which\n   * alternatives are still in the running for the amount of input we've\n   * consumed at this point. The conflicting sets let us to strip away\n   * configurations that won't lead to more states because we resolve\n   * conflicts to the configuration with a minimum alternate for the\n   * conflicting set.</p>\n   *\n   * <p><strong>CASES</strong></p>\n   *\n   * <ul>\n   *\n   * <li>no conflicts and more than 1 alternative in set =&gt; continue</li>\n   *\n   * <li> {@code (s, 1, x)}, {@code (s, 2, x)}, {@code (s, 3, z)},\n   * {@code (s', 1, y)}, {@code (s', 2, y)} yields non-conflicting set\n   * {@code {3}} U conflicting sets {@code min({1,2})} U {@code min({1,2})} =\n   * {@code {1,3}} =&gt; continue\n   * </li>\n   *\n   * <li>{@code (s, 1, x)}, {@code (s, 2, x)}, {@code (s', 1, y)},\n   * {@code (s', 2, y)}, {@code (s'', 1, z)} yields non-conflicting set\n   * {@code {1}} U conflicting sets {@code min({1,2})} U {@code min({1,2})} =\n   * {@code {1}} =&gt; stop and predict 1</li>\n   *\n   * <li>{@code (s, 1, x)}, {@code (s, 2, x)}, {@code (s', 1, y)},\n   * {@code (s', 2, y)} yields conflicting, reduced sets {@code {1}} U\n   * {@code {1}} = {@code {1}} =&gt; stop and predict 1, can announce\n   * ambiguity {@code {1,2}}</li>\n   *\n   * <li>{@code (s, 1, x)}, {@code (s, 2, x)}, {@code (s', 2, y)},\n   * {@code (s', 3, y)} yields conflicting, reduced sets {@code {1}} U\n   * {@code {2}} = {@code {1,2}} =&gt; continue</li>\n   *\n   * <li>{@code (s, 1, x)}, {@code (s, 2, x)}, {@code (s', 3, y)},\n   * {@code (s', 4, y)} yields conflicting, reduced sets {@code {1}} U\n   * {@code {3}} = {@code {1,3}} =&gt; continue</li>\n   *\n   * </ul>\n   *\n   * <p><strong>EXACT AMBIGUITY DETECTION</strong></p>\n   *\n   * <p>If all states report the same conflicting set of alternatives, then we\n   * know we have the exact ambiguity set.</p>\n   *\n   * <p><code>|A_<em>i</em>|&gt;1</code> and\n   * <code>A_<em>i</em> = A_<em>j</em></code> for all <em>i</em>, <em>j</em>.</p>\n   *\n   * <p>In other words, we continue examining lookahead until all {@code A_i}\n   * have more than one alternative and all {@code A_i} are the same. If\n   * {@code A={{1,2}, {1,3}}}, then regular LL prediction would terminate\n   * because the resolved set is {@code {1}}. To determine what the real\n   * ambiguity is, we have to know whether the ambiguity is between one and\n   * two or one and three so we keep going. We can only stop prediction when\n   * we need exact ambiguity detection when the sets look like\n   * {@code A={{1,2}}} or {@code {{1,2},{1,2}}}, etc...</p>\n   */\n  resolvesToJustOneViableAlt: function (altsets) {\n    return PredictionMode.getSingleViableAlt(altsets);\n  },\n\n  /**\n   * Determines if every alternative subset in {@code altsets} contains more\n   * than one alternative.\n   *\n   * @param altsets a collection of alternative subsets\n   * @return {@code true} if every {@link BitSet} in {@code altsets} has\n   * {@link BitSet//cardinality cardinality} &gt; 1, otherwise {@code false}\n   */\n  allSubsetsConflict: function (altsets) {\n    return !PredictionMode.hasNonConflictingAltSet(altsets);\n  },\n\n  /**\n   * Determines if any single alternative subset in {@code altsets} contains\n   * exactly one alternative.\n   *\n   * @param altsets a collection of alternative subsets\n   * @return {@code true} if {@code altsets} contains a {@link BitSet} with\n   * {@link BitSet//cardinality cardinality} 1, otherwise {@code false}\n   */\n  hasNonConflictingAltSet: function (altsets) {\n    for (let i = 0; i < altsets.length; i++) {\n      const alts = altsets[i];\n\n      if (alts.length === 1) {\n        return true;\n      }\n    }\n\n    return false;\n  },\n\n  /**\n   * Determines if any single alternative subset in {@code altsets} contains\n   * more than one alternative.\n   *\n   * @param altsets a collection of alternative subsets\n   * @return {@code true} if {@code altsets} contains a {@link BitSet} with\n   * {@link BitSet//cardinality cardinality} &gt; 1, otherwise {@code false}\n   */\n  hasConflictingAltSet: function (altsets) {\n    for (let i = 0; i < altsets.length; i++) {\n      const alts = altsets[i];\n\n      if (alts.length > 1) {\n        return true;\n      }\n    }\n\n    return false;\n  },\n\n  /**\n   * Determines if every alternative subset in {@code altsets} is equivalent.\n   *\n   * @param altsets a collection of alternative subsets\n   * @return {@code true} if every member of {@code altsets} is equal to the\n   * others, otherwise {@code false}\n   */\n  allSubsetsEqual: function (altsets) {\n    let first = null;\n\n    for (let i = 0; i < altsets.length; i++) {\n      const alts = altsets[i];\n\n      if (first === null) {\n        first = alts;\n      } else if (alts !== first) {\n        return false;\n      }\n    }\n\n    return true;\n  },\n\n  /**\n   * Returns the unique alternative predicted by all alternative subsets in\n   * {@code altsets}. If no such alternative exists, this method returns\n   * {@link ATN//INVALID_ALT_NUMBER}.\n   *\n   * @param altsets a collection of alternative subsets\n   */\n  getUniqueAlt: function (altsets) {\n    const all = PredictionMode.getAlts(altsets);\n\n    if (all.length === 1) {\n      return all.minValue();\n    } else {\n      return ATN.INVALID_ALT_NUMBER;\n    }\n  },\n\n  /**\n   * Gets the complete set of represented alternatives for a collection of\n   * alternative subsets. This method returns the union of each {@link BitSet}\n   * in {@code altsets}.\n   *\n   * @param altsets a collection of alternative subsets\n   * @return the set of represented alternatives in {@code altsets}\n   */\n  getAlts: function (altsets) {\n    const all = new BitSet();\n    altsets.map(function (alts) {\n      all.or(alts);\n    });\n    return all;\n  },\n\n  /**\n   * This function gets the conflicting alt subsets from a configuration set.\n   * For each configuration {@code c} in {@code configs}:\n   *\n   * <pre>\n   * map[c] U= c.{@link ATNConfig//alt alt} // map hash/equals uses s and x, not\n   * alt and not pred\n   * </pre>\n   */\n  getConflictingAltSubsets: function (configs) {\n    const configToAlts = new Map();\n\n    configToAlts.hashFunction = function (cfg) {\n      hashStuff(cfg.state.stateNumber, cfg.context);\n    };\n\n    configToAlts.equalsFunction = function (c1, c2) {\n      return c1.state.stateNumber === c2.state.stateNumber && c1.context.equals(c2.context);\n    };\n\n    configs.items.map(function (cfg) {\n      let alts = configToAlts.get(cfg);\n\n      if (alts === null) {\n        alts = new BitSet();\n        configToAlts.put(cfg, alts);\n      }\n\n      alts.add(cfg.alt);\n    });\n    return configToAlts.getValues();\n  },\n\n  /**\n   * Get a map from state to alt subset from a configuration set. For each\n   * configuration {@code c} in {@code configs}:\n   *\n   * <pre>\n   * map[c.{@link ATNConfig//state state}] U= c.{@link ATNConfig//alt alt}\n   * </pre>\n   */\n  getStateToAltMap: function (configs) {\n    const m = new AltDict();\n    configs.items.map(function (c) {\n      let alts = m.get(c.state);\n\n      if (alts === null) {\n        alts = new BitSet();\n        m.put(c.state, alts);\n      }\n\n      alts.add(c.alt);\n    });\n    return m;\n  },\n  hasStateAssociatedWithOneAlt: function (configs) {\n    const values = PredictionMode.getStateToAltMap(configs).values();\n\n    for (let i = 0; i < values.length; i++) {\n      if (values[i].length === 1) {\n        return true;\n      }\n    }\n\n    return false;\n  },\n  getSingleViableAlt: function (altsets) {\n    let result = null;\n\n    for (let i = 0; i < altsets.length; i++) {\n      const alts = altsets[i];\n      const minAlt = alts.minValue();\n\n      if (result === null) {\n        result = minAlt;\n      } else if (result !== minAlt) {\n        // more than 1 viable alt\n        return ATN.INVALID_ALT_NUMBER;\n      }\n    }\n\n    return result;\n  }\n};\nmodule.exports = PredictionMode;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/PredictionMode.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/SemanticContext.js":
/*!***************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/SemanticContext.js ***!
  \***************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Set,\n  Hash,\n  equalArrays\n} = __webpack_require__(/*! ./../Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n/**\n * A tree structure used to record the semantic context in which\n * an ATN configuration is valid.  It's either a single predicate,\n * a conjunction {@code p1&&p2}, or a sum of products {@code p1||p2}.\n *\n * <p>I have scoped the {@link AND}, {@link OR}, and {@link Predicate} subclasses of\n * {@link SemanticContext} within the scope of this outer class.</p>\n */\n\n\nclass SemanticContext {\n  hashCode() {\n    const hash = new Hash();\n    this.updateHashCode(hash);\n    return hash.finish();\n  }\n  /**\n   * For context independent predicates, we evaluate them without a local\n   * context (i.e., null context). That way, we can evaluate them without\n   * having to create proper rule-specific context during prediction (as\n   * opposed to the parser, which creates them naturally). In a practical\n   * sense, this avoids a cast exception from RuleContext to myruleContext.\n   *\n   * <p>For context dependent predicates, we must pass in a local context so that\n   * references such as $arg evaluate properly as _localctx.arg. We only\n   * capture context dependent predicates in the context in which we begin\n   * prediction, so we passed in the outer context here in case of context\n   * dependent predicate evaluation.</p>\n   */\n\n\n  evaluate(parser, outerContext) {}\n  /**\n   * Evaluate the precedence predicates for the context and reduce the result.\n   *\n   * @param parser The parser instance.\n   * @param outerContext The current parser context object.\n   * @return The simplified semantic context after precedence predicates are\n   * evaluated, which will be one of the following values.\n   * <ul>\n   * <li>{@link //NONE}: if the predicate simplifies to {@code true} after\n   * precedence predicates are evaluated.</li>\n   * <li>{@code null}: if the predicate simplifies to {@code false} after\n   * precedence predicates are evaluated.</li>\n   * <li>{@code this}: if the semantic context is not changed as a result of\n   * precedence predicate evaluation.</li>\n   * <li>A non-{@code null} {@link SemanticContext}: the new simplified\n   * semantic context after precedence predicates are evaluated.</li>\n   * </ul>\n   */\n\n\n  evalPrecedence(parser, outerContext) {\n    return this;\n  }\n\n  static andContext(a, b) {\n    if (a === null || a === SemanticContext.NONE) {\n      return b;\n    }\n\n    if (b === null || b === SemanticContext.NONE) {\n      return a;\n    }\n\n    const result = new AND(a, b);\n\n    if (result.opnds.length === 1) {\n      return result.opnds[0];\n    } else {\n      return result;\n    }\n  }\n\n  static orContext(a, b) {\n    if (a === null) {\n      return b;\n    }\n\n    if (b === null) {\n      return a;\n    }\n\n    if (a === SemanticContext.NONE || b === SemanticContext.NONE) {\n      return SemanticContext.NONE;\n    }\n\n    const result = new OR(a, b);\n\n    if (result.opnds.length === 1) {\n      return result.opnds[0];\n    } else {\n      return result;\n    }\n  }\n\n}\n\nclass Predicate extends SemanticContext {\n  constructor(ruleIndex, predIndex, isCtxDependent) {\n    super();\n    this.ruleIndex = ruleIndex === undefined ? -1 : ruleIndex;\n    this.predIndex = predIndex === undefined ? -1 : predIndex;\n    this.isCtxDependent = isCtxDependent === undefined ? false : isCtxDependent; // e.g., $i ref in pred\n  }\n\n  evaluate(parser, outerContext) {\n    const localctx = this.isCtxDependent ? outerContext : null;\n    return parser.sempred(localctx, this.ruleIndex, this.predIndex);\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.ruleIndex, this.predIndex, this.isCtxDependent);\n  }\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof Predicate)) {\n      return false;\n    } else {\n      return this.ruleIndex === other.ruleIndex && this.predIndex === other.predIndex && this.isCtxDependent === other.isCtxDependent;\n    }\n  }\n\n  toString() {\n    return \"{\" + this.ruleIndex + \":\" + this.predIndex + \"}?\";\n  }\n\n}\n/**\n * The default {@link SemanticContext}, which is semantically equivalent to\n * a predicate of the form {@code {true}?}\n */\n\n\nSemanticContext.NONE = new Predicate();\n\nclass PrecedencePredicate extends SemanticContext {\n  constructor(precedence) {\n    super();\n    this.precedence = precedence === undefined ? 0 : precedence;\n  }\n\n  evaluate(parser, outerContext) {\n    return parser.precpred(outerContext, this.precedence);\n  }\n\n  evalPrecedence(parser, outerContext) {\n    if (parser.precpred(outerContext, this.precedence)) {\n      return SemanticContext.NONE;\n    } else {\n      return null;\n    }\n  }\n\n  compareTo(other) {\n    return this.precedence - other.precedence;\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.precedence);\n  }\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof PrecedencePredicate)) {\n      return false;\n    } else {\n      return this.precedence === other.precedence;\n    }\n  }\n\n  toString() {\n    return \"{\" + this.precedence + \">=prec}?\";\n  }\n\n  static filterPrecedencePredicates(set) {\n    const result = [];\n    set.values().map(function (context) {\n      if (context instanceof PrecedencePredicate) {\n        result.push(context);\n      }\n    });\n    return result;\n  }\n\n}\n\nclass AND extends SemanticContext {\n  /**\n   * A semantic context which is true whenever none of the contained contexts\n   * is false\n   */\n  constructor(a, b) {\n    super();\n    const operands = new Set();\n\n    if (a instanceof AND) {\n      a.opnds.map(function (o) {\n        operands.add(o);\n      });\n    } else {\n      operands.add(a);\n    }\n\n    if (b instanceof AND) {\n      b.opnds.map(function (o) {\n        operands.add(o);\n      });\n    } else {\n      operands.add(b);\n    }\n\n    const precedencePredicates = PrecedencePredicate.filterPrecedencePredicates(operands);\n\n    if (precedencePredicates.length > 0) {\n      // interested in the transition with the lowest precedence\n      let reduced = null;\n      precedencePredicates.map(function (p) {\n        if (reduced === null || p.precedence < reduced.precedence) {\n          reduced = p;\n        }\n      });\n      operands.add(reduced);\n    }\n\n    this.opnds = Array.from(operands.values());\n  }\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof AND)) {\n      return false;\n    } else {\n      return equalArrays(this.opnds, other.opnds);\n    }\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.opnds, \"AND\");\n  }\n  /**\n   * {@inheritDoc}\n   *\n   * <p>\n   * The evaluation of predicates by this context is short-circuiting, but\n   * unordered.</p>\n   */\n\n\n  evaluate(parser, outerContext) {\n    for (let i = 0; i < this.opnds.length; i++) {\n      if (!this.opnds[i].evaluate(parser, outerContext)) {\n        return false;\n      }\n    }\n\n    return true;\n  }\n\n  evalPrecedence(parser, outerContext) {\n    let differs = false;\n    const operands = [];\n\n    for (let i = 0; i < this.opnds.length; i++) {\n      const context = this.opnds[i];\n      const evaluated = context.evalPrecedence(parser, outerContext);\n      differs |= evaluated !== context;\n\n      if (evaluated === null) {\n        // The AND context is false if any element is false\n        return null;\n      } else if (evaluated !== SemanticContext.NONE) {\n        // Reduce the result by skipping true elements\n        operands.push(evaluated);\n      }\n    }\n\n    if (!differs) {\n      return this;\n    }\n\n    if (operands.length === 0) {\n      // all elements were true, so the AND context is true\n      return SemanticContext.NONE;\n    }\n\n    let result = null;\n    operands.map(function (o) {\n      result = result === null ? o : SemanticContext.andContext(result, o);\n    });\n    return result;\n  }\n\n  toString() {\n    const s = this.opnds.map(o => o.toString());\n    return (s.length > 3 ? s.slice(3) : s).join(\"&&\");\n  }\n\n}\n\nclass OR extends SemanticContext {\n  /**\n   * A semantic context which is true whenever at least one of the contained\n   * contexts is true\n   */\n  constructor(a, b) {\n    super();\n    const operands = new Set();\n\n    if (a instanceof OR) {\n      a.opnds.map(function (o) {\n        operands.add(o);\n      });\n    } else {\n      operands.add(a);\n    }\n\n    if (b instanceof OR) {\n      b.opnds.map(function (o) {\n        operands.add(o);\n      });\n    } else {\n      operands.add(b);\n    }\n\n    const precedencePredicates = PrecedencePredicate.filterPrecedencePredicates(operands);\n\n    if (precedencePredicates.length > 0) {\n      // interested in the transition with the highest precedence\n      const s = precedencePredicates.sort(function (a, b) {\n        return a.compareTo(b);\n      });\n      const reduced = s[s.length - 1];\n      operands.add(reduced);\n    }\n\n    this.opnds = Array.from(operands.values());\n  }\n\n  equals(other) {\n    if (this === other) {\n      return true;\n    } else if (!(other instanceof OR)) {\n      return false;\n    } else {\n      return equalArrays(this.opnds, other.opnds);\n    }\n  }\n\n  updateHashCode(hash) {\n    hash.update(this.opnds, \"OR\");\n  }\n  /**\n   * <p>\n   * The evaluation of predicates by this context is short-circuiting, but\n   * unordered.</p>\n   */\n\n\n  evaluate(parser, outerContext) {\n    for (let i = 0; i < this.opnds.length; i++) {\n      if (this.opnds[i].evaluate(parser, outerContext)) {\n        return true;\n      }\n    }\n\n    return false;\n  }\n\n  evalPrecedence(parser, outerContext) {\n    let differs = false;\n    const operands = [];\n\n    for (let i = 0; i < this.opnds.length; i++) {\n      const context = this.opnds[i];\n      const evaluated = context.evalPrecedence(parser, outerContext);\n      differs |= evaluated !== context;\n\n      if (evaluated === SemanticContext.NONE) {\n        // The OR context is true if any element is true\n        return SemanticContext.NONE;\n      } else if (evaluated !== null) {\n        // Reduce the result by skipping false elements\n        operands.push(evaluated);\n      }\n    }\n\n    if (!differs) {\n      return this;\n    }\n\n    if (operands.length === 0) {\n      // all elements were false, so the OR context is false\n      return null;\n    }\n\n    const result = null;\n    operands.map(function (o) {\n      return result === null ? o : SemanticContext.orContext(result, o);\n    });\n    return result;\n  }\n\n  toString() {\n    const s = this.opnds.map(o => o.toString());\n    return (s.length > 3 ? s.slice(3) : s).join(\"||\");\n  }\n\n}\n\nmodule.exports = {\n  SemanticContext,\n  PrecedencePredicate,\n  Predicate\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/SemanticContext.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/Transition.js":
/*!**********************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/Transition.js ***!
  \**********************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Token\n} = __webpack_require__(/*! ./../Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\nconst {\n  IntervalSet\n} = __webpack_require__(/*! ./../IntervalSet */ \"./node_modules/antlr4/src/antlr4/IntervalSet.js\");\n\nconst {\n  Predicate,\n  PrecedencePredicate\n} = __webpack_require__(/*! ./SemanticContext */ \"./node_modules/antlr4/src/antlr4/atn/SemanticContext.js\");\n/**\n * An ATN transition between any two ATN states.  Subclasses define\n * atom, set, epsilon, action, predicate, rule transitions.\n *\n * <p>This is a one way link.  It emanates from a state (usually via a list of\n * transitions) and has a target state.</p>\n *\n * <p>Since we never have to change the ATN transitions once we construct it,\n * we can fix these transitions as specific classes. The DFA transitions\n * on the other hand need to update the labels as it adds transitions to\n * the states. We'll use the term Edge for the DFA to distinguish them from\n * ATN transitions.</p>\n */\n\n\nclass Transition {\n  constructor(target) {\n    // The target of this transition.\n    if (target === undefined || target === null) {\n      throw \"target cannot be null.\";\n    }\n\n    this.target = target; // Are we epsilon, action, sempred?\n\n    this.isEpsilon = false;\n    this.label = null;\n  }\n\n} // constants for serialization\n\n\nTransition.EPSILON = 1;\nTransition.RANGE = 2;\nTransition.RULE = 3; // e.g., {isType(input.LT(1))}?\n\nTransition.PREDICATE = 4;\nTransition.ATOM = 5;\nTransition.ACTION = 6; // ~(A|B) or ~atom, wildcard, which convert to next 2\n\nTransition.SET = 7;\nTransition.NOT_SET = 8;\nTransition.WILDCARD = 9;\nTransition.PRECEDENCE = 10;\nTransition.serializationNames = [\"INVALID\", \"EPSILON\", \"RANGE\", \"RULE\", \"PREDICATE\", \"ATOM\", \"ACTION\", \"SET\", \"NOT_SET\", \"WILDCARD\", \"PRECEDENCE\"];\nTransition.serializationTypes = {\n  EpsilonTransition: Transition.EPSILON,\n  RangeTransition: Transition.RANGE,\n  RuleTransition: Transition.RULE,\n  PredicateTransition: Transition.PREDICATE,\n  AtomTransition: Transition.ATOM,\n  ActionTransition: Transition.ACTION,\n  SetTransition: Transition.SET,\n  NotSetTransition: Transition.NOT_SET,\n  WildcardTransition: Transition.WILDCARD,\n  PrecedencePredicateTransition: Transition.PRECEDENCE\n}; // TODO: make all transitions sets? no, should remove set edges\n\nclass AtomTransition extends Transition {\n  constructor(target, label) {\n    super(target); // The token type or character value; or, signifies special label.\n\n    this.label_ = label;\n    this.label = this.makeLabel();\n    this.serializationType = Transition.ATOM;\n  }\n\n  makeLabel() {\n    const s = new IntervalSet();\n    s.addOne(this.label_);\n    return s;\n  }\n\n  matches(symbol, minVocabSymbol, maxVocabSymbol) {\n    return this.label_ === symbol;\n  }\n\n  toString() {\n    return this.label_;\n  }\n\n}\n\nclass RuleTransition extends Transition {\n  constructor(ruleStart, ruleIndex, precedence, followState) {\n    super(ruleStart); // ptr to the rule definition object for this rule ref\n\n    this.ruleIndex = ruleIndex;\n    this.precedence = precedence; // what node to begin computations following ref to rule\n\n    this.followState = followState;\n    this.serializationType = Transition.RULE;\n    this.isEpsilon = true;\n  }\n\n  matches(symbol, minVocabSymbol, maxVocabSymbol) {\n    return false;\n  }\n\n}\n\nclass EpsilonTransition extends Transition {\n  constructor(target, outermostPrecedenceReturn) {\n    super(target);\n    this.serializationType = Transition.EPSILON;\n    this.isEpsilon = true;\n    this.outermostPrecedenceReturn = outermostPrecedenceReturn;\n  }\n\n  matches(symbol, minVocabSymbol, maxVocabSymbol) {\n    return false;\n  }\n\n  toString() {\n    return \"epsilon\";\n  }\n\n}\n\nclass RangeTransition extends Transition {\n  constructor(target, start, stop) {\n    super(target);\n    this.serializationType = Transition.RANGE;\n    this.start = start;\n    this.stop = stop;\n    this.label = this.makeLabel();\n  }\n\n  makeLabel() {\n    const s = new IntervalSet();\n    s.addRange(this.start, this.stop);\n    return s;\n  }\n\n  matches(symbol, minVocabSymbol, maxVocabSymbol) {\n    return symbol >= this.start && symbol <= this.stop;\n  }\n\n  toString() {\n    return \"'\" + String.fromCharCode(this.start) + \"'..'\" + String.fromCharCode(this.stop) + \"'\";\n  }\n\n}\n\nclass AbstractPredicateTransition extends Transition {\n  constructor(target) {\n    super(target);\n  }\n\n}\n\nclass PredicateTransition extends AbstractPredicateTransition {\n  constructor(target, ruleIndex, predIndex, isCtxDependent) {\n    super(target);\n    this.serializationType = Transition.PREDICATE;\n    this.ruleIndex = ruleIndex;\n    this.predIndex = predIndex;\n    this.isCtxDependent = isCtxDependent; // e.g., $i ref in pred\n\n    this.isEpsilon = true;\n  }\n\n  matches(symbol, minVocabSymbol, maxVocabSymbol) {\n    return false;\n  }\n\n  getPredicate() {\n    return new Predicate(this.ruleIndex, this.predIndex, this.isCtxDependent);\n  }\n\n  toString() {\n    return \"pred_\" + this.ruleIndex + \":\" + this.predIndex;\n  }\n\n}\n\nclass ActionTransition extends Transition {\n  constructor(target, ruleIndex, actionIndex, isCtxDependent) {\n    super(target);\n    this.serializationType = Transition.ACTION;\n    this.ruleIndex = ruleIndex;\n    this.actionIndex = actionIndex === undefined ? -1 : actionIndex;\n    this.isCtxDependent = isCtxDependent === undefined ? false : isCtxDependent; // e.g., $i ref in pred\n\n    this.isEpsilon = true;\n  }\n\n  matches(symbol, minVocabSymbol, maxVocabSymbol) {\n    return false;\n  }\n\n  toString() {\n    return \"action_\" + this.ruleIndex + \":\" + this.actionIndex;\n  }\n\n} // A transition containing a set of values.\n\n\nclass SetTransition extends Transition {\n  constructor(target, set) {\n    super(target);\n    this.serializationType = Transition.SET;\n\n    if (set !== undefined && set !== null) {\n      this.label = set;\n    } else {\n      this.label = new IntervalSet();\n      this.label.addOne(Token.INVALID_TYPE);\n    }\n  }\n\n  matches(symbol, minVocabSymbol, maxVocabSymbol) {\n    return this.label.contains(symbol);\n  }\n\n  toString() {\n    return this.label.toString();\n  }\n\n}\n\nclass NotSetTransition extends SetTransition {\n  constructor(target, set) {\n    super(target, set);\n    this.serializationType = Transition.NOT_SET;\n  }\n\n  matches(symbol, minVocabSymbol, maxVocabSymbol) {\n    return symbol >= minVocabSymbol && symbol <= maxVocabSymbol && !super.matches(symbol, minVocabSymbol, maxVocabSymbol);\n  }\n\n  toString() {\n    return '~' + super.toString();\n  }\n\n}\n\nclass WildcardTransition extends Transition {\n  constructor(target) {\n    super(target);\n    this.serializationType = Transition.WILDCARD;\n  }\n\n  matches(symbol, minVocabSymbol, maxVocabSymbol) {\n    return symbol >= minVocabSymbol && symbol <= maxVocabSymbol;\n  }\n\n  toString() {\n    return \".\";\n  }\n\n}\n\nclass PrecedencePredicateTransition extends AbstractPredicateTransition {\n  constructor(target, precedence) {\n    super(target);\n    this.serializationType = Transition.PRECEDENCE;\n    this.precedence = precedence;\n    this.isEpsilon = true;\n  }\n\n  matches(symbol, minVocabSymbol, maxVocabSymbol) {\n    return false;\n  }\n\n  getPredicate() {\n    return new PrecedencePredicate(this.precedence);\n  }\n\n  toString() {\n    return this.precedence + \" >= _p\";\n  }\n\n}\n\nmodule.exports = {\n  Transition,\n  AtomTransition,\n  SetTransition,\n  NotSetTransition,\n  RuleTransition,\n  ActionTransition,\n  EpsilonTransition,\n  RangeTransition,\n  WildcardTransition,\n  PredicateTransition,\n  PrecedencePredicateTransition,\n  AbstractPredicateTransition\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/Transition.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/atn/index.js":
/*!*****************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/atn/index.js ***!
  \*****************************************************/
/***/ ((__unused_webpack_module, exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nexports.ATN = __webpack_require__(/*! ./ATN */ \"./node_modules/antlr4/src/antlr4/atn/ATN.js\");\nexports.ATNDeserializer = __webpack_require__(/*! ./ATNDeserializer */ \"./node_modules/antlr4/src/antlr4/atn/ATNDeserializer.js\");\nexports.LexerATNSimulator = __webpack_require__(/*! ./LexerATNSimulator */ \"./node_modules/antlr4/src/antlr4/atn/LexerATNSimulator.js\");\nexports.ParserATNSimulator = __webpack_require__(/*! ./ParserATNSimulator */ \"./node_modules/antlr4/src/antlr4/atn/ParserATNSimulator.js\");\nexports.PredictionMode = __webpack_require__(/*! ./PredictionMode */ \"./node_modules/antlr4/src/antlr4/atn/PredictionMode.js\");\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/atn/index.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/dfa/DFA.js":
/*!***************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/dfa/DFA.js ***!
  \***************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Set\n} = __webpack_require__(/*! ../Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n\nconst {\n  DFAState\n} = __webpack_require__(/*! ./DFAState */ \"./node_modules/antlr4/src/antlr4/dfa/DFAState.js\");\n\nconst {\n  StarLoopEntryState\n} = __webpack_require__(/*! ../atn/ATNState */ \"./node_modules/antlr4/src/antlr4/atn/ATNState.js\");\n\nconst {\n  ATNConfigSet\n} = __webpack_require__(/*! ./../atn/ATNConfigSet */ \"./node_modules/antlr4/src/antlr4/atn/ATNConfigSet.js\");\n\nconst {\n  DFASerializer\n} = __webpack_require__(/*! ./DFASerializer */ \"./node_modules/antlr4/src/antlr4/dfa/DFASerializer.js\");\n\nconst {\n  LexerDFASerializer\n} = __webpack_require__(/*! ./DFASerializer */ \"./node_modules/antlr4/src/antlr4/dfa/DFASerializer.js\");\n\nclass DFA {\n  constructor(atnStartState, decision) {\n    if (decision === undefined) {\n      decision = 0;\n    }\n    /**\n     * From which ATN state did we create this DFA?\n     */\n\n\n    this.atnStartState = atnStartState;\n    this.decision = decision;\n    /**\n     * A set of all DFA states. Use {@link Map} so we can get old state back\n     * ({@link Set} only allows you to see if it's there).\n     */\n\n    this._states = new Set();\n    this.s0 = null;\n    /**\n     * {@code true} if this DFA is for a precedence decision; otherwise,\n     * {@code false}. This is the backing field for {@link //isPrecedenceDfa},\n     * {@link //setPrecedenceDfa}\n     */\n\n    this.precedenceDfa = false;\n\n    if (atnStartState instanceof StarLoopEntryState) {\n      if (atnStartState.isPrecedenceDecision) {\n        this.precedenceDfa = true;\n        const precedenceState = new DFAState(null, new ATNConfigSet());\n        precedenceState.edges = [];\n        precedenceState.isAcceptState = false;\n        precedenceState.requiresFullContext = false;\n        this.s0 = precedenceState;\n      }\n    }\n  }\n  /**\n   * Get the start state for a specific precedence value.\n   *\n   * @param precedence The current precedence.\n   * @return The start state corresponding to the specified precedence, or\n   * {@code null} if no start state exists for the specified precedence.\n   *\n   * @throws IllegalStateException if this is not a precedence DFA.\n   * @see //isPrecedenceDfa()\n   */\n\n\n  getPrecedenceStartState(precedence) {\n    if (!this.precedenceDfa) {\n      throw \"Only precedence DFAs may contain a precedence start state.\";\n    } // s0.edges is never null for a precedence DFA\n\n\n    if (precedence < 0 || precedence >= this.s0.edges.length) {\n      return null;\n    }\n\n    return this.s0.edges[precedence] || null;\n  }\n  /**\n   * Set the start state for a specific precedence value.\n   *\n   * @param precedence The current precedence.\n   * @param startState The start state corresponding to the specified\n   * precedence.\n   *\n   * @throws IllegalStateException if this is not a precedence DFA.\n   * @see //isPrecedenceDfa()\n   */\n\n\n  setPrecedenceStartState(precedence, startState) {\n    if (!this.precedenceDfa) {\n      throw \"Only precedence DFAs may contain a precedence start state.\";\n    }\n\n    if (precedence < 0) {\n      return;\n    }\n    /**\n     * synchronization on s0 here is ok. when the DFA is turned into a\n     * precedence DFA, s0 will be initialized once and not updated again\n     * s0.edges is never null for a precedence DFA\n     */\n\n\n    this.s0.edges[precedence] = startState;\n  }\n  /**\n   * Sets whether this is a precedence DFA. If the specified value differs\n   * from the current DFA configuration, the following actions are taken;\n   * otherwise no changes are made to the current DFA.\n   *\n   * <ul>\n   * <li>The {@link //states} map is cleared</li>\n   * <li>If {@code precedenceDfa} is {@code false}, the initial state\n   * {@link //s0} is set to {@code null}; otherwise, it is initialized to a new\n   * {@link DFAState} with an empty outgoing {@link DFAState//edges} array to\n   * store the start states for individual precedence values.</li>\n   * <li>The {@link //precedenceDfa} field is updated</li>\n   * </ul>\n   *\n   * @param precedenceDfa {@code true} if this is a precedence DFA; otherwise,\n   * {@code false}\n   */\n\n\n  setPrecedenceDfa(precedenceDfa) {\n    if (this.precedenceDfa !== precedenceDfa) {\n      this._states = new Set();\n\n      if (precedenceDfa) {\n        const precedenceState = new DFAState(null, new ATNConfigSet());\n        precedenceState.edges = [];\n        precedenceState.isAcceptState = false;\n        precedenceState.requiresFullContext = false;\n        this.s0 = precedenceState;\n      } else {\n        this.s0 = null;\n      }\n\n      this.precedenceDfa = precedenceDfa;\n    }\n  }\n  /**\n   * Return a list of all states in this DFA, ordered by state number.\n   */\n\n\n  sortedStates() {\n    const list = this._states.values();\n\n    return list.sort(function (a, b) {\n      return a.stateNumber - b.stateNumber;\n    });\n  }\n\n  toString(literalNames, symbolicNames) {\n    literalNames = literalNames || null;\n    symbolicNames = symbolicNames || null;\n\n    if (this.s0 === null) {\n      return \"\";\n    }\n\n    const serializer = new DFASerializer(this, literalNames, symbolicNames);\n    return serializer.toString();\n  }\n\n  toLexerString() {\n    if (this.s0 === null) {\n      return \"\";\n    }\n\n    const serializer = new LexerDFASerializer(this);\n    return serializer.toString();\n  }\n\n  get states() {\n    return this._states;\n  }\n\n}\n\nmodule.exports = DFA;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/dfa/DFA.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/dfa/DFASerializer.js":
/*!*************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/dfa/DFASerializer.js ***!
  \*************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst Utils = __webpack_require__(/*! ./../Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n/**\n * A DFA walker that knows how to dump them to serialized strings.\n */\n\n\nclass DFASerializer {\n  constructor(dfa, literalNames, symbolicNames) {\n    this.dfa = dfa;\n    this.literalNames = literalNames || [];\n    this.symbolicNames = symbolicNames || [];\n  }\n\n  toString() {\n    if (this.dfa.s0 === null) {\n      return null;\n    }\n\n    let buf = \"\";\n    const states = this.dfa.sortedStates();\n\n    for (let i = 0; i < states.length; i++) {\n      const s = states[i];\n\n      if (s.edges !== null) {\n        const n = s.edges.length;\n\n        for (let j = 0; j < n; j++) {\n          const t = s.edges[j] || null;\n\n          if (t !== null && t.stateNumber !== 0x7FFFFFFF) {\n            buf = buf.concat(this.getStateString(s));\n            buf = buf.concat(\"-\");\n            buf = buf.concat(this.getEdgeLabel(j));\n            buf = buf.concat(\"->\");\n            buf = buf.concat(this.getStateString(t));\n            buf = buf.concat('\\n');\n          }\n        }\n      }\n    }\n\n    return buf.length === 0 ? null : buf;\n  }\n\n  getEdgeLabel(i) {\n    if (i === 0) {\n      return \"EOF\";\n    } else if (this.literalNames !== null || this.symbolicNames !== null) {\n      return this.literalNames[i - 1] || this.symbolicNames[i - 1];\n    } else {\n      return String.fromCharCode(i - 1);\n    }\n  }\n\n  getStateString(s) {\n    const baseStateStr = (s.isAcceptState ? \":\" : \"\") + \"s\" + s.stateNumber + (s.requiresFullContext ? \"^\" : \"\");\n\n    if (s.isAcceptState) {\n      if (s.predicates !== null) {\n        return baseStateStr + \"=>\" + Utils.arrayToString(s.predicates);\n      } else {\n        return baseStateStr + \"=>\" + s.prediction.toString();\n      }\n    } else {\n      return baseStateStr;\n    }\n  }\n\n}\n\nclass LexerDFASerializer extends DFASerializer {\n  constructor(dfa) {\n    super(dfa, null);\n  }\n\n  getEdgeLabel(i) {\n    return \"'\" + String.fromCharCode(i) + \"'\";\n  }\n\n}\n\nmodule.exports = {\n  DFASerializer,\n  LexerDFASerializer\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/dfa/DFASerializer.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/dfa/DFAState.js":
/*!********************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/dfa/DFAState.js ***!
  \********************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  ATNConfigSet\n} = __webpack_require__(/*! ./../atn/ATNConfigSet */ \"./node_modules/antlr4/src/antlr4/atn/ATNConfigSet.js\");\n\nconst {\n  Hash,\n  Set\n} = __webpack_require__(/*! ./../Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n/**\n * Map a predicate to a predicted alternative.\n */\n\n\nclass PredPrediction {\n  constructor(pred, alt) {\n    this.alt = alt;\n    this.pred = pred;\n  }\n\n  toString() {\n    return \"(\" + this.pred + \", \" + this.alt + \")\";\n  }\n\n}\n/**\n * A DFA state represents a set of possible ATN configurations.\n * As Aho, Sethi, Ullman p. 117 says \"The DFA uses its state\n * to keep track of all possible states the ATN can be in after\n * reading each input symbol. That is to say, after reading\n * input a1a2..an, the DFA is in a state that represents the\n * subset T of the states of the ATN that are reachable from the\n * ATN's start state along some path labeled a1a2..an.\"\n * In conventional NFA&rarr;DFA conversion, therefore, the subset T\n * would be a bitset representing the set of states the\n * ATN could be in. We need to track the alt predicted by each\n * state as well, however. More importantly, we need to maintain\n * a stack of states, tracking the closure operations as they\n * jump from rule to rule, emulating rule invocations (method calls).\n * I have to add a stack to simulate the proper lookahead sequences for\n * the underlying LL grammar from which the ATN was derived.\n *\n * <p>I use a set of ATNConfig objects not simple states. An ATNConfig\n * is both a state (ala normal conversion) and a RuleContext describing\n * the chain of rules (if any) followed to arrive at that state.</p>\n *\n * <p>A DFA state may have multiple references to a particular state,\n * but with different ATN contexts (with same or different alts)\n * meaning that state was reached via a different set of rule invocations.</p>\n */\n\n\nclass DFAState {\n  constructor(stateNumber, configs) {\n    if (stateNumber === null) {\n      stateNumber = -1;\n    }\n\n    if (configs === null) {\n      configs = new ATNConfigSet();\n    }\n\n    this.stateNumber = stateNumber;\n    this.configs = configs;\n    /**\n     * {@code edges[symbol]} points to target of symbol. Shift up by 1 so (-1)\n     * {@link Token//EOF} maps to {@code edges[0]}.\n     */\n\n    this.edges = null;\n    this.isAcceptState = false;\n    /**\n     * if accept state, what ttype do we match or alt do we predict?\n     * This is set to {@link ATN//INVALID_ALT_NUMBER} when {@link//predicates}\n     * {@code !=null} or {@link //requiresFullContext}.\n     */\n\n    this.prediction = 0;\n    this.lexerActionExecutor = null;\n    /**\n     * Indicates that this state was created during SLL prediction that\n     * discovered a conflict between the configurations in the state. Future\n     * {@link ParserATNSimulator//execATN} invocations immediately jumped doing\n     * full context prediction if this field is true.\n     */\n\n    this.requiresFullContext = false;\n    /**\n     * During SLL parsing, this is a list of predicates associated with the\n     * ATN configurations of the DFA state. When we have predicates,\n     * {@link //requiresFullContext} is {@code false} since full context\n     * prediction evaluates predicates\n     * on-the-fly. If this is not null, then {@link //prediction} is\n     * {@link ATN//INVALID_ALT_NUMBER}.\n     *\n     * <p>We only use these for non-{@link //requiresFullContext} but\n     * conflicting states. That\n     * means we know from the context (it's $ or we don't dip into outer\n     * context) that it's an ambiguity not a conflict.</p>\n     *\n     * <p>This list is computed by {@link\n     * ParserATNSimulator//predicateDFAState}.</p>\n     */\n\n    this.predicates = null;\n    return this;\n  }\n  /**\n   * Get the set of all alts mentioned by all ATN configurations in this\n   * DFA state.\n   */\n\n\n  getAltSet() {\n    const alts = new Set();\n\n    if (this.configs !== null) {\n      for (let i = 0; i < this.configs.length; i++) {\n        const c = this.configs[i];\n        alts.add(c.alt);\n      }\n    }\n\n    if (alts.length === 0) {\n      return null;\n    } else {\n      return alts;\n    }\n  }\n  /**\n   * Two {@link DFAState} instances are equal if their ATN configuration sets\n   * are the same. This method is used to see if a state already exists.\n   *\n   * <p>Because the number of alternatives and number of ATN configurations are\n   * finite, there is a finite number of DFA states that can be processed.\n   * This is necessary to show that the algorithm terminates.</p>\n   *\n   * <p>Cannot test the DFA state numbers here because in\n   * {@link ParserATNSimulator//addDFAState} we need to know if any other state\n   * exists that has this exact set of ATN configurations. The\n   * {@link //stateNumber} is irrelevant.</p>\n   */\n\n\n  equals(other) {\n    // compare set of ATN configurations in this set with other\n    return this === other || other instanceof DFAState && this.configs.equals(other.configs);\n  }\n\n  toString() {\n    let s = \"\" + this.stateNumber + \":\" + this.configs;\n\n    if (this.isAcceptState) {\n      s = s + \"=>\";\n      if (this.predicates !== null) s = s + this.predicates;else s = s + this.prediction;\n    }\n\n    return s;\n  }\n\n  hashCode() {\n    const hash = new Hash();\n    hash.update(this.configs);\n    return hash.finish();\n  }\n\n}\n\nmodule.exports = {\n  DFAState,\n  PredPrediction\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/dfa/DFAState.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/dfa/index.js":
/*!*****************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/dfa/index.js ***!
  \*****************************************************/
/***/ ((__unused_webpack_module, exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nexports.DFA = __webpack_require__(/*! ./DFA */ \"./node_modules/antlr4/src/antlr4/dfa/DFA.js\");\nexports.DFASerializer = __webpack_require__(/*! ./DFASerializer */ \"./node_modules/antlr4/src/antlr4/dfa/DFASerializer.js\").DFASerializer;\nexports.LexerDFASerializer = __webpack_require__(/*! ./DFASerializer */ \"./node_modules/antlr4/src/antlr4/dfa/DFASerializer.js\").LexerDFASerializer;\nexports.PredPrediction = __webpack_require__(/*! ./DFAState */ \"./node_modules/antlr4/src/antlr4/dfa/DFAState.js\").PredPrediction;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/dfa/index.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/error/DiagnosticErrorListener.js":
/*!*************************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/error/DiagnosticErrorListener.js ***!
  \*************************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  BitSet\n} = __webpack_require__(/*! ./../Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n\nconst {\n  ErrorListener\n} = __webpack_require__(/*! ./ErrorListener */ \"./node_modules/antlr4/src/antlr4/error/ErrorListener.js\");\n\nconst {\n  Interval\n} = __webpack_require__(/*! ./../IntervalSet */ \"./node_modules/antlr4/src/antlr4/IntervalSet.js\");\n/**\n * This implementation of {@link ANTLRErrorListener} can be used to identify\n *  certain potential correctness and performance problems in grammars. \"Reports\"\n *  are made by calling {@link Parser//notifyErrorListeners} with the appropriate\n *  message.\n *\n *  <ul>\n *  <li><b>Ambiguities</b>: These are cases where more than one path through the\n *  grammar can match the input.</li>\n *  <li><b>Weak context sensitivity</b>: These are cases where full-context\n *  prediction resolved an SLL conflict to a unique alternative which equaled the\n *  minimum alternative of the SLL conflict.</li>\n *  <li><b>Strong (forced) context sensitivity</b>: These are cases where the\n *  full-context prediction resolved an SLL conflict to a unique alternative,\n *  <em>and</em> the minimum alternative of the SLL conflict was found to not be\n *  a truly viable alternative. Two-stage parsing cannot be used for inputs where\n *  this situation occurs.</li>\n *  </ul>\n */\n\n\nclass DiagnosticErrorListener extends ErrorListener {\n  constructor(exactOnly) {\n    super();\n    exactOnly = exactOnly || true; // whether all ambiguities or only exact ambiguities are reported.\n\n    this.exactOnly = exactOnly;\n  }\n\n  reportAmbiguity(recognizer, dfa, startIndex, stopIndex, exact, ambigAlts, configs) {\n    if (this.exactOnly && !exact) {\n      return;\n    }\n\n    const msg = \"reportAmbiguity d=\" + this.getDecisionDescription(recognizer, dfa) + \": ambigAlts=\" + this.getConflictingAlts(ambigAlts, configs) + \", input='\" + recognizer.getTokenStream().getText(new Interval(startIndex, stopIndex)) + \"'\";\n    recognizer.notifyErrorListeners(msg);\n  }\n\n  reportAttemptingFullContext(recognizer, dfa, startIndex, stopIndex, conflictingAlts, configs) {\n    const msg = \"reportAttemptingFullContext d=\" + this.getDecisionDescription(recognizer, dfa) + \", input='\" + recognizer.getTokenStream().getText(new Interval(startIndex, stopIndex)) + \"'\";\n    recognizer.notifyErrorListeners(msg);\n  }\n\n  reportContextSensitivity(recognizer, dfa, startIndex, stopIndex, prediction, configs) {\n    const msg = \"reportContextSensitivity d=\" + this.getDecisionDescription(recognizer, dfa) + \", input='\" + recognizer.getTokenStream().getText(new Interval(startIndex, stopIndex)) + \"'\";\n    recognizer.notifyErrorListeners(msg);\n  }\n\n  getDecisionDescription(recognizer, dfa) {\n    const decision = dfa.decision;\n    const ruleIndex = dfa.atnStartState.ruleIndex;\n    const ruleNames = recognizer.ruleNames;\n\n    if (ruleIndex < 0 || ruleIndex >= ruleNames.length) {\n      return \"\" + decision;\n    }\n\n    const ruleName = ruleNames[ruleIndex] || null;\n\n    if (ruleName === null || ruleName.length === 0) {\n      return \"\" + decision;\n    }\n\n    return `${decision} (${ruleName})`;\n  }\n  /**\n   * Computes the set of conflicting or ambiguous alternatives from a\n   * configuration set, if that information was not already provided by the\n   * parser.\n   *\n   * @param reportedAlts The set of conflicting or ambiguous alternatives, as\n   * reported by the parser.\n   * @param configs The conflicting or ambiguous configuration set.\n   * @return Returns {@code reportedAlts} if it is not {@code null}, otherwise\n   * returns the set of alternatives represented in {@code configs}.\n      */\n\n\n  getConflictingAlts(reportedAlts, configs) {\n    if (reportedAlts !== null) {\n      return reportedAlts;\n    }\n\n    const result = new BitSet();\n\n    for (let i = 0; i < configs.items.length; i++) {\n      result.add(configs.items[i].alt);\n    }\n\n    return `{${result.values().join(\", \")}}`;\n  }\n\n}\n\nmodule.exports = DiagnosticErrorListener;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/error/DiagnosticErrorListener.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/error/ErrorListener.js":
/*!***************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/error/ErrorListener.js ***!
  \***************************************************************/
/***/ ((module) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\n\n/**\n * Provides an empty default implementation of {@link ANTLRErrorListener}. The\n * default implementation of each method does nothing, but can be overridden as\n * necessary.\n */\nclass ErrorListener {\n  syntaxError(recognizer, offendingSymbol, line, column, msg, e) {}\n\n  reportAmbiguity(recognizer, dfa, startIndex, stopIndex, exact, ambigAlts, configs) {}\n\n  reportAttemptingFullContext(recognizer, dfa, startIndex, stopIndex, conflictingAlts, configs) {}\n\n  reportContextSensitivity(recognizer, dfa, startIndex, stopIndex, prediction, configs) {}\n\n}\n/**\n * {@inheritDoc}\n *\n * <p>\n * This implementation prints messages to {@link System//err} containing the\n * values of {@code line}, {@code charPositionInLine}, and {@code msg} using\n * the following format.</p>\n *\n * <pre>\n * line <em>line</em>:<em>charPositionInLine</em> <em>msg</em>\n * </pre>\n *\n */\n\n\nclass ConsoleErrorListener extends ErrorListener {\n  constructor() {\n    super();\n  }\n\n  syntaxError(recognizer, offendingSymbol, line, column, msg, e) {\n    console.error(\"line \" + line + \":\" + column + \" \" + msg);\n  }\n\n}\n/**\n * Provides a default instance of {@link ConsoleErrorListener}.\n */\n\n\nConsoleErrorListener.INSTANCE = new ConsoleErrorListener();\n\nclass ProxyErrorListener extends ErrorListener {\n  constructor(delegates) {\n    super();\n\n    if (delegates === null) {\n      throw \"delegates\";\n    }\n\n    this.delegates = delegates;\n    return this;\n  }\n\n  syntaxError(recognizer, offendingSymbol, line, column, msg, e) {\n    this.delegates.map(d => d.syntaxError(recognizer, offendingSymbol, line, column, msg, e));\n  }\n\n  reportAmbiguity(recognizer, dfa, startIndex, stopIndex, exact, ambigAlts, configs) {\n    this.delegates.map(d => d.reportAmbiguity(recognizer, dfa, startIndex, stopIndex, exact, ambigAlts, configs));\n  }\n\n  reportAttemptingFullContext(recognizer, dfa, startIndex, stopIndex, conflictingAlts, configs) {\n    this.delegates.map(d => d.reportAttemptingFullContext(recognizer, dfa, startIndex, stopIndex, conflictingAlts, configs));\n  }\n\n  reportContextSensitivity(recognizer, dfa, startIndex, stopIndex, prediction, configs) {\n    this.delegates.map(d => d.reportContextSensitivity(recognizer, dfa, startIndex, stopIndex, prediction, configs));\n  }\n\n}\n\nmodule.exports = {\n  ErrorListener,\n  ConsoleErrorListener,\n  ProxyErrorListener\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/error/ErrorListener.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/error/ErrorStrategy.js":
/*!***************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/error/ErrorStrategy.js ***!
  \***************************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Token\n} = __webpack_require__(/*! ./../Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\nconst {\n  NoViableAltException,\n  InputMismatchException,\n  FailedPredicateException,\n  ParseCancellationException\n} = __webpack_require__(/*! ./Errors */ \"./node_modules/antlr4/src/antlr4/error/Errors.js\");\n\nconst {\n  ATNState\n} = __webpack_require__(/*! ./../atn/ATNState */ \"./node_modules/antlr4/src/antlr4/atn/ATNState.js\");\n\nconst {\n  Interval,\n  IntervalSet\n} = __webpack_require__(/*! ./../IntervalSet */ \"./node_modules/antlr4/src/antlr4/IntervalSet.js\");\n\nclass ErrorStrategy {\n  reset(recognizer) {}\n\n  recoverInline(recognizer) {}\n\n  recover(recognizer, e) {}\n\n  sync(recognizer) {}\n\n  inErrorRecoveryMode(recognizer) {}\n\n  reportError(recognizer) {}\n\n}\n/**\n * This is the default implementation of {@link ANTLRErrorStrategy} used for\n * error reporting and recovery in ANTLR parsers.\n*/\n\n\nclass DefaultErrorStrategy extends ErrorStrategy {\n  constructor() {\n    super();\n    /**\n     * Indicates whether the error strategy is currently \"recovering from an\n     * error\". This is used to suppress reporting multiple error messages while\n     * attempting to recover from a detected syntax error.\n     *\n     * @see //inErrorRecoveryMode\n     */\n\n    this.errorRecoveryMode = false;\n    /**\n     * The index into the input stream where the last error occurred.\n     * This is used to prevent infinite loops where an error is found\n     * but no token is consumed during recovery...another error is found,\n     * ad nauseum. This is a failsafe mechanism to guarantee that at least\n     * one token/tree node is consumed for two errors.\n     */\n\n    this.lastErrorIndex = -1;\n    this.lastErrorStates = null;\n    this.nextTokensContext = null;\n    this.nextTokenState = 0;\n  }\n  /**\n   * <p>The default implementation simply calls {@link //endErrorCondition} to\n   * ensure that the handler is not in error recovery mode.</p>\n  */\n\n\n  reset(recognizer) {\n    this.endErrorCondition(recognizer);\n  }\n  /**\n   * This method is called to enter error recovery mode when a recognition\n   * exception is reported.\n   *\n   * @param recognizer the parser instance\n  */\n\n\n  beginErrorCondition(recognizer) {\n    this.errorRecoveryMode = true;\n  }\n\n  inErrorRecoveryMode(recognizer) {\n    return this.errorRecoveryMode;\n  }\n  /**\n   * This method is called to leave error recovery mode after recovering from\n   * a recognition exception.\n   * @param recognizer\n   */\n\n\n  endErrorCondition(recognizer) {\n    this.errorRecoveryMode = false;\n    this.lastErrorStates = null;\n    this.lastErrorIndex = -1;\n  }\n  /**\n   * {@inheritDoc}\n   * <p>The default implementation simply calls {@link //endErrorCondition}.</p>\n   */\n\n\n  reportMatch(recognizer) {\n    this.endErrorCondition(recognizer);\n  }\n  /**\n   * {@inheritDoc}\n   *\n   * <p>The default implementation returns immediately if the handler is already\n   * in error recovery mode. Otherwise, it calls {@link //beginErrorCondition}\n   * and dispatches the reporting task based on the runtime type of {@code e}\n   * according to the following table.</p>\n   *\n   * <ul>\n   * <li>{@link NoViableAltException}: Dispatches the call to\n   * {@link //reportNoViableAlternative}</li>\n   * <li>{@link InputMismatchException}: Dispatches the call to\n   * {@link //reportInputMismatch}</li>\n   * <li>{@link FailedPredicateException}: Dispatches the call to\n   * {@link //reportFailedPredicate}</li>\n   * <li>All other types: calls {@link Parser//notifyErrorListeners} to report\n   * the exception</li>\n   * </ul>\n   */\n\n\n  reportError(recognizer, e) {\n    // if we've already reported an error and have not matched a token\n    // yet successfully, don't report any errors.\n    if (this.inErrorRecoveryMode(recognizer)) {\n      return; // don't report spurious errors\n    }\n\n    this.beginErrorCondition(recognizer);\n\n    if (e instanceof NoViableAltException) {\n      this.reportNoViableAlternative(recognizer, e);\n    } else if (e instanceof InputMismatchException) {\n      this.reportInputMismatch(recognizer, e);\n    } else if (e instanceof FailedPredicateException) {\n      this.reportFailedPredicate(recognizer, e);\n    } else {\n      console.log(\"unknown recognition error type: \" + e.constructor.name);\n      console.log(e.stack);\n      recognizer.notifyErrorListeners(e.getOffendingToken(), e.getMessage(), e);\n    }\n  }\n  /**\n   *\n   * {@inheritDoc}\n   *\n   * <p>The default implementation resynchronizes the parser by consuming tokens\n   * until we find one in the resynchronization set--loosely the set of tokens\n   * that can follow the current rule.</p>\n   *\n   */\n\n\n  recover(recognizer, e) {\n    if (this.lastErrorIndex === recognizer.getInputStream().index && this.lastErrorStates !== null && this.lastErrorStates.indexOf(recognizer.state) >= 0) {\n      // uh oh, another error at same token index and previously-visited\n      // state in ATN; must be a case where LT(1) is in the recovery\n      // token set so nothing got consumed. Consume a single token\n      // at least to prevent an infinite loop; this is a failsafe.\n      recognizer.consume();\n    }\n\n    this.lastErrorIndex = recognizer._input.index;\n\n    if (this.lastErrorStates === null) {\n      this.lastErrorStates = [];\n    }\n\n    this.lastErrorStates.push(recognizer.state);\n    const followSet = this.getErrorRecoverySet(recognizer);\n    this.consumeUntil(recognizer, followSet);\n  }\n  /**\n   * The default implementation of {@link ANTLRErrorStrategy//sync} makes sure\n   * that the current lookahead symbol is consistent with what were expecting\n   * at this point in the ATN. You can call this anytime but ANTLR only\n   * generates code to check before subrules/loops and each iteration.\n   *\n   * <p>Implements Jim Idle's magic sync mechanism in closures and optional\n   * subrules. E.g.,</p>\n   *\n   * <pre>\n   * a : sync ( stuff sync )* ;\n   * sync : {consume to what can follow sync} ;\n   * </pre>\n   *\n   * At the start of a sub rule upon error, {@link //sync} performs single\n   * token deletion, if possible. If it can't do that, it bails on the current\n   * rule and uses the default error recovery, which consumes until the\n   * resynchronization set of the current rule.\n   *\n   * <p>If the sub rule is optional ({@code (...)?}, {@code (...)*}, or block\n   * with an empty alternative), then the expected set includes what follows\n   * the subrule.</p>\n   *\n   * <p>During loop iteration, it consumes until it sees a token that can start a\n   * sub rule or what follows loop. Yes, that is pretty aggressive. We opt to\n   * stay in the loop as long as possible.</p>\n   *\n   * <p><strong>ORIGINS</strong></p>\n   *\n   * <p>Previous versions of ANTLR did a poor job of their recovery within loops.\n   * A single mismatch token or missing token would force the parser to bail\n   * out of the entire rules surrounding the loop. So, for rule</p>\n   *\n   * <pre>\n   * classDef : 'class' ID '{' member* '}'\n   * </pre>\n   *\n   * input with an extra token between members would force the parser to\n   * consume until it found the next class definition rather than the next\n   * member definition of the current class.\n   *\n   * <p>This functionality cost a little bit of effort because the parser has to\n   * compare token set at the start of the loop and at each iteration. If for\n   * some reason speed is suffering for you, you can turn off this\n   * functionality by simply overriding this method as a blank { }.</p>\n   *\n   */\n\n\n  sync(recognizer) {\n    // If already recovering, don't try to sync\n    if (this.inErrorRecoveryMode(recognizer)) {\n      return;\n    }\n\n    const s = recognizer._interp.atn.states[recognizer.state];\n    const la = recognizer.getTokenStream().LA(1); // try cheaper subset first; might get lucky. seems to shave a wee bit off\n\n    const nextTokens = recognizer.atn.nextTokens(s);\n\n    if (nextTokens.contains(la)) {\n      this.nextTokensContext = null;\n      this.nextTokenState = ATNState.INVALID_STATE_NUMBER;\n      return;\n    } else if (nextTokens.contains(Token.EPSILON)) {\n      if (this.nextTokensContext === null) {\n        // It's possible the next token won't match information tracked\n        // by sync is restricted for performance.\n        this.nextTokensContext = recognizer._ctx;\n        this.nextTokensState = recognizer._stateNumber;\n      }\n\n      return;\n    }\n\n    switch (s.stateType) {\n      case ATNState.BLOCK_START:\n      case ATNState.STAR_BLOCK_START:\n      case ATNState.PLUS_BLOCK_START:\n      case ATNState.STAR_LOOP_ENTRY:\n        // report error and recover if possible\n        if (this.singleTokenDeletion(recognizer) !== null) {\n          return;\n        } else {\n          throw new InputMismatchException(recognizer);\n        }\n\n      case ATNState.PLUS_LOOP_BACK:\n      case ATNState.STAR_LOOP_BACK:\n        this.reportUnwantedToken(recognizer);\n        const expecting = new IntervalSet();\n        expecting.addSet(recognizer.getExpectedTokens());\n        const whatFollowsLoopIterationOrRule = expecting.addSet(this.getErrorRecoverySet(recognizer));\n        this.consumeUntil(recognizer, whatFollowsLoopIterationOrRule);\n        break;\n\n      default: // do nothing if we can't identify the exact kind of ATN state\n\n    }\n  }\n  /**\n   * This is called by {@link //reportError} when the exception is a\n   * {@link NoViableAltException}.\n   *\n   * @see //reportError\n   *\n   * @param recognizer the parser instance\n   * @param e the recognition exception\n   */\n\n\n  reportNoViableAlternative(recognizer, e) {\n    const tokens = recognizer.getTokenStream();\n    let input;\n\n    if (tokens !== null) {\n      if (e.startToken.type === Token.EOF) {\n        input = \"<EOF>\";\n      } else {\n        input = tokens.getText(new Interval(e.startToken.tokenIndex, e.offendingToken.tokenIndex));\n      }\n    } else {\n      input = \"<unknown input>\";\n    }\n\n    const msg = \"no viable alternative at input \" + this.escapeWSAndQuote(input);\n    recognizer.notifyErrorListeners(msg, e.offendingToken, e);\n  }\n  /**\n   * This is called by {@link //reportError} when the exception is an\n   * {@link InputMismatchException}.\n   *\n   * @see //reportError\n   *\n   * @param recognizer the parser instance\n   * @param e the recognition exception\n   */\n\n\n  reportInputMismatch(recognizer, e) {\n    const msg = \"mismatched input \" + this.getTokenErrorDisplay(e.offendingToken) + \" expecting \" + e.getExpectedTokens().toString(recognizer.literalNames, recognizer.symbolicNames);\n    recognizer.notifyErrorListeners(msg, e.offendingToken, e);\n  }\n  /**\n   * This is called by {@link //reportError} when the exception is a\n   * {@link FailedPredicateException}.\n   *\n   * @see //reportError\n   *\n   * @param recognizer the parser instance\n   * @param e the recognition exception\n   */\n\n\n  reportFailedPredicate(recognizer, e) {\n    const ruleName = recognizer.ruleNames[recognizer._ctx.ruleIndex];\n    const msg = \"rule \" + ruleName + \" \" + e.message;\n    recognizer.notifyErrorListeners(msg, e.offendingToken, e);\n  }\n  /**\n   * This method is called to report a syntax error which requires the removal\n   * of a token from the input stream. At the time this method is called, the\n   * erroneous symbol is current {@code LT(1)} symbol and has not yet been\n   * removed from the input stream. When this method returns,\n   * {@code recognizer} is in error recovery mode.\n   *\n   * <p>This method is called when {@link //singleTokenDeletion} identifies\n   * single-token deletion as a viable recovery strategy for a mismatched\n   * input error.</p>\n   *\n   * <p>The default implementation simply returns if the handler is already in\n   * error recovery mode. Otherwise, it calls {@link //beginErrorCondition} to\n   * enter error recovery mode, followed by calling\n   * {@link Parser//notifyErrorListeners}.</p>\n   *\n   * @param recognizer the parser instance\n   *\n   */\n\n\n  reportUnwantedToken(recognizer) {\n    if (this.inErrorRecoveryMode(recognizer)) {\n      return;\n    }\n\n    this.beginErrorCondition(recognizer);\n    const t = recognizer.getCurrentToken();\n    const tokenName = this.getTokenErrorDisplay(t);\n    const expecting = this.getExpectedTokens(recognizer);\n    const msg = \"extraneous input \" + tokenName + \" expecting \" + expecting.toString(recognizer.literalNames, recognizer.symbolicNames);\n    recognizer.notifyErrorListeners(msg, t, null);\n  }\n  /**\n   * This method is called to report a syntax error which requires the\n   * insertion of a missing token into the input stream. At the time this\n   * method is called, the missing token has not yet been inserted. When this\n   * method returns, {@code recognizer} is in error recovery mode.\n   *\n   * <p>This method is called when {@link //singleTokenInsertion} identifies\n   * single-token insertion as a viable recovery strategy for a mismatched\n   * input error.</p>\n   *\n   * <p>The default implementation simply returns if the handler is already in\n   * error recovery mode. Otherwise, it calls {@link //beginErrorCondition} to\n   * enter error recovery mode, followed by calling\n   * {@link Parser//notifyErrorListeners}.</p>\n   *\n   * @param recognizer the parser instance\n   */\n\n\n  reportMissingToken(recognizer) {\n    if (this.inErrorRecoveryMode(recognizer)) {\n      return;\n    }\n\n    this.beginErrorCondition(recognizer);\n    const t = recognizer.getCurrentToken();\n    const expecting = this.getExpectedTokens(recognizer);\n    const msg = \"missing \" + expecting.toString(recognizer.literalNames, recognizer.symbolicNames) + \" at \" + this.getTokenErrorDisplay(t);\n    recognizer.notifyErrorListeners(msg, t, null);\n  }\n  /**\n   * <p>The default implementation attempts to recover from the mismatched input\n   * by using single token insertion and deletion as described below. If the\n   * recovery attempt fails, this method throws an\n   * {@link InputMismatchException}.</p>\n   *\n   * <p><strong>EXTRA TOKEN</strong> (single token deletion)</p>\n   *\n   * <p>{@code LA(1)} is not what we are looking for. If {@code LA(2)} has the\n   * right token, however, then assume {@code LA(1)} is some extra spurious\n   * token and delete it. Then consume and return the next token (which was\n   * the {@code LA(2)} token) as the successful result of the match operation.</p>\n   *\n   * <p>This recovery strategy is implemented by {@link\n   * //singleTokenDeletion}.</p>\n   *\n   * <p><strong>MISSING TOKEN</strong> (single token insertion)</p>\n   *\n   * <p>If current token (at {@code LA(1)}) is consistent with what could come\n   * after the expected {@code LA(1)} token, then assume the token is missing\n   * and use the parser's {@link TokenFactory} to create it on the fly. The\n   * \"insertion\" is performed by returning the created token as the successful\n   * result of the match operation.</p>\n   *\n   * <p>This recovery strategy is implemented by {@link\n   * //singleTokenInsertion}.</p>\n   *\n   * <p><strong>EXAMPLE</strong></p>\n   *\n   * <p>For example, Input {@code i=(3;} is clearly missing the {@code ')'}. When\n   * the parser returns from the nested call to {@code expr}, it will have\n   * call chain:</p>\n   *\n   * <pre>\n   * stat &rarr; expr &rarr; atom\n   * </pre>\n   *\n   * and it will be trying to match the {@code ')'} at this point in the\n   * derivation:\n   *\n   * <pre>\n   * =&gt; ID '=' '(' INT ')' ('+' atom)* ';'\n   * ^\n   * </pre>\n   *\n   * The attempt to match {@code ')'} will fail when it sees {@code ';'} and\n   * call {@link //recoverInline}. To recover, it sees that {@code LA(1)==';'}\n   * is in the set of tokens that can follow the {@code ')'} token reference\n   * in rule {@code atom}. It can assume that you forgot the {@code ')'}.\n   */\n\n\n  recoverInline(recognizer) {\n    // SINGLE TOKEN DELETION\n    const matchedSymbol = this.singleTokenDeletion(recognizer);\n\n    if (matchedSymbol !== null) {\n      // we have deleted the extra token.\n      // now, move past ttype token as if all were ok\n      recognizer.consume();\n      return matchedSymbol;\n    } // SINGLE TOKEN INSERTION\n\n\n    if (this.singleTokenInsertion(recognizer)) {\n      return this.getMissingSymbol(recognizer);\n    } // even that didn't work; must throw the exception\n\n\n    throw new InputMismatchException(recognizer);\n  }\n  /**\n   * This method implements the single-token insertion inline error recovery\n   * strategy. It is called by {@link //recoverInline} if the single-token\n   * deletion strategy fails to recover from the mismatched input. If this\n   * method returns {@code true}, {@code recognizer} will be in error recovery\n   * mode.\n   *\n   * <p>This method determines whether or not single-token insertion is viable by\n   * checking if the {@code LA(1)} input symbol could be successfully matched\n   * if it were instead the {@code LA(2)} symbol. If this method returns\n   * {@code true}, the caller is responsible for creating and inserting a\n   * token with the correct type to produce this behavior.</p>\n   *\n   * @param recognizer the parser instance\n   * @return {@code true} if single-token insertion is a viable recovery\n   * strategy for the current mismatched input, otherwise {@code false}\n   */\n\n\n  singleTokenInsertion(recognizer) {\n    const currentSymbolType = recognizer.getTokenStream().LA(1); // if current token is consistent with what could come after current\n    // ATN state, then we know we're missing a token; error recovery\n    // is free to conjure up and insert the missing token\n\n    const atn = recognizer._interp.atn;\n    const currentState = atn.states[recognizer.state];\n    const next = currentState.transitions[0].target;\n    const expectingAtLL2 = atn.nextTokens(next, recognizer._ctx);\n\n    if (expectingAtLL2.contains(currentSymbolType)) {\n      this.reportMissingToken(recognizer);\n      return true;\n    } else {\n      return false;\n    }\n  }\n  /**\n   * This method implements the single-token deletion inline error recovery\n   * strategy. It is called by {@link //recoverInline} to attempt to recover\n   * from mismatched input. If this method returns null, the parser and error\n   * handler state will not have changed. If this method returns non-null,\n   * {@code recognizer} will <em>not</em> be in error recovery mode since the\n   * returned token was a successful match.\n   *\n   * <p>If the single-token deletion is successful, this method calls\n   * {@link //reportUnwantedToken} to report the error, followed by\n   * {@link Parser//consume} to actually \"delete\" the extraneous token. Then,\n   * before returning {@link //reportMatch} is called to signal a successful\n   * match.</p>\n   *\n   * @param recognizer the parser instance\n   * @return the successfully matched {@link Token} instance if single-token\n   * deletion successfully recovers from the mismatched input, otherwise\n   * {@code null}\n   */\n\n\n  singleTokenDeletion(recognizer) {\n    const nextTokenType = recognizer.getTokenStream().LA(2);\n    const expecting = this.getExpectedTokens(recognizer);\n\n    if (expecting.contains(nextTokenType)) {\n      this.reportUnwantedToken(recognizer); // print(\"recoverFromMismatchedToken deleting \" \\\n      // + str(recognizer.getTokenStream().LT(1)) \\\n      // + \" since \" + str(recognizer.getTokenStream().LT(2)) \\\n      // + \" is what we want\", file=sys.stderr)\n\n      recognizer.consume(); // simply delete extra token\n      // we want to return the token we're actually matching\n\n      const matchedSymbol = recognizer.getCurrentToken();\n      this.reportMatch(recognizer); // we know current token is correct\n\n      return matchedSymbol;\n    } else {\n      return null;\n    }\n  }\n  /**\n   * Conjure up a missing token during error recovery.\n   *\n   * The recognizer attempts to recover from single missing\n   * symbols. But, actions might refer to that missing symbol.\n   * For example, x=ID {f($x);}. The action clearly assumes\n   * that there has been an identifier matched previously and that\n   * $x points at that token. If that token is missing, but\n   * the next token in the stream is what we want we assume that\n   * this token is missing and we keep going. Because we\n   * have to return some token to replace the missing token,\n   * we have to conjure one up. This method gives the user control\n   * over the tokens returned for missing tokens. Mostly,\n   * you will want to create something special for identifier\n   * tokens. For literals such as '{' and ',', the default\n   * action in the parser or tree parser works. It simply creates\n   * a CommonToken of the appropriate type. The text will be the token.\n   * If you change what tokens must be created by the lexer,\n   * override this method to create the appropriate tokens.\n   *\n   */\n\n\n  getMissingSymbol(recognizer) {\n    const currentSymbol = recognizer.getCurrentToken();\n    const expecting = this.getExpectedTokens(recognizer);\n    const expectedTokenType = expecting.first(); // get any element\n\n    let tokenText;\n\n    if (expectedTokenType === Token.EOF) {\n      tokenText = \"<missing EOF>\";\n    } else {\n      tokenText = \"<missing \" + recognizer.literalNames[expectedTokenType] + \">\";\n    }\n\n    let current = currentSymbol;\n    const lookback = recognizer.getTokenStream().LT(-1);\n\n    if (current.type === Token.EOF && lookback !== null) {\n      current = lookback;\n    }\n\n    return recognizer.getTokenFactory().create(current.source, expectedTokenType, tokenText, Token.DEFAULT_CHANNEL, -1, -1, current.line, current.column);\n  }\n\n  getExpectedTokens(recognizer) {\n    return recognizer.getExpectedTokens();\n  }\n  /**\n   * How should a token be displayed in an error message? The default\n   * is to display just the text, but during development you might\n   * want to have a lot of information spit out. Override in that case\n   * to use t.toString() (which, for CommonToken, dumps everything about\n   * the token). This is better than forcing you to override a method in\n   * your token objects because you don't have to go modify your lexer\n   * so that it creates a new Java type.\n   */\n\n\n  getTokenErrorDisplay(t) {\n    if (t === null) {\n      return \"<no token>\";\n    }\n\n    let s = t.text;\n\n    if (s === null) {\n      if (t.type === Token.EOF) {\n        s = \"<EOF>\";\n      } else {\n        s = \"<\" + t.type + \">\";\n      }\n    }\n\n    return this.escapeWSAndQuote(s);\n  }\n\n  escapeWSAndQuote(s) {\n    s = s.replace(/\\n/g, \"\\\\n\");\n    s = s.replace(/\\r/g, \"\\\\r\");\n    s = s.replace(/\\t/g, \"\\\\t\");\n    return \"'\" + s + \"'\";\n  }\n  /**\n   * Compute the error recovery set for the current rule. During\n   * rule invocation, the parser pushes the set of tokens that can\n   * follow that rule reference on the stack; this amounts to\n   * computing FIRST of what follows the rule reference in the\n   * enclosing rule. See LinearApproximator.FIRST().\n   * This local follow set only includes tokens\n   * from within the rule; i.e., the FIRST computation done by\n   * ANTLR stops at the end of a rule.\n   *\n   * EXAMPLE\n   *\n   * When you find a \"no viable alt exception\", the input is not\n   * consistent with any of the alternatives for rule r. The best\n   * thing to do is to consume tokens until you see something that\n   * can legally follow a call to r//or* any rule that called r.\n   * You don't want the exact set of viable next tokens because the\n   * input might just be missing a token--you might consume the\n   * rest of the input looking for one of the missing tokens.\n   *\n   * Consider grammar:\n   *\n   * a : '[' b ']'\n   * | '(' b ')'\n   * ;\n   * b : c '^' INT ;\n   * c : ID\n   * | INT\n   * ;\n   *\n   * At each rule invocation, the set of tokens that could follow\n   * that rule is pushed on a stack. Here are the various\n   * context-sensitive follow sets:\n   *\n   * FOLLOW(b1_in_a) = FIRST(']') = ']'\n   * FOLLOW(b2_in_a) = FIRST(')') = ')'\n   * FOLLOW(c_in_b) = FIRST('^') = '^'\n   *\n   * Upon erroneous input \"[]\", the call chain is\n   *\n   * a -> b -> c\n   *\n   * and, hence, the follow context stack is:\n   *\n   * depth follow set start of rule execution\n   * 0 <EOF> a (from main())\n   * 1 ']' b\n   * 2 '^' c\n   *\n   * Notice that ')' is not included, because b would have to have\n   * been called from a different context in rule a for ')' to be\n   * included.\n   *\n   * For error recovery, we cannot consider FOLLOW(c)\n   * (context-sensitive or otherwise). We need the combined set of\n   * all context-sensitive FOLLOW sets--the set of all tokens that\n   * could follow any reference in the call chain. We need to\n   * resync to one of those tokens. Note that FOLLOW(c)='^' and if\n   * we resync'd to that token, we'd consume until EOF. We need to\n   * sync to context-sensitive FOLLOWs for a, b, and c: {']','^'}.\n   * In this case, for input \"[]\", LA(1) is ']' and in the set, so we would\n   * not consume anything. After printing an error, rule c would\n   * return normally. Rule b would not find the required '^' though.\n   * At this point, it gets a mismatched token error and throws an\n   * exception (since LA(1) is not in the viable following token\n   * set). The rule exception handler tries to recover, but finds\n   * the same recovery set and doesn't consume anything. Rule b\n   * exits normally returning to rule a. Now it finds the ']' (and\n   * with the successful match exits errorRecovery mode).\n   *\n   * So, you can see that the parser walks up the call chain looking\n   * for the token that was a member of the recovery set.\n   *\n   * Errors are not generated in errorRecovery mode.\n   *\n   * ANTLR's error recovery mechanism is based upon original ideas:\n   *\n   * \"Algorithms + Data Structures = Programs\" by Niklaus Wirth\n   *\n   * and\n   *\n   * \"A note on error recovery in recursive descent parsers\":\n   * http://portal.acm.org/citation.cfm?id=947902.947905\n   *\n   * Later, Josef Grosch had some good ideas:\n   *\n   * \"Efficient and Comfortable Error Recovery in Recursive Descent\n   * Parsers\":\n   * ftp://www.cocolab.com/products/cocktail/doca4.ps/ell.ps.zip\n   *\n   * Like Grosch I implement context-sensitive FOLLOW sets that are combined\n   * at run-time upon error to avoid overhead during parsing.\n   */\n\n\n  getErrorRecoverySet(recognizer) {\n    const atn = recognizer._interp.atn;\n    let ctx = recognizer._ctx;\n    const recoverSet = new IntervalSet();\n\n    while (ctx !== null && ctx.invokingState >= 0) {\n      // compute what follows who invoked us\n      const invokingState = atn.states[ctx.invokingState];\n      const rt = invokingState.transitions[0];\n      const follow = atn.nextTokens(rt.followState);\n      recoverSet.addSet(follow);\n      ctx = ctx.parentCtx;\n    }\n\n    recoverSet.removeOne(Token.EPSILON);\n    return recoverSet;\n  } // Consume tokens until one matches the given token set.//\n\n\n  consumeUntil(recognizer, set) {\n    let ttype = recognizer.getTokenStream().LA(1);\n\n    while (ttype !== Token.EOF && !set.contains(ttype)) {\n      recognizer.consume();\n      ttype = recognizer.getTokenStream().LA(1);\n    }\n  }\n\n}\n/**\n * This implementation of {@link ANTLRErrorStrategy} responds to syntax errors\n * by immediately canceling the parse operation with a\n * {@link ParseCancellationException}. The implementation ensures that the\n * {@link ParserRuleContext//exception} field is set for all parse tree nodes\n * that were not completed prior to encountering the error.\n *\n * <p>\n * This error strategy is useful in the following scenarios.</p>\n *\n * <ul>\n * <li><strong>Two-stage parsing:</strong> This error strategy allows the first\n * stage of two-stage parsing to immediately terminate if an error is\n * encountered, and immediately fall back to the second stage. In addition to\n * avoiding wasted work by attempting to recover from errors here, the empty\n * implementation of {@link BailErrorStrategy//sync} improves the performance of\n * the first stage.</li>\n * <li><strong>Silent validation:</strong> When syntax errors are not being\n * reported or logged, and the parse result is simply ignored if errors occur,\n * the {@link BailErrorStrategy} avoids wasting work on recovering from errors\n * when the result will be ignored either way.</li>\n * </ul>\n *\n * <p>\n * {@code myparser.setErrorHandler(new BailErrorStrategy());}</p>\n *\n * @see Parser//setErrorHandler(ANTLRErrorStrategy)\n * */\n\n\nclass BailErrorStrategy extends DefaultErrorStrategy {\n  constructor() {\n    super();\n  }\n  /**\n   * Instead of recovering from exception {@code e}, re-throw it wrapped\n   * in a {@link ParseCancellationException} so it is not caught by the\n   * rule function catches. Use {@link Exception//getCause()} to get the\n   * original {@link RecognitionException}.\n   */\n\n\n  recover(recognizer, e) {\n    let context = recognizer._ctx;\n\n    while (context !== null) {\n      context.exception = e;\n      context = context.parentCtx;\n    }\n\n    throw new ParseCancellationException(e);\n  }\n  /**\n   * Make sure we don't attempt to recover inline; if the parser\n   * successfully recovers, it won't throw an exception.\n   */\n\n\n  recoverInline(recognizer) {\n    this.recover(recognizer, new InputMismatchException(recognizer));\n  } // Make sure we don't attempt to recover from problems in subrules.//\n\n\n  sync(recognizer) {// pass\n  }\n\n}\n\nmodule.exports = {\n  BailErrorStrategy,\n  DefaultErrorStrategy\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/error/ErrorStrategy.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/error/Errors.js":
/*!********************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/error/Errors.js ***!
  \********************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\n\n/**\n * The root of the ANTLR exception hierarchy. In general, ANTLR tracks just\n *  3 kinds of errors: prediction errors, failed predicate errors, and\n *  mismatched input errors. In each case, the parser knows where it is\n *  in the input, where it is in the ATN, the rule invocation stack,\n *  and what kind of problem occurred.\n */\nconst {\n  PredicateTransition\n} = __webpack_require__(/*! ./../atn/Transition */ \"./node_modules/antlr4/src/antlr4/atn/Transition.js\");\n\nconst {\n  Interval\n} = (__webpack_require__(/*! ../IntervalSet */ \"./node_modules/antlr4/src/antlr4/IntervalSet.js\").Interval);\n\nclass RecognitionException extends Error {\n  constructor(params) {\n    super(params.message);\n\n    if (!!Error.captureStackTrace) {\n      Error.captureStackTrace(this, RecognitionException);\n    } else {\n      var stack = new Error().stack;\n    }\n\n    this.message = params.message;\n    this.recognizer = params.recognizer;\n    this.input = params.input;\n    this.ctx = params.ctx;\n    /**\n     * The current {@link Token} when an error occurred. Since not all streams\n     * support accessing symbols by index, we have to track the {@link Token}\n     * instance itself\n    */\n\n    this.offendingToken = null;\n    /**\n     * Get the ATN state number the parser was in at the time the error\n     * occurred. For {@link NoViableAltException} and\n     * {@link LexerNoViableAltException} exceptions, this is the\n     * {@link DecisionState} number. For others, it is the state whose outgoing\n     * edge we couldn't match.\n     */\n\n    this.offendingState = -1;\n\n    if (this.recognizer !== null) {\n      this.offendingState = this.recognizer.state;\n    }\n  }\n  /**\n   * Gets the set of input symbols which could potentially follow the\n   * previously matched symbol at the time this exception was thrown.\n   *\n   * <p>If the set of expected tokens is not known and could not be computed,\n   * this method returns {@code null}.</p>\n   *\n   * @return The set of token types that could potentially follow the current\n   * state in the ATN, or {@code null} if the information is not available.\n   */\n\n\n  getExpectedTokens() {\n    if (this.recognizer !== null) {\n      return this.recognizer.atn.getExpectedTokens(this.offendingState, this.ctx);\n    } else {\n      return null;\n    }\n  } // <p>If the state number is not known, this method returns -1.</p>\n\n\n  toString() {\n    return this.message;\n  }\n\n}\n\nclass LexerNoViableAltException extends RecognitionException {\n  constructor(lexer, input, startIndex, deadEndConfigs) {\n    super({\n      message: \"\",\n      recognizer: lexer,\n      input: input,\n      ctx: null\n    });\n    this.startIndex = startIndex;\n    this.deadEndConfigs = deadEndConfigs;\n  }\n\n  toString() {\n    let symbol = \"\";\n\n    if (this.startIndex >= 0 && this.startIndex < this.input.size) {\n      symbol = this.input.getText(new Interval(this.startIndex, this.startIndex));\n    }\n\n    return \"LexerNoViableAltException\" + symbol;\n  }\n\n}\n/**\n * Indicates that the parser could not decide which of two or more paths\n * to take based upon the remaining input. It tracks the starting token\n * of the offending input and also knows where the parser was\n * in the various paths when the error. Reported by reportNoViableAlternative()\n */\n\n\nclass NoViableAltException extends RecognitionException {\n  constructor(recognizer, input, startToken, offendingToken, deadEndConfigs, ctx) {\n    ctx = ctx || recognizer._ctx;\n    offendingToken = offendingToken || recognizer.getCurrentToken();\n    startToken = startToken || recognizer.getCurrentToken();\n    input = input || recognizer.getInputStream();\n    super({\n      message: \"\",\n      recognizer: recognizer,\n      input: input,\n      ctx: ctx\n    }); // Which configurations did we try at input.index() that couldn't match\n    // input.LT(1)?//\n\n    this.deadEndConfigs = deadEndConfigs; // The token object at the start index; the input stream might\n    // not be buffering tokens so get a reference to it. (At the\n    // time the error occurred, of course the stream needs to keep a\n    // buffer all of the tokens but later we might not have access to those.)\n\n    this.startToken = startToken;\n    this.offendingToken = offendingToken;\n  }\n\n}\n/**\n * This signifies any kind of mismatched input exceptions such as\n * when the current input does not match the expected token.\n*/\n\n\nclass InputMismatchException extends RecognitionException {\n  constructor(recognizer) {\n    super({\n      message: \"\",\n      recognizer: recognizer,\n      input: recognizer.getInputStream(),\n      ctx: recognizer._ctx\n    });\n    this.offendingToken = recognizer.getCurrentToken();\n  }\n\n}\n\nfunction formatMessage(predicate, message) {\n  if (message !== null) {\n    return message;\n  } else {\n    return \"failed predicate: {\" + predicate + \"}?\";\n  }\n}\n/**\n * A semantic predicate failed during validation. Validation of predicates\n * occurs when normally parsing the alternative just like matching a token.\n * Disambiguating predicate evaluation occurs when we test a predicate during\n * prediction.\n*/\n\n\nclass FailedPredicateException extends RecognitionException {\n  constructor(recognizer, predicate, message) {\n    super({\n      message: formatMessage(predicate, message || null),\n      recognizer: recognizer,\n      input: recognizer.getInputStream(),\n      ctx: recognizer._ctx\n    });\n    const s = recognizer._interp.atn.states[recognizer.state];\n    const trans = s.transitions[0];\n\n    if (trans instanceof PredicateTransition) {\n      this.ruleIndex = trans.ruleIndex;\n      this.predicateIndex = trans.predIndex;\n    } else {\n      this.ruleIndex = 0;\n      this.predicateIndex = 0;\n    }\n\n    this.predicate = predicate;\n    this.offendingToken = recognizer.getCurrentToken();\n  }\n\n}\n\nclass ParseCancellationException extends Error {\n  constructor() {\n    super();\n    Error.captureStackTrace(this, ParseCancellationException);\n  }\n\n}\n\nmodule.exports = {\n  RecognitionException,\n  NoViableAltException,\n  LexerNoViableAltException,\n  InputMismatchException,\n  FailedPredicateException,\n  ParseCancellationException\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/error/Errors.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/error/index.js":
/*!*******************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/error/index.js ***!
  \*******************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nmodule.exports.RecognitionException = __webpack_require__(/*! ./Errors */ \"./node_modules/antlr4/src/antlr4/error/Errors.js\").RecognitionException;\nmodule.exports.NoViableAltException = __webpack_require__(/*! ./Errors */ \"./node_modules/antlr4/src/antlr4/error/Errors.js\").NoViableAltException;\nmodule.exports.LexerNoViableAltException = __webpack_require__(/*! ./Errors */ \"./node_modules/antlr4/src/antlr4/error/Errors.js\").LexerNoViableAltException;\nmodule.exports.InputMismatchException = __webpack_require__(/*! ./Errors */ \"./node_modules/antlr4/src/antlr4/error/Errors.js\").InputMismatchException;\nmodule.exports.FailedPredicateException = __webpack_require__(/*! ./Errors */ \"./node_modules/antlr4/src/antlr4/error/Errors.js\").FailedPredicateException;\nmodule.exports.DiagnosticErrorListener = __webpack_require__(/*! ./DiagnosticErrorListener */ \"./node_modules/antlr4/src/antlr4/error/DiagnosticErrorListener.js\");\nmodule.exports.BailErrorStrategy = __webpack_require__(/*! ./ErrorStrategy */ \"./node_modules/antlr4/src/antlr4/error/ErrorStrategy.js\").BailErrorStrategy;\nmodule.exports.DefaultErrorStrategy = __webpack_require__(/*! ./ErrorStrategy */ \"./node_modules/antlr4/src/antlr4/error/ErrorStrategy.js\").DefaultErrorStrategy;\nmodule.exports.ErrorListener = __webpack_require__(/*! ./ErrorListener */ \"./node_modules/antlr4/src/antlr4/error/ErrorListener.js\").ErrorListener;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/error/index.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/index.js":
/*!*************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/index.js ***!
  \*************************************************/
/***/ ((__unused_webpack_module, exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nexports.atn = __webpack_require__(/*! ./atn/index */ \"./node_modules/antlr4/src/antlr4/atn/index.js\");\nexports.codepointat = __webpack_require__(/*! ./polyfills/codepointat */ \"./node_modules/antlr4/src/antlr4/polyfills/codepointat.js\");\nexports.dfa = __webpack_require__(/*! ./dfa/index */ \"./node_modules/antlr4/src/antlr4/dfa/index.js\");\nexports.fromcodepoint = __webpack_require__(/*! ./polyfills/fromcodepoint */ \"./node_modules/antlr4/src/antlr4/polyfills/fromcodepoint.js\");\nexports.tree = __webpack_require__(/*! ./tree/index */ \"./node_modules/antlr4/src/antlr4/tree/index.js\");\nexports.error = __webpack_require__(/*! ./error/index */ \"./node_modules/antlr4/src/antlr4/error/index.js\");\nexports.Token = __webpack_require__(/*! ./Token */ \"./node_modules/antlr4/src/antlr4/Token.js\").Token;\nexports.CharStreams = __webpack_require__(/*! ./CharStreams */ \"./node_modules/antlr4/src/antlr4/CharStreams.js\");\nexports.CommonToken = __webpack_require__(/*! ./Token */ \"./node_modules/antlr4/src/antlr4/Token.js\").CommonToken;\nexports.InputStream = __webpack_require__(/*! ./InputStream */ \"./node_modules/antlr4/src/antlr4/InputStream.js\");\nexports.FileStream = __webpack_require__(/*! ./FileStream */ \"./node_modules/antlr4/src/antlr4/FileStream.js\");\nexports.CommonTokenStream = __webpack_require__(/*! ./CommonTokenStream */ \"./node_modules/antlr4/src/antlr4/CommonTokenStream.js\");\nexports.Lexer = __webpack_require__(/*! ./Lexer */ \"./node_modules/antlr4/src/antlr4/Lexer.js\");\nexports.Parser = __webpack_require__(/*! ./Parser */ \"./node_modules/antlr4/src/antlr4/Parser.js\");\n\nvar pc = __webpack_require__(/*! ./PredictionContext */ \"./node_modules/antlr4/src/antlr4/PredictionContext.js\");\n\nexports.PredictionContextCache = pc.PredictionContextCache;\nexports.ParserRuleContext = __webpack_require__(/*! ./ParserRuleContext */ \"./node_modules/antlr4/src/antlr4/ParserRuleContext.js\");\nexports.Interval = __webpack_require__(/*! ./IntervalSet */ \"./node_modules/antlr4/src/antlr4/IntervalSet.js\").Interval;\nexports.IntervalSet = __webpack_require__(/*! ./IntervalSet */ \"./node_modules/antlr4/src/antlr4/IntervalSet.js\").IntervalSet;\nexports.Utils = __webpack_require__(/*! ./Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\nexports.LL1Analyzer = __webpack_require__(/*! ./LL1Analyzer */ \"./node_modules/antlr4/src/antlr4/LL1Analyzer.js\").LL1Analyzer;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/index.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/polyfills/codepointat.js":
/*!*****************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/polyfills/codepointat.js ***!
  \*****************************************************************/
/***/ (() => {

eval("/*! https://mths.be/codepointat v0.2.0 by @mathias */\nif (!String.prototype.codePointAt) {\n  (function () {\n    'use strict'; // needed to support `apply`/`call` with `undefined`/`null`\n\n    var defineProperty = function () {\n      // IE 8 only supports `Object.defineProperty` on DOM elements\n      let result;\n\n      try {\n        const object = {};\n        const $defineProperty = Object.defineProperty;\n        result = $defineProperty(object, object, object) && $defineProperty;\n      } catch (error) {}\n\n      return result;\n    }();\n\n    const codePointAt = function (position) {\n      if (this == null) {\n        throw TypeError();\n      }\n\n      const string = String(this);\n      const size = string.length; // `ToInteger`\n\n      let index = position ? Number(position) : 0;\n\n      if (index !== index) {\n        // better `isNaN`\n        index = 0;\n      } // Account for out-of-bounds indices:\n\n\n      if (index < 0 || index >= size) {\n        return undefined;\n      } // Get the first code unit\n\n\n      const first = string.charCodeAt(index);\n      let second;\n\n      if ( // check if its the start of a surrogate pair\n      first >= 0xD800 && first <= 0xDBFF && // high surrogate\n      size > index + 1 // there is a next code unit\n      ) {\n        second = string.charCodeAt(index + 1);\n\n        if (second >= 0xDC00 && second <= 0xDFFF) {\n          // low surrogate\n          // https://mathiasbynens.be/notes/javascript-encoding#surrogate-formulae\n          return (first - 0xD800) * 0x400 + second - 0xDC00 + 0x10000;\n        }\n      }\n\n      return first;\n    };\n\n    if (defineProperty) {\n      defineProperty(String.prototype, 'codePointAt', {\n        'value': codePointAt,\n        'configurable': true,\n        'writable': true\n      });\n    } else {\n      String.prototype.codePointAt = codePointAt;\n    }\n  })();\n}\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/polyfills/codepointat.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/polyfills/fromcodepoint.js":
/*!*******************************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/polyfills/fromcodepoint.js ***!
  \*******************************************************************/
/***/ (() => {

eval("/*! https://mths.be/fromcodepoint v0.2.1 by @mathias */\nif (!String.fromCodePoint) {\n  (function () {\n    const defineProperty = function () {\n      // IE 8 only supports `Object.defineProperty` on DOM elements\n      let result;\n\n      try {\n        const object = {};\n        const $defineProperty = Object.defineProperty;\n        result = $defineProperty(object, object, object) && $defineProperty;\n      } catch (error) {}\n\n      return result;\n    }();\n\n    const stringFromCharCode = String.fromCharCode;\n    const floor = Math.floor;\n\n    const fromCodePoint = function (_) {\n      const MAX_SIZE = 0x4000;\n      const codeUnits = [];\n      let highSurrogate;\n      let lowSurrogate;\n      let index = -1;\n      const length = arguments.length;\n\n      if (!length) {\n        return '';\n      }\n\n      let result = '';\n\n      while (++index < length) {\n        let codePoint = Number(arguments[index]);\n\n        if (!isFinite(codePoint) || // `NaN`, `+Infinity`, or `-Infinity`\n        codePoint < 0 || // not a valid Unicode code point\n        codePoint > 0x10FFFF || // not a valid Unicode code point\n        floor(codePoint) !== codePoint // not an integer\n        ) {\n          throw RangeError('Invalid code point: ' + codePoint);\n        }\n\n        if (codePoint <= 0xFFFF) {\n          // BMP code point\n          codeUnits.push(codePoint);\n        } else {\n          // Astral code point; split in surrogate halves\n          // https://mathiasbynens.be/notes/javascript-encoding#surrogate-formulae\n          codePoint -= 0x10000;\n          highSurrogate = (codePoint >> 10) + 0xD800;\n          lowSurrogate = codePoint % 0x400 + 0xDC00;\n          codeUnits.push(highSurrogate, lowSurrogate);\n        }\n\n        if (index + 1 === length || codeUnits.length > MAX_SIZE) {\n          result += stringFromCharCode.apply(null, codeUnits);\n          codeUnits.length = 0;\n        }\n      }\n\n      return result;\n    };\n\n    if (defineProperty) {\n      defineProperty(String, 'fromCodePoint', {\n        'value': fromCodePoint,\n        'configurable': true,\n        'writable': true\n      });\n    } else {\n      String.fromCodePoint = fromCodePoint;\n    }\n  })();\n}\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/polyfills/fromcodepoint.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/tree/Tree.js":
/*!*****************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/tree/Tree.js ***!
  \*****************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst {\n  Token\n} = __webpack_require__(/*! ./../Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\nconst {\n  Interval\n} = __webpack_require__(/*! ./../IntervalSet */ \"./node_modules/antlr4/src/antlr4/IntervalSet.js\");\n\nconst INVALID_INTERVAL = new Interval(-1, -2);\n/**\n * The basic notion of a tree has a parent, a payload, and a list of children.\n * It is the most abstract interface for all the trees used by ANTLR.\n */\n\nclass Tree {}\n\nclass SyntaxTree extends Tree {\n  constructor() {\n    super();\n  }\n\n}\n\nclass ParseTree extends SyntaxTree {\n  constructor() {\n    super();\n  }\n\n}\n\nclass RuleNode extends ParseTree {\n  constructor() {\n    super();\n  }\n\n  getRuleContext() {\n    throw new Error(\"missing interface implementation\");\n  }\n\n}\n\nclass TerminalNode extends ParseTree {\n  constructor() {\n    super();\n  }\n\n}\n\nclass ErrorNode extends TerminalNode {\n  constructor() {\n    super();\n  }\n\n}\n\nclass ParseTreeVisitor {\n  visit(ctx) {\n    if (Array.isArray(ctx)) {\n      return ctx.map(function (child) {\n        return child.accept(this);\n      }, this);\n    } else {\n      return ctx.accept(this);\n    }\n  }\n\n  visitChildren(ctx) {\n    if (ctx.children) {\n      return this.visit(ctx.children);\n    } else {\n      return null;\n    }\n  }\n\n  visitTerminal(node) {}\n\n  visitErrorNode(node) {}\n\n}\n\nclass ParseTreeListener {\n  visitTerminal(node) {}\n\n  visitErrorNode(node) {}\n\n  enterEveryRule(node) {}\n\n  exitEveryRule(node) {}\n\n}\n\nclass TerminalNodeImpl extends TerminalNode {\n  constructor(symbol) {\n    super();\n    this.parentCtx = null;\n    this.symbol = symbol;\n  }\n\n  getChild(i) {\n    return null;\n  }\n\n  getSymbol() {\n    return this.symbol;\n  }\n\n  getParent() {\n    return this.parentCtx;\n  }\n\n  getPayload() {\n    return this.symbol;\n  }\n\n  getSourceInterval() {\n    if (this.symbol === null) {\n      return INVALID_INTERVAL;\n    }\n\n    const tokenIndex = this.symbol.tokenIndex;\n    return new Interval(tokenIndex, tokenIndex);\n  }\n\n  getChildCount() {\n    return 0;\n  }\n\n  accept(visitor) {\n    return visitor.visitTerminal(this);\n  }\n\n  getText() {\n    return this.symbol.text;\n  }\n\n  toString() {\n    if (this.symbol.type === Token.EOF) {\n      return \"<EOF>\";\n    } else {\n      return this.symbol.text;\n    }\n  }\n\n}\n/**\n * Represents a token that was consumed during resynchronization\n * rather than during a valid match operation. For example,\n * we will create this kind of a node during single token insertion\n * and deletion as well as during \"consume until error recovery set\"\n * upon no viable alternative exceptions.\n */\n\n\nclass ErrorNodeImpl extends TerminalNodeImpl {\n  constructor(token) {\n    super(token);\n  }\n\n  isErrorNode() {\n    return true;\n  }\n\n  accept(visitor) {\n    return visitor.visitErrorNode(this);\n  }\n\n}\n\nclass ParseTreeWalker {\n  /**\n   * Performs a walk on the given parse tree starting at the root and going down recursively\n   * with depth-first search. On each node, {@link ParseTreeWalker//enterRule} is called before\n   * recursively walking down into child nodes, then\n   * {@link ParseTreeWalker//exitRule} is called after the recursive call to wind up.\n   * @param listener The listener used by the walker to process grammar rules\n   * @param t The parse tree to be walked on\n   */\n  walk(listener, t) {\n    const errorNode = t instanceof ErrorNode || t.isErrorNode !== undefined && t.isErrorNode();\n\n    if (errorNode) {\n      listener.visitErrorNode(t);\n    } else if (t instanceof TerminalNode) {\n      listener.visitTerminal(t);\n    } else {\n      this.enterRule(listener, t);\n\n      for (let i = 0; i < t.getChildCount(); i++) {\n        const child = t.getChild(i);\n        this.walk(listener, child);\n      }\n\n      this.exitRule(listener, t);\n    }\n  }\n  /**\n   * Enters a grammar rule by first triggering the generic event {@link ParseTreeListener//enterEveryRule}\n   * then by triggering the event specific to the given parse tree node\n   * @param listener The listener responding to the trigger events\n   * @param r The grammar rule containing the rule context\n   */\n\n\n  enterRule(listener, r) {\n    const ctx = r.getRuleContext();\n    listener.enterEveryRule(ctx);\n    ctx.enterRule(listener);\n  }\n  /**\n   * Exits a grammar rule by first triggering the event specific to the given parse tree node\n   * then by triggering the generic event {@link ParseTreeListener//exitEveryRule}\n   * @param listener The listener responding to the trigger events\n   * @param r The grammar rule containing the rule context\n   */\n\n\n  exitRule(listener, r) {\n    const ctx = r.getRuleContext();\n    ctx.exitRule(listener);\n    listener.exitEveryRule(ctx);\n  }\n\n}\n\nParseTreeWalker.DEFAULT = new ParseTreeWalker();\nmodule.exports = {\n  RuleNode,\n  ErrorNode,\n  TerminalNode,\n  ErrorNodeImpl,\n  TerminalNodeImpl,\n  ParseTreeListener,\n  ParseTreeVisitor,\n  ParseTreeWalker,\n  INVALID_INTERVAL\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/tree/Tree.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/tree/Trees.js":
/*!******************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/tree/Trees.js ***!
  \******************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst Utils = __webpack_require__(/*! ./../Utils */ \"./node_modules/antlr4/src/antlr4/Utils.js\");\n\nconst {\n  Token\n} = __webpack_require__(/*! ./../Token */ \"./node_modules/antlr4/src/antlr4/Token.js\");\n\nconst {\n  ErrorNode,\n  TerminalNode,\n  RuleNode\n} = __webpack_require__(/*! ./Tree */ \"./node_modules/antlr4/src/antlr4/tree/Tree.js\");\n/** A set of utility routines useful for all kinds of ANTLR trees. */\n\n\nconst Trees = {\n  /**\n   * Print out a whole tree in LISP form. {@link //getNodeText} is used on the\n   *  node payloads to get the text for the nodes.  Detect\n   *  parse trees and extract data appropriately.\n   */\n  toStringTree: function (tree, ruleNames, recog) {\n    ruleNames = ruleNames || null;\n    recog = recog || null;\n\n    if (recog !== null) {\n      ruleNames = recog.ruleNames;\n    }\n\n    let s = Trees.getNodeText(tree, ruleNames);\n    s = Utils.escapeWhitespace(s, false);\n    const c = tree.getChildCount();\n\n    if (c === 0) {\n      return s;\n    }\n\n    let res = \"(\" + s + ' ';\n\n    if (c > 0) {\n      s = Trees.toStringTree(tree.getChild(0), ruleNames);\n      res = res.concat(s);\n    }\n\n    for (let i = 1; i < c; i++) {\n      s = Trees.toStringTree(tree.getChild(i), ruleNames);\n      res = res.concat(' ' + s);\n    }\n\n    res = res.concat(\")\");\n    return res;\n  },\n  getNodeText: function (t, ruleNames, recog) {\n    ruleNames = ruleNames || null;\n    recog = recog || null;\n\n    if (recog !== null) {\n      ruleNames = recog.ruleNames;\n    }\n\n    if (ruleNames !== null) {\n      if (t instanceof RuleNode) {\n        const context = t.getRuleContext();\n        const altNumber = context.getAltNumber(); // use const value of ATN.INVALID_ALT_NUMBER to avoid circular dependency\n\n        if (altNumber != 0) {\n          return ruleNames[t.ruleIndex] + \":\" + altNumber;\n        }\n\n        return ruleNames[t.ruleIndex];\n      } else if (t instanceof ErrorNode) {\n        return t.toString();\n      } else if (t instanceof TerminalNode) {\n        if (t.symbol !== null) {\n          return t.symbol.text;\n        }\n      }\n    } // no recog for rule names\n\n\n    const payload = t.getPayload();\n\n    if (payload instanceof Token) {\n      return payload.text;\n    }\n\n    return t.getPayload().toString();\n  },\n\n  /**\n   * Return ordered list of all children of this node\n   */\n  getChildren: function (t) {\n    const list = [];\n\n    for (let i = 0; i < t.getChildCount(); i++) {\n      list.push(t.getChild(i));\n    }\n\n    return list;\n  },\n\n  /**\n   * Return a list of all ancestors of this node.  The first node of\n   * list is the root and the last is the parent of this node.\n   */\n  getAncestors: function (t) {\n    let ancestors = [];\n    t = t.getParent();\n\n    while (t !== null) {\n      ancestors = [t].concat(ancestors);\n      t = t.getParent();\n    }\n\n    return ancestors;\n  },\n  findAllTokenNodes: function (t, ttype) {\n    return Trees.findAllNodes(t, ttype, true);\n  },\n  findAllRuleNodes: function (t, ruleIndex) {\n    return Trees.findAllNodes(t, ruleIndex, false);\n  },\n  findAllNodes: function (t, index, findTokens) {\n    const nodes = [];\n\n    Trees._findAllNodes(t, index, findTokens, nodes);\n\n    return nodes;\n  },\n  _findAllNodes: function (t, index, findTokens, nodes) {\n    // check this node (the root) first\n    if (findTokens && t instanceof TerminalNode) {\n      if (t.symbol.type === index) {\n        nodes.push(t);\n      }\n    } else if (!findTokens && t instanceof RuleNode) {\n      if (t.ruleIndex === index) {\n        nodes.push(t);\n      }\n    } // check children\n\n\n    for (let i = 0; i < t.getChildCount(); i++) {\n      Trees._findAllNodes(t.getChild(i), index, findTokens, nodes);\n    }\n  },\n  descendants: function (t) {\n    let nodes = [t];\n\n    for (let i = 0; i < t.getChildCount(); i++) {\n      nodes = nodes.concat(Trees.descendants(t.getChild(i)));\n    }\n\n    return nodes;\n  }\n};\nmodule.exports = Trees;\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/tree/Trees.js?");

/***/ }),

/***/ "./node_modules/antlr4/src/antlr4/tree/index.js":
/*!******************************************************!*\
  !*** ./node_modules/antlr4/src/antlr4/tree/index.js ***!
  \******************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("/* Copyright (c) 2012-2017 The ANTLR Project. All rights reserved.\n * Use of this file is governed by the BSD 3-clause license that\n * can be found in the LICENSE.txt file in the project root.\n */\nconst Tree = __webpack_require__(/*! ./Tree */ \"./node_modules/antlr4/src/antlr4/tree/Tree.js\");\n\nconst Trees = __webpack_require__(/*! ./Trees */ \"./node_modules/antlr4/src/antlr4/tree/Trees.js\");\n\nmodule.exports = { ...Tree,\n  Trees\n};\n\n//# sourceURL=webpack://test/./node_modules/antlr4/src/antlr4/tree/index.js?");

/***/ }),

/***/ "./simplerlangLexer.js":
/*!*****************************!*\
  !*** ./simplerlangLexer.js ***!
  \*****************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   \"default\": () => (/* binding */ simplerlangLexer)\n/* harmony export */ });\n/* harmony import */ var antlr4__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! antlr4 */ \"./node_modules/antlr4/src/antlr4/index.js\");\n// Generated from simplerlang.g4 by ANTLR 4.9.2\n// jshint ignore: start\n\nconst serializedATN = [\"\\u0003\\u608b\\ua72a\\u8133\\ub9ed\\u417c\\u3be7\\u7786\", \"\\u5964\\u0002%\\u0129\\b\\u0001\\u0004\\u0002\\t\\u0002\\u0004\\u0003\\t\\u0003\", \"\\u0004\\u0004\\t\\u0004\\u0004\\u0005\\t\\u0005\\u0004\\u0006\\t\\u0006\\u0004\\u0007\", \"\\t\\u0007\\u0004\\b\\t\\b\\u0004\\t\\t\\t\\u0004\\n\\t\\n\\u0004\\u000b\\t\\u000b\\u0004\", \"\\f\\t\\f\\u0004\\r\\t\\r\\u0004\\u000e\\t\\u000e\\u0004\\u000f\\t\\u000f\\u0004\\u0010\", \"\\t\\u0010\\u0004\\u0011\\t\\u0011\\u0004\\u0012\\t\\u0012\\u0004\\u0013\\t\\u0013\", \"\\u0004\\u0014\\t\\u0014\\u0004\\u0015\\t\\u0015\\u0004\\u0016\\t\\u0016\\u0004\\u0017\", \"\\t\\u0017\\u0004\\u0018\\t\\u0018\\u0004\\u0019\\t\\u0019\\u0004\\u001a\\t\\u001a\", \"\\u0004\\u001b\\t\\u001b\\u0004\\u001c\\t\\u001c\\u0004\\u001d\\t\\u001d\\u0004\\u001e\", \"\\t\\u001e\\u0004\\u001f\\t\\u001f\\u0004 \\t \\u0004!\\t!\\u0004\\\"\\t\\\"\\u0004#\", \"\\t#\\u0004$\\t$\\u0003\\u0002\\u0003\\u0002\\u0003\\u0002\\u0003\\u0002\\u0003\", \"\\u0002\\u0003\\u0002\\u0003\\u0002\\u0003\\u0003\\u0003\\u0003\\u0003\\u0004\\u0003\", \"\\u0004\\u0003\\u0004\\u0003\\u0004\\u0003\\u0004\\u0003\\u0004\\u0003\\u0004\\u0003\", \"\\u0005\\u0003\\u0005\\u0003\\u0005\\u0003\\u0005\\u0003\\u0005\\u0003\\u0005\\u0003\", \"\\u0005\\u0003\\u0005\\u0003\\u0006\\u0003\\u0006\\u0003\\u0006\\u0003\\u0006\\u0003\", \"\\u0006\\u0003\\u0006\\u0003\\u0006\\u0003\\u0007\\u0003\\u0007\\u0003\\b\\u0003\", \"\\b\\u0003\\t\\u0003\\t\\u0003\\n\\u0003\\n\\u0003\\u000b\\u0003\\u000b\\u0003\\u000b\", \"\\u0003\\u000b\\u0003\\u000b\\u0003\\u000b\\u0003\\u000b\\u0003\\u000b\\u0003\\u000b\", \"\\u0003\\u000b\\u0003\\u000b\\u0003\\f\\u0003\\f\\u0003\\f\\u0003\\f\\u0003\\f\\u0003\", \"\\f\\u0003\\f\\u0003\\f\\u0003\\f\\u0003\\f\\u0003\\f\\u0003\\r\\u0003\\r\\u0003\\r\\u0003\", \"\\r\\u0003\\r\\u0003\\r\\u0003\\r\\u0003\\r\\u0003\\r\\u0003\\r\\u0003\\r\\u0003\\u000e\", \"\\u0003\\u000e\\u0003\\u000e\\u0003\\u000e\\u0003\\u000e\\u0003\\u000e\\u0003\\u000f\", \"\\u0003\\u000f\\u0003\\u000f\\u0003\\u000f\\u0003\\u000f\\u0003\\u000f\\u0003\\u000f\", \"\\u0003\\u000f\\u0003\\u0010\\u0003\\u0010\\u0003\\u0010\\u0003\\u0010\\u0003\\u0010\", \"\\u0003\\u0010\\u0003\\u0010\\u0003\\u0010\\u0003\\u0011\\u0003\\u0011\\u0003\\u0011\", \"\\u0003\\u0011\\u0003\\u0011\\u0003\\u0011\\u0003\\u0011\\u0003\\u0011\\u0003\\u0012\", \"\\u0003\\u0012\\u0003\\u0012\\u0003\\u0012\\u0003\\u0012\\u0003\\u0012\\u0003\\u0012\", \"\\u0003\\u0013\\u0003\\u0013\\u0003\\u0013\\u0003\\u0013\\u0003\\u0013\\u0003\\u0013\", \"\\u0003\\u0013\\u0003\\u0013\\u0003\\u0013\\u0003\\u0013\\u0003\\u0013\\u0003\\u0013\", \"\\u0003\\u0013\\u0003\\u0014\\u0003\\u0014\\u0003\\u0015\\u0003\\u0015\\u0003\\u0015\", \"\\u0003\\u0015\\u0003\\u0015\\u0003\\u0015\\u0003\\u0015\\u0003\\u0015\\u0003\\u0015\", \"\\u0003\\u0015\\u0003\\u0016\\u0003\\u0016\\u0003\\u0017\\u0003\\u0017\\u0003\\u0018\", \"\\u0003\\u0018\\u0003\\u0019\\u0003\\u0019\\u0003\\u0019\\u0003\\u0019\\u0003\\u0019\", \"\\u0003\\u0019\\u0003\\u0019\\u0003\\u001a\\u0003\\u001a\\u0003\\u001a\\u0003\\u001a\", \"\\u0003\\u001a\\u0003\\u001a\\u0003\\u001b\\u0003\\u001b\\u0003\\u001b\\u0003\\u001b\", \"\\u0003\\u001c\\u0003\\u001c\\u0003\\u001c\\u0003\\u001c\\u0003\\u001c\\u0003\\u001d\", \"\\u0003\\u001d\\u0003\\u001d\\u0003\\u001d\\u0003\\u001d\\u0003\\u001d\\u0003\\u001e\", \"\\u0003\\u001e\\u0003\\u001e\\u0003\\u001e\\u0003\\u001e\\u0003\\u001e\\u0003\\u001e\", \"\\u0003\\u001e\\u0003\\u001e\\u0003\\u001f\\u0003\\u001f\\u0003\\u001f\\u0003\\u001f\", \"\\u0003\\u001f\\u0003 \\u0003 \\u0003 \\u0003 \\u0003 \\u0003 \\u0003 \\u0003\", \"!\\u0003!\\u0003!\\u0003!\\u0003!\\u0003!\\u0003!\\u0003!\\u0003\\\"\\u0003\\\"\\u0007\", \"\\\"\\u0111\\n\\\"\\f\\\"\\u000e\\\"\\u0114\\u000b\\\"\\u0003#\\u0003#\\u0005#\\u0118\\n\", \"#\\u0003#\\u0003#\\u0007#\\u011c\\n#\\f#\\u000e#\\u011f\\u000b#\\u0005#\\u0121\", \"\\n#\\u0003$\\u0006$\\u0124\\n$\\r$\\u000e$\\u0125\\u0003$\\u0003$\\u0002\\u0002\", \"%\\u0003\\u0003\\u0005\\u0004\\u0007\\u0005\\t\\u0006\\u000b\\u0007\\r\\b\\u000f\", \"\\t\\u0011\\n\\u0013\\u000b\\u0015\\f\\u0017\\r\\u0019\\u000e\\u001b\\u000f\\u001d\", \"\\u0010\\u001f\\u0011!\\u0012#\\u0013%\\u0014\\'\\u0015)\\u0016+\\u0017-\\u0018\", \"/\\u00191\\u001a3\\u001b5\\u001c7\\u001d9\\u001e;\\u001f= ?!A\\\"C#E$G%\\u0003\", \"\\u0002\\u0007\\u0003\\u0002c|\\u0006\\u00022;C\\\\aac|\\u0003\\u00023;\\u0003\", \"\\u00022;\\u0004\\u0002\\u000b\\f\\\"\\\"\\u0002\\u012d\\u0002\\u0003\\u0003\\u0002\", \"\\u0002\\u0002\\u0002\\u0005\\u0003\\u0002\\u0002\\u0002\\u0002\\u0007\\u0003\\u0002\", \"\\u0002\\u0002\\u0002\\t\\u0003\\u0002\\u0002\\u0002\\u0002\\u000b\\u0003\\u0002\", \"\\u0002\\u0002\\u0002\\r\\u0003\\u0002\\u0002\\u0002\\u0002\\u000f\\u0003\\u0002\", \"\\u0002\\u0002\\u0002\\u0011\\u0003\\u0002\\u0002\\u0002\\u0002\\u0013\\u0003\\u0002\", \"\\u0002\\u0002\\u0002\\u0015\\u0003\\u0002\\u0002\\u0002\\u0002\\u0017\\u0003\\u0002\", \"\\u0002\\u0002\\u0002\\u0019\\u0003\\u0002\\u0002\\u0002\\u0002\\u001b\\u0003\\u0002\", \"\\u0002\\u0002\\u0002\\u001d\\u0003\\u0002\\u0002\\u0002\\u0002\\u001f\\u0003\\u0002\", \"\\u0002\\u0002\\u0002!\\u0003\\u0002\\u0002\\u0002\\u0002#\\u0003\\u0002\\u0002\", \"\\u0002\\u0002%\\u0003\\u0002\\u0002\\u0002\\u0002\\'\\u0003\\u0002\\u0002\\u0002\", \"\\u0002)\\u0003\\u0002\\u0002\\u0002\\u0002+\\u0003\\u0002\\u0002\\u0002\\u0002\", \"-\\u0003\\u0002\\u0002\\u0002\\u0002/\\u0003\\u0002\\u0002\\u0002\\u00021\\u0003\", \"\\u0002\\u0002\\u0002\\u00023\\u0003\\u0002\\u0002\\u0002\\u00025\\u0003\\u0002\", \"\\u0002\\u0002\\u00027\\u0003\\u0002\\u0002\\u0002\\u00029\\u0003\\u0002\\u0002\", \"\\u0002\\u0002;\\u0003\\u0002\\u0002\\u0002\\u0002=\\u0003\\u0002\\u0002\\u0002\", \"\\u0002?\\u0003\\u0002\\u0002\\u0002\\u0002A\\u0003\\u0002\\u0002\\u0002\\u0002\", \"C\\u0003\\u0002\\u0002\\u0002\\u0002E\\u0003\\u0002\\u0002\\u0002\\u0002G\\u0003\", \"\\u0002\\u0002\\u0002\\u0003I\\u0003\\u0002\\u0002\\u0002\\u0005P\\u0003\\u0002\", \"\\u0002\\u0002\\u0007R\\u0003\\u0002\\u0002\\u0002\\tY\\u0003\\u0002\\u0002\\u0002\", \"\\u000ba\\u0003\\u0002\\u0002\\u0002\\rh\\u0003\\u0002\\u0002\\u0002\\u000fj\\u0003\", \"\\u0002\\u0002\\u0002\\u0011l\\u0003\\u0002\\u0002\\u0002\\u0013n\\u0003\\u0002\", \"\\u0002\\u0002\\u0015p\\u0003\\u0002\\u0002\\u0002\\u0017{\\u0003\\u0002\\u0002\", \"\\u0002\\u0019\\u0086\\u0003\\u0002\\u0002\\u0002\\u001b\\u0091\\u0003\\u0002\\u0002\", \"\\u0002\\u001d\\u0097\\u0003\\u0002\\u0002\\u0002\\u001f\\u009f\\u0003\\u0002\\u0002\", \"\\u0002!\\u00a7\\u0003\\u0002\\u0002\\u0002#\\u00af\\u0003\\u0002\\u0002\\u0002\", \"%\\u00b6\\u0003\\u0002\\u0002\\u0002\\'\\u00c3\\u0003\\u0002\\u0002\\u0002)\\u00c5\", \"\\u0003\\u0002\\u0002\\u0002+\\u00cf\\u0003\\u0002\\u0002\\u0002-\\u00d1\\u0003\", \"\\u0002\\u0002\\u0002/\\u00d3\\u0003\\u0002\\u0002\\u00021\\u00d5\\u0003\\u0002\", \"\\u0002\\u00023\\u00dc\\u0003\\u0002\\u0002\\u00025\\u00e2\\u0003\\u0002\\u0002\", \"\\u00027\\u00e6\\u0003\\u0002\\u0002\\u00029\\u00eb\\u0003\\u0002\\u0002\\u0002\", \";\\u00f1\\u0003\\u0002\\u0002\\u0002=\\u00fa\\u0003\\u0002\\u0002\\u0002?\\u00ff\", \"\\u0003\\u0002\\u0002\\u0002A\\u0106\\u0003\\u0002\\u0002\\u0002C\\u010e\\u0003\", \"\\u0002\\u0002\\u0002E\\u0120\\u0003\\u0002\\u0002\\u0002G\\u0123\\u0003\\u0002\", \"\\u0002\\u0002IJ\\u0007Q\\u0002\\u0002JK\\u0007d\\u0002\\u0002KL\\u0007l\\u0002\", \"\\u0002LM\\u0007g\\u0002\\u0002MN\\u0007e\\u0002\\u0002NO\\u0007v\\u0002\\u0002\", \"O\\u0004\\u0003\\u0002\\u0002\\u0002PQ\\u0007?\\u0002\\u0002Q\\u0006\\u0003\\u0002\", \"\\u0002\\u0002RS\\u0007P\\u0002\\u0002ST\\u0007w\\u0002\\u0002TU\\u0007o\\u0002\", \"\\u0002UV\\u0007d\\u0002\\u0002VW\\u0007g\\u0002\\u0002WX\\u0007t\\u0002\\u0002\", \"X\\b\\u0003\\u0002\\u0002\\u0002YZ\\u0007V\\u0002\\u0002Z[\\u0007g\\u0002\\u0002\", \"[\\\\\\u0007z\\u0002\\u0002\\\\]\\u0007v\\u0002\\u0002]^\\u0007w\\u0002\\u0002^_\", \"\\u0007t\\u0002\\u0002_`\\u0007g\\u0002\\u0002`\\n\\u0003\\u0002\\u0002\\u0002\", \"ab\\u0007E\\u0002\\u0002bc\\u0007t\\u0002\\u0002cd\\u0007g\\u0002\\u0002de\\u0007\", \"c\\u0002\\u0002ef\\u0007v\\u0002\\u0002fg\\u0007g\\u0002\\u0002g\\f\\u0003\\u0002\", \"\\u0002\\u0002hi\\u0007}\\u0002\\u0002i\\u000e\\u0003\\u0002\\u0002\\u0002jk\\u0007\", \"\\u007f\\u0002\\u0002k\\u0010\\u0003\\u0002\\u0002\\u0002lm\\u0007*\\u0002\\u0002\", \"m\\u0012\\u0003\\u0002\\u0002\\u0002no\\u0007+\\u0002\\u0002o\\u0014\\u0003\\u0002\", \"\\u0002\\u0002pq\\u0007V\\u0002\\u0002qr\\u0007t\\u0002\\u0002rs\\u0007c\\u0002\", \"\\u0002st\\u0007p\\u0002\\u0002tu\\u0007u\\u0002\\u0002uv\\u0007n\\u0002\\u0002\", \"vw\\u0007c\\u0002\\u0002wx\\u0007v\\u0002\\u0002xy\\u0007g\\u0002\\u0002yz\\u0007\", \"Z\\u0002\\u0002z\\u0016\\u0003\\u0002\\u0002\\u0002{|\\u0007V\\u0002\\u0002|}\", \"\\u0007t\\u0002\\u0002}~\\u0007c\\u0002\\u0002~\\u007f\\u0007p\\u0002\\u0002\\u007f\", \"\\u0080\\u0007u\\u0002\\u0002\\u0080\\u0081\\u0007n\\u0002\\u0002\\u0081\\u0082\", \"\\u0007c\\u0002\\u0002\\u0082\\u0083\\u0007v\\u0002\\u0002\\u0083\\u0084\\u0007\", \"g\\u0002\\u0002\\u0084\\u0085\\u0007[\\u0002\\u0002\\u0085\\u0018\\u0003\\u0002\", \"\\u0002\\u0002\\u0086\\u0087\\u0007V\\u0002\\u0002\\u0087\\u0088\\u0007t\\u0002\", \"\\u0002\\u0088\\u0089\\u0007c\\u0002\\u0002\\u0089\\u008a\\u0007p\\u0002\\u0002\", \"\\u008a\\u008b\\u0007u\\u0002\\u0002\\u008b\\u008c\\u0007n\\u0002\\u0002\\u008c\", \"\\u008d\\u0007c\\u0002\\u0002\\u008d\\u008e\\u0007v\\u0002\\u0002\\u008e\\u008f\", \"\\u0007g\\u0002\\u0002\\u008f\\u0090\\u0007\\\\\\u0002\\u0002\\u0090\\u001a\\u0003\", \"\\u0002\\u0002\\u0002\\u0091\\u0092\\u0007U\\u0002\\u0002\\u0092\\u0093\\u0007\", \"e\\u0002\\u0002\\u0093\\u0094\\u0007c\\u0002\\u0002\\u0094\\u0095\\u0007n\\u0002\", \"\\u0002\\u0095\\u0096\\u0007g\\u0002\\u0002\\u0096\\u001c\\u0003\\u0002\\u0002\", \"\\u0002\\u0097\\u0098\\u0007T\\u0002\\u0002\\u0098\\u0099\\u0007q\\u0002\\u0002\", \"\\u0099\\u009a\\u0007v\\u0002\\u0002\\u009a\\u009b\\u0007c\\u0002\\u0002\\u009b\", \"\\u009c\\u0007v\\u0002\\u0002\\u009c\\u009d\\u0007g\\u0002\\u0002\\u009d\\u009e\", \"\\u0007Z\\u0002\\u0002\\u009e\\u001e\\u0003\\u0002\\u0002\\u0002\\u009f\\u00a0\", \"\\u0007T\\u0002\\u0002\\u00a0\\u00a1\\u0007q\\u0002\\u0002\\u00a1\\u00a2\\u0007\", \"v\\u0002\\u0002\\u00a2\\u00a3\\u0007c\\u0002\\u0002\\u00a3\\u00a4\\u0007v\\u0002\", \"\\u0002\\u00a4\\u00a5\\u0007g\\u0002\\u0002\\u00a5\\u00a6\\u0007[\\u0002\\u0002\", \"\\u00a6 \\u0003\\u0002\\u0002\\u0002\\u00a7\\u00a8\\u0007T\\u0002\\u0002\\u00a8\", \"\\u00a9\\u0007q\\u0002\\u0002\\u00a9\\u00aa\\u0007v\\u0002\\u0002\\u00aa\\u00ab\", \"\\u0007c\\u0002\\u0002\\u00ab\\u00ac\\u0007v\\u0002\\u0002\\u00ac\\u00ad\\u0007\", \"g\\u0002\\u0002\\u00ad\\u00ae\\u0007\\\\\\u0002\\u0002\\u00ae\\\"\\u0003\\u0002\\u0002\", \"\\u0002\\u00af\\u00b0\\u0007o\\u0002\\u0002\\u00b0\\u00b1\\u0007c\\u0002\\u0002\", \"\\u00b1\\u00b2\\u0007v\\u0002\\u0002\\u00b2\\u00b3\\u0007t\\u0002\\u0002\\u00b3\", \"\\u00b4\\u0007k\\u0002\\u0002\\u00b4\\u00b5\\u0007z\\u0002\\u0002\\u00b5$\\u0003\", \"\\u0002\\u0002\\u0002\\u00b6\\u00b7\\u0007C\\u0002\\u0002\\u00b7\\u00b8\\u0007\", \"r\\u0002\\u0002\\u00b8\\u00b9\\u0007r\\u0002\\u0002\\u00b9\\u00ba\\u0007n\\u0002\", \"\\u0002\\u00ba\\u00bb\\u0007{\\u0002\\u0002\\u00bb\\u00bc\\u0007V\\u0002\\u0002\", \"\\u00bc\\u00bd\\u0007g\\u0002\\u0002\\u00bd\\u00be\\u0007z\\u0002\\u0002\\u00be\", \"\\u00bf\\u0007v\\u0002\\u0002\\u00bf\\u00c0\\u0007w\\u0002\\u0002\\u00c0\\u00c1\", \"\\u0007t\\u0002\\u0002\\u00c1\\u00c2\\u0007g\\u0002\\u0002\\u00c2&\\u0003\\u0002\", \"\\u0002\\u0002\\u00c3\\u00c4\\u0007.\\u0002\\u0002\\u00c4(\\u0003\\u0002\\u0002\", \"\\u0002\\u00c5\\u00c6\\u0007V\\u0002\\u0002\\u00c6\\u00c7\\u0007t\\u0002\\u0002\", \"\\u00c7\\u00c8\\u0007c\\u0002\\u0002\\u00c8\\u00c9\\u0007p\\u0002\\u0002\\u00c9\", \"\\u00ca\\u0007u\\u0002\\u0002\\u00ca\\u00cb\\u0007h\\u0002\\u0002\\u00cb\\u00cc\", \"\\u0007q\\u0002\\u0002\\u00cc\\u00cd\\u0007t\\u0002\\u0002\\u00cd\\u00ce\\u0007\", \"o\\u0002\\u0002\\u00ce*\\u0003\\u0002\\u0002\\u0002\\u00cf\\u00d0\\u0007Z\\u0002\", \"\\u0002\\u00d0,\\u0003\\u0002\\u0002\\u0002\\u00d1\\u00d2\\u0007[\\u0002\\u0002\", \"\\u00d2.\\u0003\\u0002\\u0002\\u0002\\u00d3\\u00d4\\u0007\\\\\\u0002\\u0002\\u00d4\", \"0\\u0003\\u0002\\u0002\\u0002\\u00d5\\u00d6\\u0007T\\u0002\\u0002\\u00d6\\u00d7\", \"\\u0007c\\u0002\\u0002\\u00d7\\u00d8\\u0007f\\u0002\\u0002\\u00d8\\u00d9\\u0007\", \"k\\u0002\\u0002\\u00d9\\u00da\\u0007w\\u0002\\u0002\\u00da\\u00db\\u0007u\\u0002\", \"\\u0002\\u00db2\\u0003\\u0002\\u0002\\u0002\\u00dc\\u00dd\\u0007E\\u0002\\u0002\", \"\\u00dd\\u00de\\u0007q\\u0002\\u0002\\u00de\\u00df\\u0007n\\u0002\\u0002\\u00df\", \"\\u00e0\\u0007q\\u0002\\u0002\\u00e0\\u00e1\\u0007t\\u0002\\u0002\\u00e14\\u0003\", \"\\u0002\\u0002\\u0002\\u00e2\\u00e3\\u0007T\\u0002\\u0002\\u00e3\\u00e4\\u0007\", \"g\\u0002\\u0002\\u00e4\\u00e5\\u0007f\\u0002\\u0002\\u00e56\\u0003\\u0002\\u0002\", \"\\u0002\\u00e6\\u00e7\\u0007D\\u0002\\u0002\\u00e7\\u00e8\\u0007n\\u0002\\u0002\", \"\\u00e8\\u00e9\\u0007w\\u0002\\u0002\\u00e9\\u00ea\\u0007g\\u0002\\u0002\\u00ea\", \"8\\u0003\\u0002\\u0002\\u0002\\u00eb\\u00ec\\u0007D\\u0002\\u0002\\u00ec\\u00ed\", \"\\u0007n\\u0002\\u0002\\u00ed\\u00ee\\u0007c\\u0002\\u0002\\u00ee\\u00ef\\u0007\", \"e\\u0002\\u0002\\u00ef\\u00f0\\u0007m\\u0002\\u0002\\u00f0:\\u0003\\u0002\\u0002\", \"\\u0002\\u00f1\\u00f2\\u0007V\\u0002\\u0002\\u00f2\\u00f3\\u0007t\\u0002\\u0002\", \"\\u00f3\\u00f4\\u0007k\\u0002\\u0002\\u00f4\\u00f5\\u0007c\\u0002\\u0002\\u00f5\", \"\\u00f6\\u0007p\\u0002\\u0002\\u00f6\\u00f7\\u0007i\\u0002\\u0002\\u00f7\\u00f8\", \"\\u0007n\\u0002\\u0002\\u00f8\\u00f9\\u0007g\\u0002\\u0002\\u00f9<\\u0003\\u0002\", \"\\u0002\\u0002\\u00fa\\u00fb\\u0007E\\u0002\\u0002\\u00fb\\u00fc\\u0007w\\u0002\", \"\\u0002\\u00fc\\u00fd\\u0007d\\u0002\\u0002\\u00fd\\u00fe\\u0007g\\u0002\\u0002\", \"\\u00fe>\\u0003\\u0002\\u0002\\u0002\\u00ff\\u0100\\u0007U\\u0002\\u0002\\u0100\", \"\\u0101\\u0007r\\u0002\\u0002\\u0101\\u0102\\u0007j\\u0002\\u0002\\u0102\\u0103\", \"\\u0007g\\u0002\\u0002\\u0103\\u0104\\u0007t\\u0002\\u0002\\u0104\\u0105\\u0007\", \"g\\u0002\\u0002\\u0105@\\u0003\\u0002\\u0002\\u0002\\u0106\\u0107\\u0007R\\u0002\", \"\\u0002\\u0107\\u0108\\u0007{\\u0002\\u0002\\u0108\\u0109\\u0007t\\u0002\\u0002\", \"\\u0109\\u010a\\u0007c\\u0002\\u0002\\u010a\\u010b\\u0007o\\u0002\\u0002\\u010b\", \"\\u010c\\u0007k\\u0002\\u0002\\u010c\\u010d\\u0007f\\u0002\\u0002\\u010dB\\u0003\", \"\\u0002\\u0002\\u0002\\u010e\\u0112\\t\\u0002\\u0002\\u0002\\u010f\\u0111\\t\\u0003\", \"\\u0002\\u0002\\u0110\\u010f\\u0003\\u0002\\u0002\\u0002\\u0111\\u0114\\u0003\\u0002\", \"\\u0002\\u0002\\u0112\\u0110\\u0003\\u0002\\u0002\\u0002\\u0112\\u0113\\u0003\\u0002\", \"\\u0002\\u0002\\u0113D\\u0003\\u0002\\u0002\\u0002\\u0114\\u0112\\u0003\\u0002\", \"\\u0002\\u0002\\u0115\\u0121\\u00072\\u0002\\u0002\\u0116\\u0118\\u0007/\\u0002\", \"\\u0002\\u0117\\u0116\\u0003\\u0002\\u0002\\u0002\\u0117\\u0118\\u0003\\u0002\\u0002\", \"\\u0002\\u0118\\u0119\\u0003\\u0002\\u0002\\u0002\\u0119\\u011d\\t\\u0004\\u0002\", \"\\u0002\\u011a\\u011c\\t\\u0005\\u0002\\u0002\\u011b\\u011a\\u0003\\u0002\\u0002\", \"\\u0002\\u011c\\u011f\\u0003\\u0002\\u0002\\u0002\\u011d\\u011b\\u0003\\u0002\\u0002\", \"\\u0002\\u011d\\u011e\\u0003\\u0002\\u0002\\u0002\\u011e\\u0121\\u0003\\u0002\\u0002\", \"\\u0002\\u011f\\u011d\\u0003\\u0002\\u0002\\u0002\\u0120\\u0115\\u0003\\u0002\\u0002\", \"\\u0002\\u0120\\u0117\\u0003\\u0002\\u0002\\u0002\\u0121F\\u0003\\u0002\\u0002\", \"\\u0002\\u0122\\u0124\\t\\u0006\\u0002\\u0002\\u0123\\u0122\\u0003\\u0002\\u0002\", \"\\u0002\\u0124\\u0125\\u0003\\u0002\\u0002\\u0002\\u0125\\u0123\\u0003\\u0002\\u0002\", \"\\u0002\\u0125\\u0126\\u0003\\u0002\\u0002\\u0002\\u0126\\u0127\\u0003\\u0002\\u0002\", \"\\u0002\\u0127\\u0128\\b$\\u0002\\u0002\\u0128H\\u0003\\u0002\\u0002\\u0002\\b\\u0002\", \"\\u0112\\u0117\\u011d\\u0120\\u0125\\u0003\\b\\u0002\\u0002\"].join(\"\");\nconst atn = new antlr4__WEBPACK_IMPORTED_MODULE_0__.atn.ATNDeserializer().deserialize(serializedATN);\nconst decisionsToDFA = atn.decisionToState.map((ds, index) => new antlr4__WEBPACK_IMPORTED_MODULE_0__.dfa.DFA(ds, index));\nclass simplerlangLexer extends antlr4__WEBPACK_IMPORTED_MODULE_0__.Lexer {\n  static grammarFileName = \"simplerlang.g4\";\n  static channelNames = [\"DEFAULT_TOKEN_CHANNEL\", \"HIDDEN\"];\n  static modeNames = [\"DEFAULT_MODE\"];\n  static literalNames = [null, \"'Object'\", \"'='\", \"'Number'\", \"'Texture'\", \"'Create'\", \"'{'\", \"'}'\", \"'('\", \"')'\", \"'TranslateX'\", \"'TranslateY'\", \"'TranslateZ'\", \"'Scale'\", \"'RotateX'\", \"'RotateY'\", \"'RotateZ'\", \"'matrix'\", \"'ApplyTexture'\", \"','\", \"'Transform'\", \"'X'\", \"'Y'\", \"'Z'\", \"'Radius'\", \"'Color'\", \"'Red'\", \"'Blue'\", \"'Black'\", \"'Triangle'\", \"'Cube'\", \"'Sphere'\", \"'Pyramid'\"];\n  static symbolicNames = [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, \"ID\", \"NUM\", \"WS\"];\n  static ruleNames = [\"T__0\", \"T__1\", \"T__2\", \"T__3\", \"T__4\", \"T__5\", \"T__6\", \"T__7\", \"T__8\", \"T__9\", \"T__10\", \"T__11\", \"T__12\", \"T__13\", \"T__14\", \"T__15\", \"T__16\", \"T__17\", \"T__18\", \"T__19\", \"T__20\", \"T__21\", \"T__22\", \"T__23\", \"T__24\", \"T__25\", \"T__26\", \"T__27\", \"T__28\", \"T__29\", \"T__30\", \"T__31\", \"ID\", \"NUM\", \"WS\"];\n\n  constructor(input) {\n    super(input);\n    this._interp = new antlr4__WEBPACK_IMPORTED_MODULE_0__.atn.LexerATNSimulator(this, atn, decisionsToDFA, new antlr4__WEBPACK_IMPORTED_MODULE_0__.PredictionContextCache());\n  }\n\n  get atn() {\n    return atn;\n  }\n\n}\nsimplerlangLexer.EOF = antlr4__WEBPACK_IMPORTED_MODULE_0__.Token.EOF;\nsimplerlangLexer.T__0 = 1;\nsimplerlangLexer.T__1 = 2;\nsimplerlangLexer.T__2 = 3;\nsimplerlangLexer.T__3 = 4;\nsimplerlangLexer.T__4 = 5;\nsimplerlangLexer.T__5 = 6;\nsimplerlangLexer.T__6 = 7;\nsimplerlangLexer.T__7 = 8;\nsimplerlangLexer.T__8 = 9;\nsimplerlangLexer.T__9 = 10;\nsimplerlangLexer.T__10 = 11;\nsimplerlangLexer.T__11 = 12;\nsimplerlangLexer.T__12 = 13;\nsimplerlangLexer.T__13 = 14;\nsimplerlangLexer.T__14 = 15;\nsimplerlangLexer.T__15 = 16;\nsimplerlangLexer.T__16 = 17;\nsimplerlangLexer.T__17 = 18;\nsimplerlangLexer.T__18 = 19;\nsimplerlangLexer.T__19 = 20;\nsimplerlangLexer.T__20 = 21;\nsimplerlangLexer.T__21 = 22;\nsimplerlangLexer.T__22 = 23;\nsimplerlangLexer.T__23 = 24;\nsimplerlangLexer.T__24 = 25;\nsimplerlangLexer.T__25 = 26;\nsimplerlangLexer.T__26 = 27;\nsimplerlangLexer.T__27 = 28;\nsimplerlangLexer.T__28 = 29;\nsimplerlangLexer.T__29 = 30;\nsimplerlangLexer.T__30 = 31;\nsimplerlangLexer.T__31 = 32;\nsimplerlangLexer.ID = 33;\nsimplerlangLexer.NUM = 34;\nsimplerlangLexer.WS = 35;\n\n//# sourceURL=webpack://test/./simplerlangLexer.js?");

/***/ }),

/***/ "./simplerlangListener.js":
/*!********************************!*\
  !*** ./simplerlangListener.js ***!
  \********************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   \"default\": () => (/* binding */ simplerlangListener)\n/* harmony export */ });\n/* harmony import */ var antlr4__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! antlr4 */ \"./node_modules/antlr4/src/antlr4/index.js\");\n// Generated from simplerlang.g4 by ANTLR 4.9.2\n// jshint ignore: start\n // This class defines a complete listener for a parse tree produced by simplerlangParser.\n\nclass simplerlangListener extends antlr4__WEBPACK_IMPORTED_MODULE_0__.tree.ParseTreeListener {\n  // Enter a parse tree produced by simplerlangParser#prog.\n  constructor() {\n    super();\n    this.state = {};\n    this.matrices = {};\n  }\n\n  enterProg(ctx) {} // Exit a parse tree produced by simplerlangParser#prog.\n\n\n  exitProg(ctx) {} // Enter a parse tree produced by simplerlangParser#statement_list.\n\n\n  enterStatement_list(ctx) {} // Exit a parse tree produced by simplerlangParser#statement_list.\n\n\n  exitStatement_list(ctx) {} // Enter a parse tree produced by simplerlangParser#statement.\n\n\n  enterStatement(ctx) {} // Exit a parse tree produced by simplerlangParser#statement.\n\n\n  exitStatement(ctx) {} // Enter a parse tree produced by simplerlangParser#var_declaration.\n\n\n  enterVar_declaration(ctx) {\n    const objectId = ctx.getChild(1).getText();\n    const object = {};\n    const pos = {};\n    const color = ctx.getChild(3).getChild(3).getChild(4).getText().split(\"=\")[1];\n    pos.X = ctx.getChild(3).getChild(3).getChild(0).getText().split(\"=\")[1];\n    pos.Y = ctx.getChild(3).getChild(3).getChild(1).getText().split(\"=\")[1];\n    pos.Z = ctx.getChild(3).getChild(3).getChild(2).getText().split(\"=\")[1];\n    object.scale = ctx.getChild(3).getChild(3).getChild(3).getText().split(\"=\")[1];\n    object.type = ctx.getChild(3).getChild(1).getText();\n    object.pos = pos;\n    object.color = color;\n    this.state[objectId] = object;\n  }\n\n  enterColor_type(ctx) {}\n\n  exitColor_type(ctx) {} // Exit a parse tree produced by simplerlangParser#var_declaration.\n\n\n  exitVar_declaration(ctx) {} // Enter a parse tree produced by simplerlangParser#create_command.\n\n\n  enterCreate_command(ctx) {} // Exit a parse tree produced by simplerlangParser#create_command.\n\n\n  exitCreate_command(ctx) {} // Enter a parse tree produced by simplerlangParser#transform_command.\n\n\n  enterTransform_command(ctx) {} // Exit a parse tree produced by simplerlangParser#transform_command.\n\n\n  exitTransform_command(ctx) {} // Enter a parse tree produced by simplerlangParser#transform_function.\n\n\n  enterTransform_function(ctx) {} // Exit a parse tree produced by simplerlangParser#transform_function.\n\n\n  exitTransform_function(ctx) {} // Enter a parse tree produced by simplerlangParser#transform_matrix_declaration.\n\n\n  enterTransform_matrix_declaration(ctx) {\n    const matrixId = ctx.getChild(1).getText();\n    const transformX = ctx.getChild(3).getChild(0).getChild(2).getText();\n    const transformY = ctx.getChild(3).getChild(1).getChild(2).getText();\n    const transformZ = ctx.getChild(3).getChild(2).getChild(2).getText();\n    this.matrices[matrixId] = {\n      transformX,\n      transformY,\n      transformZ\n    };\n  } // Exit a parse tree produced by simplerlangParser#transform_matrix_declaration.\n\n\n  exitTransform_matrix_declaration(ctx) {} // Enter a parse tree produced by simplerlangParser#transform_command_list.\n\n\n  enterTransform_command_list(ctx) {} // Exit a parse tree produced by simplerlangParser#transform_command_list.\n\n\n  exitTransform_command_list(ctx) {} // Enter a parse tree produced by simplerlangParser#apply_texture_command.\n\n\n  enterApply_texture_command(ctx) {} // Exit a parse tree produced by simplerlangParser#apply_texture_command.\n\n\n  exitApply_texture_command(ctx) {} // Enter a parse tree produced by simplerlangParser#apply_transform_command.\n\n\n  enterApply_transform_command(ctx) {\n    const objId = ctx.getChild(2).getText();\n    const matrixId = ctx.getChild(4).getText();\n    this.state[objId].transform = this.matrices[matrixId];\n  } // Exit a parse tree produced by simplerlangParser#apply_transform_command.\n\n\n  exitApply_transform_command(ctx) {} // Enter a parse tree produced by simplerlangParser#object_props.\n\n\n  enterObject_props(ctx) {} // Exit a parse tree produced by simplerlangParser#object_props.\n\n\n  exitObject_props(ctx) {} // Enter a parse tree produced by simplerlangParser#object_prop.\n\n\n  enterObject_prop(ctx) {} // Exit a parse tree produced by simplerlangParser#object_prop.\n\n\n  exitObject_prop(ctx) {} // Enter a parse tree produced by simplerlangParser#prop_name.\n\n\n  enterProp_name(ctx) {} // Exit a parse tree produced by simplerlangParser#prop_name.\n\n\n  exitProp_name(ctx) {} // Enter a parse tree produced by simplerlangParser#object_type.\n\n\n  enterObject_type(ctx) {} // Exit a parse tree produced by simplerlangParser#object_type.\n\n\n  exitObject_type(ctx) {}\n\n  getState() {\n    return this.state;\n  }\n\n}\n\n//# sourceURL=webpack://test/./simplerlangListener.js?");

/***/ }),

/***/ "./simplerlangParser.js":
/*!******************************!*\
  !*** ./simplerlangParser.js ***!
  \******************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

"use strict";
eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   \"default\": () => (/* binding */ simplerlangParser)\n/* harmony export */ });\n/* harmony import */ var antlr4__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! antlr4 */ \"./node_modules/antlr4/src/antlr4/index.js\");\n/* harmony import */ var _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./simplerlangListener.js */ \"./simplerlangListener.js\");\n// Generated from simplerlang.g4 by ANTLR 4.9.2\n// jshint ignore: start\n\n\nconst serializedATN = [\"\\u0003\\u608b\\ua72a\\u8133\\ub9ed\\u417c\\u3be7\\u7786\", \"\\u5964\\u0003%\\u0086\\u0004\\u0002\\t\\u0002\\u0004\\u0003\\t\\u0003\\u0004\\u0004\", \"\\t\\u0004\\u0004\\u0005\\t\\u0005\\u0004\\u0006\\t\\u0006\\u0004\\u0007\\t\\u0007\", \"\\u0004\\b\\t\\b\\u0004\\t\\t\\t\\u0004\\n\\t\\n\\u0004\\u000b\\t\\u000b\\u0004\\f\\t\\f\", \"\\u0004\\r\\t\\r\\u0004\\u000e\\t\\u000e\\u0004\\u000f\\t\\u000f\\u0004\\u0010\\t\\u0010\", \"\\u0004\\u0011\\t\\u0011\\u0003\\u0002\\u0006\\u0002$\\n\\u0002\\r\\u0002\\u000e\", \"\\u0002%\\u0003\\u0002\\u0003\\u0002\\u0003\\u0003\\u0006\\u0003+\\n\\u0003\\r\\u0003\", \"\\u000e\\u0003,\\u0003\\u0004\\u0003\\u0004\\u0003\\u0004\\u0003\\u0004\\u0005\", \"\\u00043\\n\\u0004\\u0003\\u0005\\u0003\\u0005\\u0003\\u0005\\u0003\\u0005\\u0003\", \"\\u0005\\u0003\\u0005\\u0003\\u0005\\u0003\\u0005\\u0003\\u0005\\u0003\\u0005\\u0003\", \"\\u0005\\u0003\\u0005\\u0005\\u0005A\\n\\u0005\\u0003\\u0006\\u0003\\u0006\\u0003\", \"\\u0006\\u0003\\u0006\\u0003\\u0006\\u0003\\u0006\\u0003\\u0006\\u0003\\u0006\\u0005\", \"\\u0006K\\n\\u0006\\u0003\\u0007\\u0003\\u0007\\u0003\\u0007\\u0003\\u0007\\u0003\", \"\\u0007\\u0003\\b\\u0003\\b\\u0003\\t\\u0003\\t\\u0003\\t\\u0003\\t\\u0003\\t\\u0003\", \"\\t\\u0003\\n\\u0006\\n[\\n\\n\\r\\n\\u000e\\n\\\\\\u0003\\u000b\\u0003\\u000b\\u0003\", \"\\u000b\\u0003\\u000b\\u0003\\u000b\\u0003\\u000b\\u0003\\u000b\\u0003\\f\\u0003\", \"\\f\\u0003\\f\\u0003\\f\\u0003\\f\\u0003\\f\\u0003\\f\\u0003\\r\\u0006\\rn\\n\\r\\r\\r\", \"\\u000e\\ro\\u0003\\u000e\\u0003\\u000e\\u0003\\u000e\\u0003\\u000e\\u0003\\u000e\", \"\\u0003\\u000e\\u0003\\u000e\\u0003\\u000e\\u0003\\u000e\\u0003\\u000e\\u0003\\u000e\", \"\\u0003\\u000e\\u0005\\u000e~\\n\\u000e\\u0003\\u000f\\u0003\\u000f\\u0003\\u0010\", \"\\u0003\\u0010\\u0003\\u0011\\u0003\\u0011\\u0003\\u0011\\u0002\\u0002\\u0012\\u0002\", \"\\u0004\\u0006\\b\\n\\f\\u000e\\u0010\\u0012\\u0014\\u0016\\u0018\\u001a\\u001c\\u001e\", \" \\u0002\\u0007\\u0003\\u0002#$\\u0003\\u0002\\f\\u0012\\u0004\\u0002\\u000f\\u000f\", \"\\u0017\\u001b\\u0003\\u0002\\u001c\\u001e\\u0003\\u0002\\u001f\\\"\\u0002\\u0081\", \"\\u0002#\\u0003\\u0002\\u0002\\u0002\\u0004*\\u0003\\u0002\\u0002\\u0002\\u0006\", \"2\\u0003\\u0002\\u0002\\u0002\\b@\\u0003\\u0002\\u0002\\u0002\\nJ\\u0003\\u0002\", \"\\u0002\\u0002\\fL\\u0003\\u0002\\u0002\\u0002\\u000eQ\\u0003\\u0002\\u0002\\u0002\", \"\\u0010S\\u0003\\u0002\\u0002\\u0002\\u0012Z\\u0003\\u0002\\u0002\\u0002\\u0014\", \"^\\u0003\\u0002\\u0002\\u0002\\u0016e\\u0003\\u0002\\u0002\\u0002\\u0018m\\u0003\", \"\\u0002\\u0002\\u0002\\u001a}\\u0003\\u0002\\u0002\\u0002\\u001c\\u007f\\u0003\", \"\\u0002\\u0002\\u0002\\u001e\\u0081\\u0003\\u0002\\u0002\\u0002 \\u0083\\u0003\", \"\\u0002\\u0002\\u0002\\\"$\\u0005\\u0004\\u0003\\u0002#\\\"\\u0003\\u0002\\u0002\\u0002\", \"$%\\u0003\\u0002\\u0002\\u0002%#\\u0003\\u0002\\u0002\\u0002%&\\u0003\\u0002\\u0002\", \"\\u0002&\\'\\u0003\\u0002\\u0002\\u0002\\'(\\u0007\\u0002\\u0002\\u0003(\\u0003\", \"\\u0003\\u0002\\u0002\\u0002)+\\u0005\\u0006\\u0004\\u0002*)\\u0003\\u0002\\u0002\", \"\\u0002+,\\u0003\\u0002\\u0002\\u0002,*\\u0003\\u0002\\u0002\\u0002,-\\u0003\\u0002\", \"\\u0002\\u0002-\\u0005\\u0003\\u0002\\u0002\\u0002.3\\u0005\\b\\u0005\\u0002/3\", \"\\u0005\\u0010\\t\\u000203\\u0005\\u0014\\u000b\\u000213\\u0005\\u0016\\f\\u0002\", \"2.\\u0003\\u0002\\u0002\\u00022/\\u0003\\u0002\\u0002\\u000220\\u0003\\u0002\\u0002\", \"\\u000221\\u0003\\u0002\\u0002\\u00023\\u0007\\u0003\\u0002\\u0002\\u000245\\u0007\", \"\\u0003\\u0002\\u000256\\u0007#\\u0002\\u000267\\u0007\\u0004\\u0002\\u00027A\", \"\\u0005\\n\\u0006\\u000289\\u0007\\u0005\\u0002\\u00029:\\u0007#\\u0002\\u0002\", \":;\\u0007\\u0004\\u0002\\u0002;A\\u0007$\\u0002\\u0002<=\\u0007\\u0006\\u0002\", \"\\u0002=>\\u0007#\\u0002\\u0002>?\\u0007\\u0004\\u0002\\u0002?A\\u0005\\n\\u0006\", \"\\u0002@4\\u0003\\u0002\\u0002\\u0002@8\\u0003\\u0002\\u0002\\u0002@<\\u0003\\u0002\", \"\\u0002\\u0002A\\t\\u0003\\u0002\\u0002\\u0002BC\\u0007\\u0007\\u0002\\u0002CK\", \"\\u0005 \\u0011\\u0002DE\\u0007\\u0007\\u0002\\u0002EF\\u0005 \\u0011\\u0002F\", \"G\\u0007\\b\\u0002\\u0002GH\\u0005\\u0018\\r\\u0002HI\\u0007\\t\\u0002\\u0002IK\", \"\\u0003\\u0002\\u0002\\u0002JB\\u0003\\u0002\\u0002\\u0002JD\\u0003\\u0002\\u0002\", \"\\u0002K\\u000b\\u0003\\u0002\\u0002\\u0002LM\\u0005\\u000e\\b\\u0002MN\\u0007\", \"\\n\\u0002\\u0002NO\\t\\u0002\\u0002\\u0002OP\\u0007\\u000b\\u0002\\u0002P\\r\\u0003\", \"\\u0002\\u0002\\u0002QR\\t\\u0003\\u0002\\u0002R\\u000f\\u0003\\u0002\\u0002\\u0002\", \"ST\\u0007\\u0013\\u0002\\u0002TU\\u0007#\\u0002\\u0002UV\\u0007\\b\\u0002\\u0002\", \"VW\\u0005\\u0012\\n\\u0002WX\\u0007\\t\\u0002\\u0002X\\u0011\\u0003\\u0002\\u0002\", \"\\u0002Y[\\u0005\\f\\u0007\\u0002ZY\\u0003\\u0002\\u0002\\u0002[\\\\\\u0003\\u0002\", \"\\u0002\\u0002\\\\Z\\u0003\\u0002\\u0002\\u0002\\\\]\\u0003\\u0002\\u0002\\u0002]\", \"\\u0013\\u0003\\u0002\\u0002\\u0002^_\\u0007\\u0014\\u0002\\u0002_`\\u0007\\n\\u0002\", \"\\u0002`a\\u0007#\\u0002\\u0002ab\\u0007\\u0015\\u0002\\u0002bc\\u0007#\\u0002\", \"\\u0002cd\\u0007\\u000b\\u0002\\u0002d\\u0015\\u0003\\u0002\\u0002\\u0002ef\\u0007\", \"\\u0016\\u0002\\u0002fg\\u0007\\n\\u0002\\u0002gh\\u0007#\\u0002\\u0002hi\\u0007\", \"\\u0015\\u0002\\u0002ij\\u0007#\\u0002\\u0002jk\\u0007\\u000b\\u0002\\u0002k\\u0017\", \"\\u0003\\u0002\\u0002\\u0002ln\\u0005\\u001a\\u000e\\u0002ml\\u0003\\u0002\\u0002\", \"\\u0002no\\u0003\\u0002\\u0002\\u0002om\\u0003\\u0002\\u0002\\u0002op\\u0003\\u0002\", \"\\u0002\\u0002p\\u0019\\u0003\\u0002\\u0002\\u0002qr\\u0005\\u001c\\u000f\\u0002\", \"rs\\u0007\\u0004\\u0002\\u0002st\\u0007#\\u0002\\u0002t~\\u0003\\u0002\\u0002\", \"\\u0002uv\\u0005\\u001c\\u000f\\u0002vw\\u0007\\u0004\\u0002\\u0002wx\\u0007$\", \"\\u0002\\u0002x~\\u0003\\u0002\\u0002\\u0002yz\\u0005\\u001c\\u000f\\u0002z{\\u0007\", \"\\u0004\\u0002\\u0002{|\\u0005\\u001e\\u0010\\u0002|~\\u0003\\u0002\\u0002\\u0002\", \"}q\\u0003\\u0002\\u0002\\u0002}u\\u0003\\u0002\\u0002\\u0002}y\\u0003\\u0002\\u0002\", \"\\u0002~\\u001b\\u0003\\u0002\\u0002\\u0002\\u007f\\u0080\\t\\u0004\\u0002\\u0002\", \"\\u0080\\u001d\\u0003\\u0002\\u0002\\u0002\\u0081\\u0082\\t\\u0005\\u0002\\u0002\", \"\\u0082\\u001f\\u0003\\u0002\\u0002\\u0002\\u0083\\u0084\\t\\u0006\\u0002\\u0002\", \"\\u0084!\\u0003\\u0002\\u0002\\u0002\\n%,2@J\\\\o}\"].join(\"\");\nconst atn = new antlr4__WEBPACK_IMPORTED_MODULE_0__.atn.ATNDeserializer().deserialize(serializedATN);\nconst decisionsToDFA = atn.decisionToState.map((ds, index) => new antlr4__WEBPACK_IMPORTED_MODULE_0__.dfa.DFA(ds, index));\nconst sharedContextCache = new antlr4__WEBPACK_IMPORTED_MODULE_0__.PredictionContextCache();\nclass simplerlangParser extends antlr4__WEBPACK_IMPORTED_MODULE_0__.Parser {\n  static grammarFileName = \"simplerlang.g4\";\n  static literalNames = [null, \"'Object'\", \"'='\", \"'Number'\", \"'Texture'\", \"'Create'\", \"'{'\", \"'}'\", \"'('\", \"')'\", \"'TranslateX'\", \"'TranslateY'\", \"'TranslateZ'\", \"'Scale'\", \"'RotateX'\", \"'RotateY'\", \"'RotateZ'\", \"'matrix'\", \"'ApplyTexture'\", \"','\", \"'Transform'\", \"'X'\", \"'Y'\", \"'Z'\", \"'Radius'\", \"'Color'\", \"'Red'\", \"'Blue'\", \"'Black'\", \"'Triangle'\", \"'Cube'\", \"'Sphere'\", \"'Pyramid'\"];\n  static symbolicNames = [null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, \"ID\", \"NUM\", \"WS\"];\n  static ruleNames = [\"prog\", \"statement_list\", \"statement\", \"var_declaration\", \"create_command\", \"transform_command\", \"transform_function\", \"transform_matrix_declaration\", \"transform_command_list\", \"apply_texture_command\", \"apply_transform_command\", \"object_props\", \"object_prop\", \"prop_name\", \"color_type\", \"object_type\"];\n\n  constructor(input) {\n    super(input);\n    this._interp = new antlr4__WEBPACK_IMPORTED_MODULE_0__.atn.ParserATNSimulator(this, atn, decisionsToDFA, sharedContextCache);\n    this.ruleNames = simplerlangParser.ruleNames;\n    this.literalNames = simplerlangParser.literalNames;\n    this.symbolicNames = simplerlangParser.symbolicNames;\n  }\n\n  get atn() {\n    return atn;\n  }\n\n  prog() {\n    let localctx = new ProgContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 0, simplerlangParser.RULE_prog);\n    var _la = 0; // Token type\n\n    try {\n      this.enterOuterAlt(localctx, 1);\n      this.state = 33;\n\n      this._errHandler.sync(this);\n\n      _la = this._input.LA(1);\n\n      do {\n        this.state = 32;\n        this.statement_list();\n        this.state = 35;\n\n        this._errHandler.sync(this);\n\n        _la = this._input.LA(1);\n      } while ((_la & ~0x1f) == 0 && (1 << _la & (1 << simplerlangParser.T__0 | 1 << simplerlangParser.T__2 | 1 << simplerlangParser.T__3 | 1 << simplerlangParser.T__16 | 1 << simplerlangParser.T__17 | 1 << simplerlangParser.T__19)) !== 0);\n\n      this.state = 37;\n      this.match(simplerlangParser.EOF);\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  statement_list() {\n    let localctx = new Statement_listContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 2, simplerlangParser.RULE_statement_list);\n\n    try {\n      this.enterOuterAlt(localctx, 1);\n      this.state = 40;\n\n      this._errHandler.sync(this);\n\n      var _alt = 1;\n\n      do {\n        switch (_alt) {\n          case 1:\n            this.state = 39;\n            this.statement();\n            break;\n\n          default:\n            throw new antlr4__WEBPACK_IMPORTED_MODULE_0__.error.NoViableAltException(this);\n        }\n\n        this.state = 42;\n\n        this._errHandler.sync(this);\n\n        _alt = this._interp.adaptivePredict(this._input, 1, this._ctx);\n      } while (_alt != 2 && _alt != antlr4__WEBPACK_IMPORTED_MODULE_0__.atn.ATN.INVALID_ALT_NUMBER);\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  statement() {\n    let localctx = new StatementContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 4, simplerlangParser.RULE_statement);\n\n    try {\n      this.state = 48;\n\n      this._errHandler.sync(this);\n\n      switch (this._input.LA(1)) {\n        case simplerlangParser.T__0:\n        case simplerlangParser.T__2:\n        case simplerlangParser.T__3:\n          this.enterOuterAlt(localctx, 1);\n          this.state = 44;\n          this.var_declaration();\n          break;\n\n        case simplerlangParser.T__16:\n          this.enterOuterAlt(localctx, 2);\n          this.state = 45;\n          this.transform_matrix_declaration();\n          break;\n\n        case simplerlangParser.T__17:\n          this.enterOuterAlt(localctx, 3);\n          this.state = 46;\n          this.apply_texture_command();\n          break;\n\n        case simplerlangParser.T__19:\n          this.enterOuterAlt(localctx, 4);\n          this.state = 47;\n          this.apply_transform_command();\n          break;\n\n        default:\n          throw new antlr4__WEBPACK_IMPORTED_MODULE_0__.error.NoViableAltException(this);\n      }\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  var_declaration() {\n    let localctx = new Var_declarationContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 6, simplerlangParser.RULE_var_declaration);\n\n    try {\n      this.state = 62;\n\n      this._errHandler.sync(this);\n\n      switch (this._input.LA(1)) {\n        case simplerlangParser.T__0:\n          this.enterOuterAlt(localctx, 1);\n          this.state = 50;\n          this.match(simplerlangParser.T__0);\n          this.state = 51;\n          this.match(simplerlangParser.ID);\n          this.state = 52;\n          this.match(simplerlangParser.T__1);\n          this.state = 53;\n          this.create_command();\n          break;\n\n        case simplerlangParser.T__2:\n          this.enterOuterAlt(localctx, 2);\n          this.state = 54;\n          this.match(simplerlangParser.T__2);\n          this.state = 55;\n          this.match(simplerlangParser.ID);\n          this.state = 56;\n          this.match(simplerlangParser.T__1);\n          this.state = 57;\n          this.match(simplerlangParser.NUM);\n          break;\n\n        case simplerlangParser.T__3:\n          this.enterOuterAlt(localctx, 3);\n          this.state = 58;\n          this.match(simplerlangParser.T__3);\n          this.state = 59;\n          this.match(simplerlangParser.ID);\n          this.state = 60;\n          this.match(simplerlangParser.T__1);\n          this.state = 61;\n          this.create_command();\n          break;\n\n        default:\n          throw new antlr4__WEBPACK_IMPORTED_MODULE_0__.error.NoViableAltException(this);\n      }\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  create_command() {\n    let localctx = new Create_commandContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 8, simplerlangParser.RULE_create_command);\n\n    try {\n      this.state = 72;\n\n      this._errHandler.sync(this);\n\n      var la_ = this._interp.adaptivePredict(this._input, 4, this._ctx);\n\n      switch (la_) {\n        case 1:\n          this.enterOuterAlt(localctx, 1);\n          this.state = 64;\n          this.match(simplerlangParser.T__4);\n          this.state = 65;\n          this.object_type();\n          break;\n\n        case 2:\n          this.enterOuterAlt(localctx, 2);\n          this.state = 66;\n          this.match(simplerlangParser.T__4);\n          this.state = 67;\n          this.object_type();\n          this.state = 68;\n          this.match(simplerlangParser.T__5);\n          this.state = 69;\n          this.object_props();\n          this.state = 70;\n          this.match(simplerlangParser.T__6);\n          break;\n      }\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  transform_command() {\n    let localctx = new Transform_commandContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 10, simplerlangParser.RULE_transform_command);\n    var _la = 0; // Token type\n\n    try {\n      this.enterOuterAlt(localctx, 1);\n      this.state = 74;\n      this.transform_function();\n      this.state = 75;\n      this.match(simplerlangParser.T__7);\n      this.state = 76;\n      _la = this._input.LA(1);\n\n      if (!(_la === simplerlangParser.ID || _la === simplerlangParser.NUM)) {\n        this._errHandler.recoverInline(this);\n      } else {\n        this._errHandler.reportMatch(this);\n\n        this.consume();\n      }\n\n      this.state = 77;\n      this.match(simplerlangParser.T__8);\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  transform_function() {\n    let localctx = new Transform_functionContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 12, simplerlangParser.RULE_transform_function);\n    var _la = 0; // Token type\n\n    try {\n      this.enterOuterAlt(localctx, 1);\n      this.state = 79;\n      _la = this._input.LA(1);\n\n      if (!((_la & ~0x1f) == 0 && (1 << _la & (1 << simplerlangParser.T__9 | 1 << simplerlangParser.T__10 | 1 << simplerlangParser.T__11 | 1 << simplerlangParser.T__12 | 1 << simplerlangParser.T__13 | 1 << simplerlangParser.T__14 | 1 << simplerlangParser.T__15)) !== 0)) {\n        this._errHandler.recoverInline(this);\n      } else {\n        this._errHandler.reportMatch(this);\n\n        this.consume();\n      }\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  transform_matrix_declaration() {\n    let localctx = new Transform_matrix_declarationContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 14, simplerlangParser.RULE_transform_matrix_declaration);\n\n    try {\n      this.enterOuterAlt(localctx, 1);\n      this.state = 81;\n      this.match(simplerlangParser.T__16);\n      this.state = 82;\n      this.match(simplerlangParser.ID);\n      this.state = 83;\n      this.match(simplerlangParser.T__5);\n      this.state = 84;\n      this.transform_command_list();\n      this.state = 85;\n      this.match(simplerlangParser.T__6);\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  transform_command_list() {\n    let localctx = new Transform_command_listContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 16, simplerlangParser.RULE_transform_command_list);\n    var _la = 0; // Token type\n\n    try {\n      this.enterOuterAlt(localctx, 1);\n      this.state = 88;\n\n      this._errHandler.sync(this);\n\n      _la = this._input.LA(1);\n\n      do {\n        this.state = 87;\n        this.transform_command();\n        this.state = 90;\n\n        this._errHandler.sync(this);\n\n        _la = this._input.LA(1);\n      } while ((_la & ~0x1f) == 0 && (1 << _la & (1 << simplerlangParser.T__9 | 1 << simplerlangParser.T__10 | 1 << simplerlangParser.T__11 | 1 << simplerlangParser.T__12 | 1 << simplerlangParser.T__13 | 1 << simplerlangParser.T__14 | 1 << simplerlangParser.T__15)) !== 0);\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  apply_texture_command() {\n    let localctx = new Apply_texture_commandContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 18, simplerlangParser.RULE_apply_texture_command);\n\n    try {\n      this.enterOuterAlt(localctx, 1);\n      this.state = 92;\n      this.match(simplerlangParser.T__17);\n      this.state = 93;\n      this.match(simplerlangParser.T__7);\n      this.state = 94;\n      this.match(simplerlangParser.ID);\n      this.state = 95;\n      this.match(simplerlangParser.T__18);\n      this.state = 96;\n      this.match(simplerlangParser.ID);\n      this.state = 97;\n      this.match(simplerlangParser.T__8);\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  apply_transform_command() {\n    let localctx = new Apply_transform_commandContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 20, simplerlangParser.RULE_apply_transform_command);\n\n    try {\n      this.enterOuterAlt(localctx, 1);\n      this.state = 99;\n      this.match(simplerlangParser.T__19);\n      this.state = 100;\n      this.match(simplerlangParser.T__7);\n      this.state = 101;\n      this.match(simplerlangParser.ID);\n      this.state = 102;\n      this.match(simplerlangParser.T__18);\n      this.state = 103;\n      this.match(simplerlangParser.ID);\n      this.state = 104;\n      this.match(simplerlangParser.T__8);\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  object_props() {\n    let localctx = new Object_propsContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 22, simplerlangParser.RULE_object_props);\n    var _la = 0; // Token type\n\n    try {\n      this.enterOuterAlt(localctx, 1);\n      this.state = 107;\n\n      this._errHandler.sync(this);\n\n      _la = this._input.LA(1);\n\n      do {\n        this.state = 106;\n        this.object_prop();\n        this.state = 109;\n\n        this._errHandler.sync(this);\n\n        _la = this._input.LA(1);\n      } while ((_la & ~0x1f) == 0 && (1 << _la & (1 << simplerlangParser.T__12 | 1 << simplerlangParser.T__20 | 1 << simplerlangParser.T__21 | 1 << simplerlangParser.T__22 | 1 << simplerlangParser.T__23 | 1 << simplerlangParser.T__24)) !== 0);\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  object_prop() {\n    let localctx = new Object_propContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 24, simplerlangParser.RULE_object_prop);\n\n    try {\n      this.state = 123;\n\n      this._errHandler.sync(this);\n\n      var la_ = this._interp.adaptivePredict(this._input, 7, this._ctx);\n\n      switch (la_) {\n        case 1:\n          this.enterOuterAlt(localctx, 1);\n          this.state = 111;\n          this.prop_name();\n          this.state = 112;\n          this.match(simplerlangParser.T__1);\n          this.state = 113;\n          this.match(simplerlangParser.ID);\n          break;\n\n        case 2:\n          this.enterOuterAlt(localctx, 2);\n          this.state = 115;\n          this.prop_name();\n          this.state = 116;\n          this.match(simplerlangParser.T__1);\n          this.state = 117;\n          this.match(simplerlangParser.NUM);\n          break;\n\n        case 3:\n          this.enterOuterAlt(localctx, 3);\n          this.state = 119;\n          this.prop_name();\n          this.state = 120;\n          this.match(simplerlangParser.T__1);\n          this.state = 121;\n          this.color_type();\n          break;\n      }\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  prop_name() {\n    let localctx = new Prop_nameContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 26, simplerlangParser.RULE_prop_name);\n    var _la = 0; // Token type\n\n    try {\n      this.enterOuterAlt(localctx, 1);\n      this.state = 125;\n      _la = this._input.LA(1);\n\n      if (!((_la & ~0x1f) == 0 && (1 << _la & (1 << simplerlangParser.T__12 | 1 << simplerlangParser.T__20 | 1 << simplerlangParser.T__21 | 1 << simplerlangParser.T__22 | 1 << simplerlangParser.T__23 | 1 << simplerlangParser.T__24)) !== 0)) {\n        this._errHandler.recoverInline(this);\n      } else {\n        this._errHandler.reportMatch(this);\n\n        this.consume();\n      }\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  color_type() {\n    let localctx = new Color_typeContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 28, simplerlangParser.RULE_color_type);\n    var _la = 0; // Token type\n\n    try {\n      this.enterOuterAlt(localctx, 1);\n      this.state = 127;\n      _la = this._input.LA(1);\n\n      if (!((_la & ~0x1f) == 0 && (1 << _la & (1 << simplerlangParser.T__25 | 1 << simplerlangParser.T__26 | 1 << simplerlangParser.T__27)) !== 0)) {\n        this._errHandler.recoverInline(this);\n      } else {\n        this._errHandler.reportMatch(this);\n\n        this.consume();\n      }\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n  object_type() {\n    let localctx = new Object_typeContext(this, this._ctx, this.state);\n    this.enterRule(localctx, 30, simplerlangParser.RULE_object_type);\n    var _la = 0; // Token type\n\n    try {\n      this.enterOuterAlt(localctx, 1);\n      this.state = 129;\n      _la = this._input.LA(1);\n\n      if (!((_la - 29 & ~0x1f) == 0 && (1 << _la - 29 & (1 << simplerlangParser.T__28 - 29 | 1 << simplerlangParser.T__29 - 29 | 1 << simplerlangParser.T__30 - 29 | 1 << simplerlangParser.T__31 - 29)) !== 0)) {\n        this._errHandler.recoverInline(this);\n      } else {\n        this._errHandler.reportMatch(this);\n\n        this.consume();\n      }\n    } catch (re) {\n      if (re instanceof antlr4__WEBPACK_IMPORTED_MODULE_0__.error.RecognitionException) {\n        localctx.exception = re;\n\n        this._errHandler.reportError(this, re);\n\n        this._errHandler.recover(this, re);\n      } else {\n        throw re;\n      }\n    } finally {\n      this.exitRule();\n    }\n\n    return localctx;\n  }\n\n}\nsimplerlangParser.EOF = antlr4__WEBPACK_IMPORTED_MODULE_0__.Token.EOF;\nsimplerlangParser.T__0 = 1;\nsimplerlangParser.T__1 = 2;\nsimplerlangParser.T__2 = 3;\nsimplerlangParser.T__3 = 4;\nsimplerlangParser.T__4 = 5;\nsimplerlangParser.T__5 = 6;\nsimplerlangParser.T__6 = 7;\nsimplerlangParser.T__7 = 8;\nsimplerlangParser.T__8 = 9;\nsimplerlangParser.T__9 = 10;\nsimplerlangParser.T__10 = 11;\nsimplerlangParser.T__11 = 12;\nsimplerlangParser.T__12 = 13;\nsimplerlangParser.T__13 = 14;\nsimplerlangParser.T__14 = 15;\nsimplerlangParser.T__15 = 16;\nsimplerlangParser.T__16 = 17;\nsimplerlangParser.T__17 = 18;\nsimplerlangParser.T__18 = 19;\nsimplerlangParser.T__19 = 20;\nsimplerlangParser.T__20 = 21;\nsimplerlangParser.T__21 = 22;\nsimplerlangParser.T__22 = 23;\nsimplerlangParser.T__23 = 24;\nsimplerlangParser.T__24 = 25;\nsimplerlangParser.T__25 = 26;\nsimplerlangParser.T__26 = 27;\nsimplerlangParser.T__27 = 28;\nsimplerlangParser.T__28 = 29;\nsimplerlangParser.T__29 = 30;\nsimplerlangParser.T__30 = 31;\nsimplerlangParser.T__31 = 32;\nsimplerlangParser.ID = 33;\nsimplerlangParser.NUM = 34;\nsimplerlangParser.WS = 35;\nsimplerlangParser.RULE_prog = 0;\nsimplerlangParser.RULE_statement_list = 1;\nsimplerlangParser.RULE_statement = 2;\nsimplerlangParser.RULE_var_declaration = 3;\nsimplerlangParser.RULE_create_command = 4;\nsimplerlangParser.RULE_transform_command = 5;\nsimplerlangParser.RULE_transform_function = 6;\nsimplerlangParser.RULE_transform_matrix_declaration = 7;\nsimplerlangParser.RULE_transform_command_list = 8;\nsimplerlangParser.RULE_apply_texture_command = 9;\nsimplerlangParser.RULE_apply_transform_command = 10;\nsimplerlangParser.RULE_object_props = 11;\nsimplerlangParser.RULE_object_prop = 12;\nsimplerlangParser.RULE_prop_name = 13;\nsimplerlangParser.RULE_color_type = 14;\nsimplerlangParser.RULE_object_type = 15;\n\nclass ProgContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_prog;\n  }\n\n  EOF() {\n    return this.getToken(simplerlangParser.EOF, 0);\n  }\n\n  statement_list = function (i) {\n    if (i === undefined) {\n      i = null;\n    }\n\n    if (i === null) {\n      return this.getTypedRuleContexts(Statement_listContext);\n    } else {\n      return this.getTypedRuleContext(Statement_listContext, i);\n    }\n  };\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterProg(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitProg(this);\n    }\n  }\n\n}\n\nclass Statement_listContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_statement_list;\n  }\n\n  statement = function (i) {\n    if (i === undefined) {\n      i = null;\n    }\n\n    if (i === null) {\n      return this.getTypedRuleContexts(StatementContext);\n    } else {\n      return this.getTypedRuleContext(StatementContext, i);\n    }\n  };\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterStatement_list(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitStatement_list(this);\n    }\n  }\n\n}\n\nclass StatementContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_statement;\n  }\n\n  var_declaration() {\n    return this.getTypedRuleContext(Var_declarationContext, 0);\n  }\n\n  transform_matrix_declaration() {\n    return this.getTypedRuleContext(Transform_matrix_declarationContext, 0);\n  }\n\n  apply_texture_command() {\n    return this.getTypedRuleContext(Apply_texture_commandContext, 0);\n  }\n\n  apply_transform_command() {\n    return this.getTypedRuleContext(Apply_transform_commandContext, 0);\n  }\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterStatement(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitStatement(this);\n    }\n  }\n\n}\n\nclass Var_declarationContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_var_declaration;\n  }\n\n  ID() {\n    return this.getToken(simplerlangParser.ID, 0);\n  }\n\n  create_command() {\n    return this.getTypedRuleContext(Create_commandContext, 0);\n  }\n\n  NUM() {\n    return this.getToken(simplerlangParser.NUM, 0);\n  }\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterVar_declaration(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitVar_declaration(this);\n    }\n  }\n\n}\n\nclass Create_commandContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_create_command;\n  }\n\n  object_type() {\n    return this.getTypedRuleContext(Object_typeContext, 0);\n  }\n\n  object_props() {\n    return this.getTypedRuleContext(Object_propsContext, 0);\n  }\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterCreate_command(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitCreate_command(this);\n    }\n  }\n\n}\n\nclass Transform_commandContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_transform_command;\n  }\n\n  transform_function() {\n    return this.getTypedRuleContext(Transform_functionContext, 0);\n  }\n\n  ID() {\n    return this.getToken(simplerlangParser.ID, 0);\n  }\n\n  NUM() {\n    return this.getToken(simplerlangParser.NUM, 0);\n  }\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterTransform_command(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitTransform_command(this);\n    }\n  }\n\n}\n\nclass Transform_functionContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_transform_function;\n  }\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterTransform_function(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitTransform_function(this);\n    }\n  }\n\n}\n\nclass Transform_matrix_declarationContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_transform_matrix_declaration;\n  }\n\n  ID() {\n    return this.getToken(simplerlangParser.ID, 0);\n  }\n\n  transform_command_list() {\n    return this.getTypedRuleContext(Transform_command_listContext, 0);\n  }\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterTransform_matrix_declaration(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitTransform_matrix_declaration(this);\n    }\n  }\n\n}\n\nclass Transform_command_listContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_transform_command_list;\n  }\n\n  transform_command = function (i) {\n    if (i === undefined) {\n      i = null;\n    }\n\n    if (i === null) {\n      return this.getTypedRuleContexts(Transform_commandContext);\n    } else {\n      return this.getTypedRuleContext(Transform_commandContext, i);\n    }\n  };\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterTransform_command_list(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitTransform_command_list(this);\n    }\n  }\n\n}\n\nclass Apply_texture_commandContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_apply_texture_command;\n  }\n\n  ID = function (i) {\n    if (i === undefined) {\n      i = null;\n    }\n\n    if (i === null) {\n      return this.getTokens(simplerlangParser.ID);\n    } else {\n      return this.getToken(simplerlangParser.ID, i);\n    }\n  };\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterApply_texture_command(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitApply_texture_command(this);\n    }\n  }\n\n}\n\nclass Apply_transform_commandContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_apply_transform_command;\n  }\n\n  ID = function (i) {\n    if (i === undefined) {\n      i = null;\n    }\n\n    if (i === null) {\n      return this.getTokens(simplerlangParser.ID);\n    } else {\n      return this.getToken(simplerlangParser.ID, i);\n    }\n  };\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterApply_transform_command(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitApply_transform_command(this);\n    }\n  }\n\n}\n\nclass Object_propsContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_object_props;\n  }\n\n  object_prop = function (i) {\n    if (i === undefined) {\n      i = null;\n    }\n\n    if (i === null) {\n      return this.getTypedRuleContexts(Object_propContext);\n    } else {\n      return this.getTypedRuleContext(Object_propContext, i);\n    }\n  };\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterObject_props(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitObject_props(this);\n    }\n  }\n\n}\n\nclass Object_propContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_object_prop;\n  }\n\n  prop_name() {\n    return this.getTypedRuleContext(Prop_nameContext, 0);\n  }\n\n  ID() {\n    return this.getToken(simplerlangParser.ID, 0);\n  }\n\n  NUM() {\n    return this.getToken(simplerlangParser.NUM, 0);\n  }\n\n  color_type() {\n    return this.getTypedRuleContext(Color_typeContext, 0);\n  }\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterObject_prop(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitObject_prop(this);\n    }\n  }\n\n}\n\nclass Prop_nameContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_prop_name;\n  }\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterProp_name(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitProp_name(this);\n    }\n  }\n\n}\n\nclass Color_typeContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_color_type;\n  }\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterColor_type(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitColor_type(this);\n    }\n  }\n\n}\n\nclass Object_typeContext extends antlr4__WEBPACK_IMPORTED_MODULE_0__.ParserRuleContext {\n  constructor(parser, parent, invokingState) {\n    if (parent === undefined) {\n      parent = null;\n    }\n\n    if (invokingState === undefined || invokingState === null) {\n      invokingState = -1;\n    }\n\n    super(parent, invokingState);\n    this.parser = parser;\n    this.ruleIndex = simplerlangParser.RULE_object_type;\n  }\n\n  enterRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.enterObject_type(this);\n    }\n  }\n\n  exitRule(listener) {\n    if (listener instanceof _simplerlangListener_js__WEBPACK_IMPORTED_MODULE_1__[\"default\"]) {\n      listener.exitObject_type(this);\n    }\n  }\n\n}\n\nsimplerlangParser.ProgContext = ProgContext;\nsimplerlangParser.Statement_listContext = Statement_listContext;\nsimplerlangParser.StatementContext = StatementContext;\nsimplerlangParser.Var_declarationContext = Var_declarationContext;\nsimplerlangParser.Create_commandContext = Create_commandContext;\nsimplerlangParser.Transform_commandContext = Transform_commandContext;\nsimplerlangParser.Transform_functionContext = Transform_functionContext;\nsimplerlangParser.Transform_matrix_declarationContext = Transform_matrix_declarationContext;\nsimplerlangParser.Transform_command_listContext = Transform_command_listContext;\nsimplerlangParser.Apply_texture_commandContext = Apply_texture_commandContext;\nsimplerlangParser.Apply_transform_commandContext = Apply_transform_commandContext;\nsimplerlangParser.Object_propsContext = Object_propsContext;\nsimplerlangParser.Object_propContext = Object_propContext;\nsimplerlangParser.Prop_nameContext = Prop_nameContext;\nsimplerlangParser.Color_typeContext = Color_typeContext;\nsimplerlangParser.Object_typeContext = Object_typeContext;\n\n//# sourceURL=webpack://test/./simplerlangParser.js?");

/***/ }),

/***/ "fs":
/*!*********************!*\
  !*** external "fs" ***!
  \*********************/
/***/ ((module) => {

"use strict";
module.exports = require("fs");

/***/ })

/******/ 	});
/************************************************************************/
/******/ 	// The module cache
/******/ 	var __webpack_module_cache__ = {};
/******/ 	
/******/ 	// The require function
/******/ 	function __webpack_require__(moduleId) {
/******/ 		// Check if module is in cache
/******/ 		var cachedModule = __webpack_module_cache__[moduleId];
/******/ 		if (cachedModule !== undefined) {
/******/ 			return cachedModule.exports;
/******/ 		}
/******/ 		// Create a new module (and put it into the cache)
/******/ 		var module = __webpack_module_cache__[moduleId] = {
/******/ 			// no module.id needed
/******/ 			// no module.loaded needed
/******/ 			exports: {}
/******/ 		};
/******/ 	
/******/ 		// Execute the module function
/******/ 		__webpack_modules__[moduleId](module, module.exports, __webpack_require__);
/******/ 	
/******/ 		// Return the exports of the module
/******/ 		return module.exports;
/******/ 	}
/******/ 	
/************************************************************************/
/******/ 	/* webpack/runtime/define property getters */
/******/ 	(() => {
/******/ 		// define getter functions for harmony exports
/******/ 		__webpack_require__.d = (exports, definition) => {
/******/ 			for(var key in definition) {
/******/ 				if(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {
/******/ 					Object.defineProperty(exports, key, { enumerable: true, get: definition[key] });
/******/ 				}
/******/ 			}
/******/ 		};
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/hasOwnProperty shorthand */
/******/ 	(() => {
/******/ 		__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/make namespace object */
/******/ 	(() => {
/******/ 		// define __esModule on exports
/******/ 		__webpack_require__.r = (exports) => {
/******/ 			if(typeof Symbol !== 'undefined' && Symbol.toStringTag) {
/******/ 				Object.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });
/******/ 			}
/******/ 			Object.defineProperty(exports, '__esModule', { value: true });
/******/ 		};
/******/ 	})();
/******/ 	
/************************************************************************/
/******/ 	
/******/ 	// startup
/******/ 	// Load entry module and return exports
/******/ 	// This entry module can't be inlined because the eval devtool is used.
/******/ 	var __webpack_exports__ = __webpack_require__("./index.js");
/******/ 	
/******/ })()
;